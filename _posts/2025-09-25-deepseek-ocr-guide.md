---
title: "DeepSeek-OCR å®Œæ•´ä½¿ç”¨æŒ‡å—ï¼šå¤šæ¨¡æ€æ–‡æ¡£ç†è§£ä¸OCRè¯†åˆ«"
date: 2025-09-25T07:00:00+08:00
categories:
  - AIå·¥å…·
tags:
  - AI
  - å·¥å…·
toc: true
mermaid: true
---

# DeepSeek-OCR å®Œæ•´ä½¿ç”¨æŒ‡å—ï¼šå¤šæ¨¡æ€æ–‡æ¡£ç†è§£ä¸OCRè¯†åˆ«

## é¡¹ç›®æ¦‚è¿°

DeepSeek-OCR æ˜¯ä¸€æ¬¾å…ˆè¿›çš„å¤šæ¨¡æ€æ¨¡å‹ï¼Œä¸“é—¨ç”¨äºå…‰å­¦å­—ç¬¦è¯†åˆ«ï¼ˆOCRï¼‰å’Œè§†è§‰æ–‡æ¡£ç†è§£ã€‚å®ƒé‡‡ç”¨åŒè§†è§‰ç¼–ç å™¨æ¶æ„ï¼ŒåŒ…æ‹¬ SAM ViT-B å’Œ CLIP-Lï¼Œèƒ½å¤Ÿå®ç°å¤šå°ºåº¦ç‰¹å¾æå–å’Œè‡ªé€‚åº”åˆ†è¾¨ç‡å¤„ç†ã€‚è¯¥æ¨¡å‹æä¾›çµæ´»çš„æ¨ç†åç«¯ï¼Œæ”¯æŒç”Ÿäº§ç¯å¢ƒçš„ vLLM å’Œå¼€å‘ç¯å¢ƒçš„ Transformersï¼Œå¯å¹¿æ³›åº”ç”¨äº OCRã€å¸ƒå±€åˆ†æã€Markdown è½¬æ¢å’Œè§†è§‰å®šä½ç­‰å¤šç§ä»»åŠ¡ã€‚

```mermaid
graph TB
    A[DeepSeek-OCR ç³»ç»Ÿæ¶æ„] --> B[è¾“å…¥å±‚]
    A --> C[è§†è§‰ç¼–ç å±‚]
    A --> D[ç‰¹å¾èåˆå±‚]
    A --> E[è¯­è¨€æ¨¡å‹å±‚]
    A --> F[è¾“å‡ºå±‚]
    
    B --> B1[å›¾åƒè¾“å…¥]
    B --> B2[PDF æ–‡æ¡£]
    B --> B3[åŠ¨æ€åˆ†è¾¨ç‡]
    
    C --> C1[SAM ViT-B ç¼–ç å™¨]
    C --> C2[CLIP-L ç¼–ç å™¨]
    C --> C3[å¤šå°ºåº¦ç‰¹å¾æå–]
    
    D --> D1[ç‰¹å¾æŠ•å½±]
    D --> D2[ä¸Šä¸‹æ–‡èåˆ]
    
    E --> E1[æ–‡æœ¬ç”Ÿæˆ]
    E --> E2[å¸ƒå±€æ£€æµ‹]
    E --> E3[ç»“æ„ç†è§£]
    
    F --> F1[Markdown è¾“å‡º]
    F --> F2[æ–‡æœ¬æå–]
    F --> F3[è¾¹ç•Œæ¡†æ ‡æ³¨]
    
    style A fill:#e1f5fe
    style C fill:#e8f5e8
    style E fill:#fff3e0
    style F fill:#f3e5f5
```

## æ ¸å¿ƒç‰¹æ€§

DeepSeek-OCR æä¾›äº†å…¨é¢çš„æ–‡æ¡£ç†è§£å’Œ OCR è§£å†³æ–¹æ¡ˆï¼Œå…·æœ‰ä»¥ä¸‹æ ¸å¿ƒç‰¹æ€§ï¼š

- **ğŸ” åŒè§†è§‰ç¼–ç å™¨æ¶æ„**ï¼šé›†æˆ SAM ViT-B å’Œ CLIP-Lï¼Œå®ç°å¤šå°ºåº¦ç‰¹å¾æå–
- **ğŸ“„ å¤šæ ¼å¼æ”¯æŒ**ï¼šæ”¯æŒå›¾åƒå’Œ PDF æ–‡æ¡£å¤„ç†
- **âš¡ é«˜æ€§èƒ½æ¨ç†**ï¼šæä¾› vLLM å’Œ Transformers ä¸¤ç§æ¨ç†åç«¯
- **ğŸ¯ å¤šä»»åŠ¡èƒ½åŠ›**ï¼šæ¶µç›– OCRã€å¸ƒå±€åˆ†æã€Markdown è½¬æ¢å’Œè§†è§‰å®šä½
- **ğŸ”„ è‡ªé€‚åº”åˆ†è¾¨ç‡**ï¼šæ”¯æŒåŸç”Ÿå’ŒåŠ¨æ€åˆ†è¾¨ç‡å¤„ç†
- **ğŸ“Š æ‰¹é‡å¤„ç†**ï¼šé«˜æ•ˆå¤„ç†å¤§æ‰¹é‡æ–‡æ¡£

```mermaid
pie title åŠŸèƒ½ç‰¹æ€§åˆ†å¸ƒ
    "OCR è¯†åˆ«" : 30
    "å¸ƒå±€åˆ†æ" : 25
    "Markdown è½¬æ¢" : 20
    "è§†è§‰å®šä½" : 15
    "æ‰¹é‡å¤„ç†" : 10
```

## ç³»ç»Ÿæ¶æ„

### æ•´ä½“æ¶æ„è®¾è®¡

DeepSeek-OCR å®ç°äº†ä¸€ä¸ªå¤šæ¨¡æ€æ–‡æ¡£ç†è§£ç³»ç»Ÿï¼Œå…·æœ‰ä¸¤ä¸ªå…±äº«ç›¸åŒæ¨¡å‹æ¶æ„çš„æ¨ç†è·¯å¾„ï¼š

```mermaid
graph TB
    subgraph "è¾“å…¥å¤„ç†å±‚"
        A[å›¾åƒè¾“å…¥]
        B[PDF æ–‡æ¡£]
        C[å›¾åƒé¢„å¤„ç†]
    end
    
    subgraph "è§†è§‰ç¼–ç å±‚"
        D[SAM ViT-B ç¼–ç å™¨]
        E[CLIP-L ç¼–ç å™¨]
        F[å¤šå°ºåº¦ç‰¹å¾æå–]
    end
    
    subgraph "ç‰¹å¾èåˆå±‚"
        G[ç‰¹å¾æŠ•å½±]
        H[ä¸Šä¸‹æ–‡èåˆ]
    end
    
    subgraph "æ¨ç†åç«¯"
        I[Transformers åç«¯]
        J[vLLM åç«¯]
    end
    
    subgraph "è¯­è¨€æ¨¡å‹å±‚"
        K[æ–‡æœ¬ç”Ÿæˆ]
        L[å¸ƒå±€æ£€æµ‹]
        M[ç»“æ„ç†è§£]
    end
    
    subgraph "è¾“å‡ºå¤„ç†å±‚"
        N[Markdown è¾“å‡º]
        O[æ–‡æœ¬æå–]
        P[è¾¹ç•Œæ¡†æ ‡æ³¨]
    end
    
    A --> C
    B --> C
    C --> D
    C --> E
    D --> F
    E --> F
    F --> G
    G --> H
    H --> I
    H --> J
    I --> K
    J --> K
    K --> L
    L --> M
    M --> N
    M --> O
    M --> P
    
    style A fill:#e1f5fe
    style D fill:#e8f5e8
    style K fill:#fff3e0
    style N fill:#f3e5f5
```

### åŒæ¨ç†è·¯å¾„æ¶æ„

DeepSeek-OCR æ”¯æŒä¸¤ç§ä¸åŒçš„æ‰§è¡Œæ¨¡å¼ï¼Œå„è‡ªé’ˆå¯¹ä¸åŒçš„ç”¨ä¾‹è¿›è¡Œäº†ä¼˜åŒ–ï¼š

```mermaid
flowchart TD
    A[è¾“å…¥è¯·æ±‚] --> B{é€‰æ‹©æ¨ç†è·¯å¾„}
    
    B -->|å¼€å‘/å®éªŒ| C[Transformers è·¯å¾„]
    B -->|ç”Ÿäº§/æ‰¹é‡| D[vLLM è·¯å¾„]
    
    C --> C1[run_dpsk_ocr.py]
    C1 --> C2[æ ‡å‡† HuggingFace æ¥å£]
    C2 --> C3[é€‚ç”¨äºå¼€å‘è°ƒè¯•]
    
    D --> D1[run_dpsk_ocr_image.py<br/>å•å›¾åƒå¤„ç†]
    D --> D2[run_dpsk_ocr_pdf.py<br/>PDF æ–‡æ¡£å¤„ç†]
    D --> D3[run_dpsk_ocr_eval_batch.py<br/>æ‰¹é‡è¯„ä¼°]
    
    D1 --> D4[AsyncLLMEngine]
    D2 --> D4
    D3 --> D4
    
    D4 --> D5[é«˜ååé‡å¤„ç†]
    D5 --> D6[å¹¶å‘å¤„ç†èƒ½åŠ›]
    D6 --> D7[æµå¼è¾“å‡ºæ”¯æŒ]
    
    C3 --> E[è¾“å‡ºç»“æœ]
    D7 --> E
    
    style C fill:#e8f5e8
    style D fill:#fff3e0
    style E fill:#f3e5f5
```

## å®‰è£…ä¸ç¯å¢ƒé…ç½®

### ç³»ç»Ÿè¦æ±‚

åœ¨å¼€å§‹å®‰è£…ä¹‹å‰ï¼Œè¯·ç¡®ä¿æ‚¨çš„ç³»ç»Ÿæ»¡è¶³ä»¥ä¸‹è¦æ±‚ï¼š

```mermaid
graph LR
    subgraph "ç¡¬ä»¶è¦æ±‚"
        A[GPU: NVIDIA<br/>æ”¯æŒ CUDA]
        B[æ˜¾å­˜: 16GB+]
        C[å†…å­˜: 32GB+]
        D[å­˜å‚¨: 50GB+]
    end
    
    subgraph "è½¯ä»¶è¦æ±‚"
        E[CUDA: 11.8]
        F[Python: 3.12.9]
        G[PyTorch: 2.6.0]
        H[vLLM: 0.8.5+cu118]
    end
    
    subgraph "ä¾èµ–é¡¹"
        I[Flash Attention<br/>2.7.3]
        J[Transformers<br/>4.51.1+]
        K[PyMuPDF]
    end
    
    style A fill:#e8f5e8
    style E fill:#e1f5fe
    style I fill:#fff3e0
```

### ç¯å¢ƒå®‰è£…æ­¥éª¤

#### 1. åŸºç¡€ç¯å¢ƒå‡†å¤‡

```bash
# æ£€æŸ¥ CUDA ç‰ˆæœ¬
nvidia-smi
nvcc --version

# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
conda create -n deepseek-ocr python=3.12.9
conda activate deepseek-ocr
```

#### 2. å®‰è£… PyTorch

```bash
# å®‰è£… CUDA 11.8 ç‰ˆæœ¬çš„ PyTorch
pip install torch==2.6.0 torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
```

#### 3. å®‰è£…æ ¸å¿ƒä¾èµ–

```bash
# å®‰è£… vLLM
pip install vllm==0.8.5+cu118

# å®‰è£… Flash Attention
pip install flash-attn==2.7.3 --no-build-isolation

# å®‰è£… Transformers
pip install transformers>=4.51.1

# å®‰è£… PDF å¤„ç†åº“
pip install PyMuPDF

# å®‰è£…å…¶ä»–ä¾èµ–
pip install pillow opencv-python numpy
```

#### 4. éªŒè¯å®‰è£…

```bash
# éªŒè¯ PyTorch å’Œ CUDA
python -c "import torch; print(torch.cuda.is_available()); print(torch.__version__)"

# éªŒè¯ vLLM
python -c "import vllm; print(vllm.__version__)"

# éªŒè¯ Flash Attention
python -c "import flash_attn; print('Flash Attention installed')"
```

### é…ç½®ç³»ç»Ÿ

DeepSeek-OCR çš„æ‰€æœ‰æ“ä½œå‚æ•°éƒ½é›†ä¸­åœ¨ `config.py` ä¸­ç®¡ç†ï¼š

```mermaid
graph TD
    A[config.py] --> B[æ¨ç†é…ç½®]
    A --> C[æ€§èƒ½é…ç½®]
    A --> D[å¤„ç†é…ç½®]
    
    B --> B1[MAX_CONCURRENCY<br/>æœ€å¤§å¹¶å‘æ•°]
    B --> B2[NUM_WORKERS<br/>å·¥ä½œçº¿ç¨‹æ•°]
    
    C --> C1[CROP_MODE<br/>è£å‰ªæ¨¡å¼]
    C --> C2[RESOLUTION<br/>åˆ†è¾¨ç‡è®¾ç½®]
    
    D --> D1[BATCH_SIZE<br/>æ‰¹é‡å¤§å°]
    D --> D2[STREAM_OUTPUT<br/>æµå¼è¾“å‡º]
    
    style A fill:#e1f5fe
    style B fill:#e8f5e8
    style C fill:#fff3e0
    style D fill:#f3e5f5
```

#### é…ç½®å‚æ•°è¯´æ˜

```python
# config.py ç¤ºä¾‹é…ç½®
MAX_CONCURRENCY = 100  # æœ€å¤§å¹¶å‘è¯·æ±‚æ•°
NUM_WORKERS = 64       # çº¿ç¨‹æ± å·¥ä½œçº¿ç¨‹æ•°
CROP_MODE = "auto"    # è£å‰ªæ¨¡å¼ï¼šauto, manual, none
BATCH_SIZE = 32       # æ‰¹é‡å¤„ç†å¤§å°
STREAM_OUTPUT = True  # æ˜¯å¦å¯ç”¨æµå¼è¾“å‡º
RESOLUTION = 1024     # é»˜è®¤åˆ†è¾¨ç‡
```

## ä½¿ç”¨æŒ‡å—

### Transformers åç«¯ä½¿ç”¨

Transformers è·¯å¾„æä¾›äº†æ ‡å‡†çš„ HuggingFace æ¥å£ï¼Œé€‚ç”¨äºå¼€å‘å’Œå®éªŒï¼š

```mermaid
sequenceDiagram
    participant User as ç”¨æˆ·
    participant Script as run_dpsk_ocr.py
    participant Model as AutoModel
    participant Output as è¾“å‡ºç»“æœ
    
    User->>Script: æä¾›å›¾åƒè¾“å…¥
    Script->>Model: åŠ è½½æ¨¡å‹<br/>trust_remote_code=True
    Model->>Model: åˆå§‹åŒ–è§†è§‰ç¼–ç å™¨
    Model->>Model: å¤„ç†å›¾åƒè¾“å…¥
    Model->>Output: ç”Ÿæˆæ–‡æœ¬ç»“æœ
    Output->>User: è¿”å› OCR ç»“æœ
```

#### åŸºæœ¬ä½¿ç”¨ç¤ºä¾‹

```python
from transformers import AutoModel, AutoProcessor
from PIL import Image

# åŠ è½½æ¨¡å‹å’Œå¤„ç†å™¨
model = AutoModel.from_pretrained(
    "deepseek-ai/DeepSeek-OCR",
    trust_remote_code=True
)
processor = AutoProcessor.from_pretrained(
    "deepseek-ai/DeepSeek-OCR",
    trust_remote_code=True
)

# å¤„ç†å›¾åƒ
image = Image.open("example.png")
inputs = processor(images=image, return_tensors="pt")

# æ‰§è¡Œ OCR
outputs = model.generate(**inputs)
result = processor.decode(outputs[0], skip_special_tokens=True)

print(result)
```

### vLLM åç«¯ä½¿ç”¨

vLLM è·¯å¾„æä¾›äº†é«˜ååé‡çš„ GPU ä¼˜åŒ–è·¯å¾„ï¼Œé€‚ç”¨äºç”Ÿäº§å·¥ä½œè´Ÿè½½ã€æ‰¹å¤„ç†å’Œ PDF æ–‡æ¡£å¤„ç†ï¼š

#### 1. å•å›¾åƒå¤„ç†

```python
# run_dpsk_ocr_image.py ä½¿ç”¨ç¤ºä¾‹
from vllm import LLM, SamplingParams
from PIL import Image
import base64
import io

# åˆå§‹åŒ– vLLM å¼•æ“
llm = LLM(
    model="deepseek-ai/DeepSeek-OCR",
    trust_remote_code=True,
    max_model_len=8192
)

# å‡†å¤‡å›¾åƒ
image = Image.open("example.png")
buffer = io.BytesIO()
image.save(buffer, format="PNG")
image_base64 = base64.b64encode(buffer.getvalue()).decode()

# æ„å»ºæç¤º
prompt = f"<image>\n<|grounding|>å¯¹è¿™å¼ å›¾åƒè¿›è¡Œ OCRã€‚"

# æ‰§è¡Œæ¨ç†
sampling_params = SamplingParams(temperature=0.1, max_tokens=4096)
outputs = llm.generate([prompt], sampling_params)

print(outputs[0].outputs[0].text)
```

#### 2. PDF æ–‡æ¡£å¤„ç†

```mermaid
flowchart TD
    A[PDF æ–‡æ¡£è¾“å…¥] --> B[PDF è§£æ]
    B --> C[é¡µé¢æå–]
    C --> D[å¹¶å‘å›¾åƒé¢„å¤„ç†]
    D --> E[çº¿ç¨‹æ± å¤„ç†]
    E --> F[æ‰¹é‡ OCR å¤„ç†]
    F --> G[ç»“æœåˆå¹¶]
    G --> H[Markdown è¾“å‡º]
    
    D --> D1[NUM_WORKERS=64]
    F --> F1[MAX_CONCURRENCY=100]
    
    style A fill:#e1f5fe
    style F fill:#e8f5e8
    style H fill:#f3e5f5
```

```python
# run_dpsk_ocr_pdf.py ä½¿ç”¨ç¤ºä¾‹
from vllm import AsyncLLMEngine, AsyncEngineArgs, SamplingParams
import fitz  # PyMuPDF
import asyncio

# é…ç½®å¼‚æ­¥å¼•æ“
engine_args = AsyncEngineArgs(
    model="deepseek-ai/DeepSeek-OCR",
    trust_remote_code=True,
    max_model_len=8192,
    max_num_seqs=100
)

# åˆå§‹åŒ–å¼‚æ­¥å¼•æ“
engine = AsyncLLMEngine.from_engine_args(engine_args)

async def process_pdf(pdf_path):
    # æ‰“å¼€ PDF
    doc = fitz.open(pdf_path)
    
    # æå–æ‰€æœ‰é¡µé¢
    tasks = []
    for page_num in range(len(doc)):
        page = doc[page_num]
        pix = page.get_pixmap()
        img_data = pix.tobytes("png")
        
        # åˆ›å»ºå¼‚æ­¥ä»»åŠ¡
        prompt = f"<image>\n<|grounding|>å°†æ–‡æ¡£è½¬æ¢ä¸º markdownã€‚"
        task = engine.generate(prompt, SamplingParams())
        tasks.append(task)
    
    # å¹¶å‘å¤„ç†æ‰€æœ‰é¡µé¢
    results = await asyncio.gather(*tasks)
    
    # åˆå¹¶ç»“æœ
    markdown_output = "\n\n".join([r.outputs[0].text for r in results])
    
    return markdown_output

# æ‰§è¡Œå¤„ç†
result = asyncio.run(process_pdf("document.pdf"))
print(result)
```

#### 3. æ‰¹é‡å¤„ç†

```python
# run_dpsk_ocr_eval_batch.py ä½¿ç”¨ç¤ºä¾‹
from vllm import LLM, SamplingParams
from concurrent.futures import ThreadPoolExecutor
import glob

# åˆå§‹åŒ–å¼•æ“
llm = LLM(
    model="deepseek-ai/DeepSeek-OCR",
    trust_remote_code=True,
    tensor_parallel_size=2  # å¤š GPU å¹¶è¡Œ
)

# å‡†å¤‡æ‰¹é‡å›¾åƒ
image_files = glob.glob("images/*.png")

# æ‰¹é‡å¤„ç†
prompts = []
for img_file in image_files:
    # å¤„ç†å›¾åƒå¹¶æ„å»ºæç¤º
    prompt = f"<image>\n<|grounding|>å¯¹è¿™å¼ å›¾åƒè¿›è¡Œ OCRã€‚"
    prompts.append(prompt)

# æ‰§è¡Œæ‰¹é‡æ¨ç†
sampling_params = SamplingParams(temperature=0.1, max_tokens=4096)
outputs = llm.generate(prompts, sampling_params)

# å¤„ç†ç»“æœ
for i, output in enumerate(outputs):
    print(f"Image {i}: {output.outputs[0].text}")
```

### æ”¯æŒçš„ä»»åŠ¡ç±»å‹

DeepSeek-OCR æ”¯æŒé€šè¿‡æç¤ºå·¥ç¨‹è¿›è¡Œå¤šç§ OCR å’Œæ–‡æ¡£ç†è§£ä»»åŠ¡ï¼š

```mermaid
graph TB
    A[ä»»åŠ¡ç±»å‹] --> B[æ–‡æ¡£è½¬ Markdown]
    A --> C[è‡ªç”± OCR]
    A --> D[å›¾åƒ OCR]
    A --> E[å¸ƒå±€åˆ†æ]
    A --> F[è§†è§‰å®šä½]
    
    B --> B1[è¾“å‡ºæ ¼å¼:<br/>Markdown + å¸ƒå±€æ ‡ç­¾]
    C --> C1[è¾“å‡ºæ ¼å¼:<br/>çº¯æ–‡æœ¬]
    D --> D1[è¾“å‡ºæ ¼å¼:<br/>æ–‡æœ¬ + è¾¹ç•Œæ¡†]
    E --> E1[è¾“å‡ºæ ¼å¼:<br/>ç»“æ„åŒ–çš„å¸ƒå±€ä¿¡æ¯]
    F --> F1[è¾“å‡ºæ ¼å¼:<br/>å®šä½åæ ‡]
    
    style A fill:#e1f5fe
    style B fill:#e8f5e8
    style C fill:#fff3e0
    style D fill:#f3e5f5
```

#### ä»»åŠ¡æç¤ºæ¨¡å¼å¯¹ç…§è¡¨

| ä»»åŠ¡                 | æç¤ºæ¨¡å¼                                      | è¾“å‡ºæ ¼å¼                   | åº”ç”¨åœºæ™¯           |
| -------------------- | --------------------------------------------- | -------------------------- | ------------------ |
| æ–‡æ¡£è½¬æ¢ä¸º Markdown  | `<image>\n<|grounding|>å°†æ–‡æ¡£è½¬æ¢ä¸º markdownã€‚` | å¸¦æœ‰å¸ƒå±€æ ‡ç­¾çš„ Markdown    | æ–‡æ¡£è½¬æ¢ã€ç»“æ„åŒ–   |
| è‡ªç”± OCR             | `<image>\nè‡ªç”± OCRã€‚`                         | ä¸å¸¦å¸ƒå±€çš„çº¯æ–‡æœ¬           | ç®€å•æ–‡æœ¬æå–       |
| å›¾åƒ OCR             | `<image>\n<|grounding|>å¯¹è¿™å¼ å›¾åƒè¿›è¡Œ OCRã€‚`   | å¸¦æœ‰è¾¹ç•Œæ¡†çš„æ–‡æœ¬           | ä¸€èˆ¬å›¾åƒå¤„ç†       |
| å¸ƒå±€åˆ†æ             | `<image>\n<|grounding|>åˆ†ææ–‡æ¡£å¸ƒå±€ã€‚`         | ç»“æ„åŒ–çš„å¸ƒå±€ä¿¡æ¯           | æ–‡æ¡£ç†è§£           |
| è§†è§‰å®šä½             | `<image>\n<|grounding|>å®šä½æ–‡æœ¬ä½ç½®ã€‚`          | å®šä½åæ ‡å’Œè¾¹ç•Œæ¡†           | ç²¾ç¡®å®šä½           |

### å®Œæ•´ç¤ºä¾‹

#### ç¤ºä¾‹ 1ï¼šæ–‡æ¡£è½¬æ¢æµç¨‹

```mermaid
sequenceDiagram
    participant User as ç”¨æˆ·
    participant PDF as PDF æ–‡æ¡£
    participant Preprocess as é¢„å¤„ç†æ¨¡å—
    participant OCR as OCR å¼•æ“
    participant Output as è¾“å‡ºæ¨¡å—
    
    User->>PDF: ä¸Šä¼  PDF æ–‡æ¡£
    PDF->>Preprocess: æå–é¡µé¢å›¾åƒ
    Preprocess->>OCR: å‘é€å›¾åƒæ•°æ®
    OCR->>OCR: æ‰§è¡Œ OCR è¯†åˆ«
    OCR->>OCR: ç”Ÿæˆ Markdown
    OCR->>Output: è¿”å›ç»“æœ
    Output->>User: æ˜¾ç¤ºè½¬æ¢ç»“æœ
```

```python
# å®Œæ•´æ–‡æ¡£è½¬æ¢ç¤ºä¾‹
import fitz
from vllm import LLM, SamplingParams
import base64
import io
from PIL import Image

def pdf_to_markdown(pdf_path, output_path):
    # åˆå§‹åŒ–æ¨¡å‹
    llm = LLM(
        model="deepseek-ai/DeepSeek-OCR",
        trust_remote_code=True
    )
    
    # æ‰“å¼€ PDF
    doc = fitz.open(pdf_path)
    markdown_pages = []
    
    sampling_params = SamplingParams(temperature=0.1, max_tokens=4096)
    
    # å¤„ç†æ¯ä¸€é¡µ
    for page_num in range(len(doc)):
        page = doc[page_num]
        
        # è½¬æ¢ä¸ºå›¾åƒ
        pix = page.get_pixmap(dpi=300)
        img = Image.frombytes("RGB", [pix.width, pix.height], pix.samples)
        
        # ç¼–ç å›¾åƒ
        buffer = io.BytesIO()
        img.save(buffer, format="PNG")
        image_base64 = base64.b64encode(buffer.getvalue()).decode()
        
        # æ„å»ºæç¤º
        prompt = f"<image>\n<|grounding|>å°†æ–‡æ¡£è½¬æ¢ä¸º markdownã€‚"
        
        # æ‰§è¡Œ OCR
        outputs = llm.generate([prompt], sampling_params)
        markdown_text = outputs[0].outputs[0].text
        
        markdown_pages.append(f"# Page {page_num + 1}\n\n{markdown_text}\n\n")
        
        print(f"Processed page {page_num + 1}/{len(doc)}")
    
    # ä¿å­˜ç»“æœ
    with open(output_path, "w", encoding="utf-8") as f:
        f.write("\n".join(markdown_pages))
    
    print(f"Markdown saved to {output_path}")

# ä½¿ç”¨ç¤ºä¾‹
pdf_to_markdown("document.pdf", "output.md")
```

#### ç¤ºä¾‹ 2ï¼šæ‰¹é‡å›¾åƒå¤„ç†

```python
# æ‰¹é‡å›¾åƒå¤„ç†ç¤ºä¾‹
from vllm import LLM, SamplingParams
from pathlib import Path
import json

def batch_process_images(image_dir, output_dir):
    # åˆå§‹åŒ–æ¨¡å‹
    llm = LLM(
        model="deepseek-ai/DeepSeek-OCR",
        trust_remote_code=True,
        tensor_parallel_size=2  # ä½¿ç”¨ 2 ä¸ª GPU
    )
    
    # å‡†å¤‡å›¾åƒåˆ—è¡¨
    image_files = list(Path(image_dir).glob("*.png"))
    image_files += list(Path(image_dir).glob("*.jpg"))
    
    results = []
    prompts = []
    
    # æ„å»ºæç¤º
    for img_file in image_files:
        prompt = f"<image>\n<|grounding|>å¯¹è¿™å¼ å›¾åƒè¿›è¡Œ OCRã€‚"
        prompts.append((img_file, prompt))
    
    # æ‰¹é‡å¤„ç†
    sampling_params = SamplingParams(temperature=0.1, max_tokens=4096)
    prompt_texts = [p[1] for p in prompts]
    
    outputs = llm.generate(prompt_texts, sampling_params)
    
    # ä¿å­˜ç»“æœ
    output_path = Path(output_dir)
    output_path.mkdir(exist_ok=True)
    
    for i, (img_file, _) in enumerate(prompts):
        result = {
            "image": str(img_file),
            "text": outputs[i].outputs[0].text
        }
        results.append(result)
        
        # ä¿å­˜å•ä¸ªç»“æœ
        output_file = output_path / f"{img_file.stem}.txt"
        output_file.write_text(result["text"], encoding="utf-8")
    
    # ä¿å­˜æ±‡æ€»ç»“æœ
    summary_file = output_path / "summary.json"
    summary_file.write_text(json.dumps(results, indent=2, ensure_ascii=False))
    
    print(f"Processed {len(results)} images")
    print(f"Results saved to {output_dir}")

# ä½¿ç”¨ç¤ºä¾‹
batch_process_images("input_images", "output_results")
```

## æ€§èƒ½ä¼˜åŒ–

### æ€§èƒ½ä¼˜åŒ–ç­–ç•¥

```mermaid
graph TD
    A[æ€§èƒ½ä¼˜åŒ–ç­–ç•¥] --> B[æ¨¡å‹ä¼˜åŒ–]
    A --> C[å¤„ç†ä¼˜åŒ–]
    A --> D[ç³»ç»Ÿä¼˜åŒ–]
    
    B --> B1[æ¨¡å‹é‡åŒ–]
    B --> B2[æ‰¹å¤„ç†ä¼˜åŒ–]
    B --> B3[å¤š GPU å¹¶è¡Œ]
    
    C --> C1[å›¾åƒé¢„å¤„ç†ä¼˜åŒ–]
    C --> C2[å¹¶å‘å¤„ç†]
    C --> C3[ç¼“å­˜æœºåˆ¶]
    
    D --> D1[å†…å­˜ç®¡ç†]
    D --> D2[CPU ä¼˜åŒ–]
    D --> D3[ç½‘ç»œä¼˜åŒ–]
    
    style A fill:#e1f5fe
    style B fill:#e8f5e8
    style C fill:#fff3e0
    style D fill:#f3e5f5
```

### é…ç½®ä¼˜åŒ–å»ºè®®

```python
# é«˜æ€§èƒ½é…ç½®ç¤ºä¾‹
# config.py

# å¹¶å‘é…ç½®
MAX_CONCURRENCY = 100  # æ ¹æ® GPU å†…å­˜è°ƒæ•´
NUM_WORKERS = 64       # æ ¹æ® CPU æ ¸å¿ƒæ•°è°ƒæ•´

# æ‰¹é‡å¤„ç†é…ç½®
BATCH_SIZE = 32        # æ ¹æ®æ˜¾å­˜è°ƒæ•´
MAX_BATCH_TOKENS = 8192

# GPU é…ç½®
TENSOR_PARALLEL_SIZE = 2  # å¤š GPU å¹¶è¡Œ
PIPELINE_PARALLEL_SIZE = 1

# å†…å­˜ä¼˜åŒ–
ENABLE_PAGED_ATTENTION = True
MAX_MODEL_LEN = 8192
```

### æ€§èƒ½åŸºå‡†æµ‹è¯•

```mermaid
graph LR
    A[æ€§èƒ½æŒ‡æ ‡] --> B[ååé‡]
    A --> C[å»¶è¿Ÿ]
    A --> D[å‡†ç¡®ç‡]
    A --> E[èµ„æºä½¿ç”¨]
    
    B --> B1[A100-40G:<br/>2500 tokens/s]
    C --> C1[å•å›¾åƒ:<br/>< 2s]
    D --> D1[OCR å‡†ç¡®ç‡:<br/>> 95%]
    E --> E1[GPU åˆ©ç”¨ç‡:<br/>> 80%]
    
    style A fill:#e1f5fe
    style B fill:#e8f5e8
    style C fill:#fff3e0
```

## é«˜çº§åŠŸèƒ½

### æµå¼è¾“å‡º

vLLM åç«¯æ”¯æŒæµå¼è¾“å‡ºï¼Œé€‚ç”¨äºäº¤äº’å¼ä½¿ç”¨åœºæ™¯ï¼š

```python
# æµå¼è¾“å‡ºç¤ºä¾‹
from vllm import LLM, SamplingParams

llm = LLM(
    model="deepseek-ai/DeepSeek-OCR",
    trust_remote_code=True
)

prompt = "<image>\n<|grounding|>å¯¹è¿™å¼ å›¾åƒè¿›è¡Œ OCRã€‚"

sampling_params = SamplingParams(
    temperature=0.1,
    max_tokens=4096,
    stream=True
)

# æµå¼ç”Ÿæˆ
for output in llm.generate([prompt], sampling_params, use_tqdm=True):
    if output.outputs:
        text = output.outputs[0].text
        print(text, end="", flush=True)
```

### è‡ªå®šä¹‰æç¤ºå·¥ç¨‹

DeepSeek-OCR æ”¯æŒé€šè¿‡è‡ªå®šä¹‰æç¤ºæ¥é€‚åº”ä¸åŒçš„ä»»åŠ¡éœ€æ±‚ï¼š

```python
# è‡ªå®šä¹‰æç¤ºç¤ºä¾‹
def create_custom_prompt(task_type, image_description=""):
    prompts = {
        "markdown": f"<image>\n<|grounding|>å°†æ–‡æ¡£è½¬æ¢ä¸º markdownã€‚",
        "ocr": f"<image>\n<|grounding|>å¯¹è¿™å¼ å›¾åƒè¿›è¡Œ OCRã€‚",
        "layout": f"<image>\n<|grounding|>åˆ†ææ–‡æ¡£å¸ƒå±€ç»“æ„ã€‚",
        "extract": f"<image>\n<|grounding|>æå–æ–‡æ¡£ä¸­çš„å…³é”®ä¿¡æ¯ã€‚"
    }
    
    base_prompt = prompts.get(task_type, prompts["ocr"])
    
    if image_description:
        base_prompt += f"\n\nå›¾åƒæè¿°ï¼š{image_description}"
    
    return base_prompt

# ä½¿ç”¨è‡ªå®šä¹‰æç¤º
prompt = create_custom_prompt("markdown", "è¿™æ˜¯ä¸€ä»½æŠ€æœ¯æ–‡æ¡£")
```

## æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜è§£å†³

```mermaid
flowchart TD
    A[é‡åˆ°é—®é¢˜] --> B{é—®é¢˜ç±»å‹}
    
    B -->|æ¨¡å‹åŠ è½½å¤±è´¥| C[æ£€æŸ¥æ¨¡å‹è·¯å¾„å’Œæƒé™]
    B -->|CUDA é”™è¯¯| D[æ£€æŸ¥ CUDA ç‰ˆæœ¬å’Œé©±åŠ¨]
    B -->|å†…å­˜ä¸è¶³| E[è°ƒæ•´æ‰¹é‡å¤§å°å’Œå¹¶å‘æ•°]
    B -->|æ€§èƒ½é—®é¢˜| F[ä¼˜åŒ–é…ç½®å‚æ•°]
    
    C --> C1[éªŒè¯æ¨¡å‹è·¯å¾„]
    C --> C2[æ£€æŸ¥ trust_remote_code]
    
    D --> D1[æ›´æ–° CUDA é©±åŠ¨]
    D --> D2[éªŒè¯ PyTorch CUDA]
    
    E --> E1[å‡å° BATCH_SIZE]
    E --> E2[é™ä½ MAX_CONCURRENCY]
    
    F --> F1[å¯ç”¨å¤š GPU]
    F --> F2[ä¼˜åŒ–å›¾åƒåˆ†è¾¨ç‡]
    
    style A fill:#e1f5fe
    style C fill:#fff3e0
    style E fill:#fce4ec
```

#### 1. æ¨¡å‹åŠ è½½å¤±è´¥

```bash
# æ£€æŸ¥æ¨¡å‹è·¯å¾„
python -c "from transformers import AutoModel; AutoModel.from_pretrained('deepseek-ai/DeepSeek-OCR', trust_remote_code=True)"

# æ£€æŸ¥ç½‘ç»œè¿æ¥
# å¦‚æœä½¿ç”¨ HuggingFace Hubï¼Œç¡®ä¿ç½‘ç»œå¯ä»¥è®¿é—®
```

#### 2. CUDA ç›¸å…³é”™è¯¯

```bash
# æ£€æŸ¥ CUDA ç‰ˆæœ¬
nvidia-smi
nvcc --version

# éªŒè¯ PyTorch CUDA
python -c "import torch; print(torch.cuda.is_available()); print(torch.version.cuda)"
```

#### 3. å†…å­˜ä¸è¶³é—®é¢˜

```python
# å‡å°æ‰¹é‡å¤§å°
BATCH_SIZE = 8  # ä» 32 å‡å°åˆ° 8

# é™ä½å¹¶å‘æ•°
MAX_CONCURRENCY = 20  # ä» 100 é™ä½åˆ° 20

# å‡å°æ¨¡å‹é•¿åº¦
MAX_MODEL_LEN = 4096  # ä» 8192 å‡å°åˆ° 4096
```

#### 4. æ€§èƒ½ä¼˜åŒ–

```python
# å¯ç”¨å¤š GPU å¹¶è¡Œ
TENSOR_PARALLEL_SIZE = 2  # ä½¿ç”¨ 2 ä¸ª GPU

# å¯ç”¨åˆ†é¡µæ³¨æ„åŠ›
ENABLE_PAGED_ATTENTION = True

# ä¼˜åŒ–å›¾åƒåˆ†è¾¨ç‡
RESOLUTION = 1024  # æ ¹æ®éœ€æ±‚è°ƒæ•´
```

## æœ€ä½³å®è·µ

### å¼€å‘ç¯å¢ƒå»ºè®®

```mermaid
graph TD
    A[å¼€å‘ç¯å¢ƒé…ç½®] --> B[ä½¿ç”¨ Transformers]
    A --> C[å°æ‰¹é‡æµ‹è¯•]
    A --> D[è¯¦ç»†æ—¥å¿—]
    
    B --> B1[æ˜“äºè°ƒè¯•]
    B --> B2[çµæ´»é…ç½®]
    
    C --> C1[å¿«é€Ÿè¿­ä»£]
    C --> C2[èµ„æºå‹å¥½]
    
    D --> D1[é”™è¯¯è¿½è¸ª]
    D --> D2[æ€§èƒ½åˆ†æ]
    
    style A fill:#e1f5fe
    style B fill:#e8f5e8
```

### ç”Ÿäº§ç¯å¢ƒå»ºè®®

```mermaid
graph TD
    A[ç”Ÿäº§ç¯å¢ƒé…ç½®] --> B[ä½¿ç”¨ vLLM]
    A --> C[æ‰¹é‡å¤„ç†]
    A --> D[ç›‘æ§å‘Šè­¦]
    
    B --> B1[é«˜æ€§èƒ½]
    B --> B2[é«˜ååé‡]
    
    C --> C1[å¹¶å‘å¤„ç†]
    C --> C2[èµ„æºä¼˜åŒ–]
    
    D --> D1[æ€§èƒ½ç›‘æ§]
    D --> D2[é”™è¯¯å‘Šè­¦]
    
    style A fill:#e1f5fe
    style B fill:#e8f5e8
```

### ä»£ç è´¨é‡å»ºè®®

1. **é”™è¯¯å¤„ç†**ï¼šå®ç°å®Œå–„çš„é”™è¯¯æ£€æŸ¥å’Œå¼‚å¸¸å¤„ç†
2. **æ—¥å¿—è®°å½•**ï¼šè®°å½•å…³é”®æ“ä½œå’Œé”™è¯¯ä¿¡æ¯
3. **èµ„æºç®¡ç†**ï¼šåŠæ—¶é‡Šæ”¾ GPU å’Œå†…å­˜èµ„æº
4. **æ€§èƒ½ç›‘æ§**ï¼šç›‘æ§å¤„ç†æ—¶é—´å’Œèµ„æºä½¿ç”¨æƒ…å†µ

```python
# æœ€ä½³å®è·µç¤ºä¾‹
import logging
from contextlib import contextmanager

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@contextmanager
def ocr_processing(image_path):
    try:
        logger.info(f"Processing image: {image_path}")
        # æ‰§è¡Œ OCR å¤„ç†
        yield result
        logger.info(f"Successfully processed: {image_path}")
    except Exception as e:
        logger.error(f"Error processing {image_path}: {e}")
        raise
    finally:
        # æ¸…ç†èµ„æº
        torch.cuda.empty_cache()
```

## æ€»ç»“

DeepSeek-OCR æ˜¯ä¸€ä¸ªåŠŸèƒ½å¼ºå¤§çš„å¤šæ¨¡æ€æ–‡æ¡£ç†è§£å’Œ OCR è¯†åˆ«ç³»ç»Ÿã€‚é€šè¿‡æœ¬æŒ‡å—ï¼Œæ‚¨å¯ä»¥ï¼š

1. **å¿«é€Ÿéƒ¨ç½²**ï¼šäº†è§£ç³»ç»Ÿè¦æ±‚å’Œå®‰è£…æ­¥éª¤
2. **çµæ´»ä½¿ç”¨**ï¼šæŒæ¡ Transformers å’Œ vLLM ä¸¤ç§æ¨ç†æ–¹å¼
3. **é«˜æ•ˆå¤„ç†**ï¼šå­¦ä¹ æ‰¹é‡å¤„ç†å’Œæ€§èƒ½ä¼˜åŒ–æŠ€å·§
4. **è§£å†³é—®é¢˜**ï¼šæŒæ¡å¸¸è§é—®é¢˜çš„è§£å†³æ–¹æ³•

DeepSeek-OCR çš„åŒè§†è§‰ç¼–ç å™¨æ¶æ„å’Œçµæ´»çš„æ¨ç†åç«¯ä½¿å…¶æˆä¸ºæ–‡æ¡£å¤„ç†é¢†åŸŸçš„å¼ºå¤§å·¥å…·ã€‚æ— è®ºæ˜¯å¼€å‘ç¯å¢ƒçš„å¿«é€ŸåŸå‹ï¼Œè¿˜æ˜¯ç”Ÿäº§ç¯å¢ƒçš„å¤§è§„æ¨¡å¤„ç†ï¼Œéƒ½èƒ½æä¾›å‡ºè‰²çš„æ€§èƒ½å’Œå‡†ç¡®åº¦ã€‚

## å‚è€ƒèµ„æº

- [DeepSeek-OCR GitHub ä»“åº“](https://github.com/deepseek-ai/DeepSeek-OCR) - æºä»£ç å’Œæœ€æ–°æ›´æ–°
- [DeepWiki - DeepSeek-OCR æ–‡æ¡£](https://deepwiki.com/deepseek-ai/DeepSeek-OCR) - å®Œæ•´æ–‡æ¡£å’Œä½¿ç”¨æŒ‡å—
- [vLLM å®˜æ–¹æ–‡æ¡£](https://docs.vllm.ai/) - vLLM æ¨ç†å¼•æ“æ–‡æ¡£
- [Transformers æ–‡æ¡£](https://huggingface.co/docs/transformers) - HuggingFace Transformers æ–‡æ¡£

---

*æœ¬æ–‡æ¡£åŸºäº DeepSeek-OCR æœ€æ–°ç‰ˆæœ¬ç¼–å†™ï¼Œæ¶µç›–äº†ä»åŸºç¡€å®‰è£…åˆ°é«˜çº§åº”ç”¨çš„å…¨æ–¹ä½æŒ‡å—ã€‚*
