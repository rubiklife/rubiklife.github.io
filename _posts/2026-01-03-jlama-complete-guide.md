---
title: "Jlama å®Œæ•´æŒ‡å—ï¼šJavaç”Ÿæ€çš„ç°ä»£åŒ–LLMæ¨ç†å¼•æ“"
date: 2026-01-03 10:00:00 +0800
categories:
  - AIå·¥å…·
tags:
  - Java
  - LLM
  - AI
toc: true
mermaid: true
---

## Jlama ç®€ä»‹

Jlama æ˜¯ä¸€ä¸ªä¸“ä¸º Java ç”Ÿæ€æ‰“é€ çš„ç°ä»£åŒ–å¤§è¯­è¨€æ¨¡å‹(LLM)æ¨ç†å¼•æ“ã€‚å®ƒåˆ©ç”¨ Java 21 çš„æœ€æ–°ç‰¹æ€§ï¼ˆå¦‚å‘é‡ APIï¼‰å®ç°äº†é«˜æ€§èƒ½çš„æ¨¡å‹æ¨ç†ï¼Œè®© Java å¼€å‘è€…èƒ½å¤Ÿè½»æ¾åœ°åœ¨è‡ªå·±çš„åº”ç”¨ä¸­é›†æˆå¤§è¯­è¨€æ¨¡å‹èƒ½åŠ›ã€‚

```mermaid
mindmap
  root((Jlama))
    æ ¸å¿ƒç‰¹æ€§
      çº¯Javaå®ç°
      é‡åŒ–æ”¯æŒ
      é«˜æ€§èƒ½æ¨ç†
      åˆ†å¸ƒå¼èƒ½åŠ›
    æŠ€æœ¯äº®ç‚¹
      Java 21å‘é‡API
      SIMDä¼˜åŒ–
      å†…å­˜é«˜æ•ˆ
      NativeåŠ é€Ÿ
    åº”ç”¨åœºæ™¯
      ä¼ä¸šåº”ç”¨é›†æˆ
      è¾¹ç¼˜è®¾å¤‡éƒ¨ç½²
      ç§æœ‰åŒ–éƒ¨ç½²
      å¾®æœåŠ¡æ¶æ„
    ç”Ÿæ€æ”¯æŒ
      Langchain4jé›†æˆ
      OpenAIå…¼å®¹API
      HuggingFaceæ¨¡å‹
      Dockeréƒ¨ç½²
```

### ä¸ºä»€ä¹ˆé€‰æ‹© Jlamaï¼Ÿ

åœ¨ Python ä¸»å¯¼çš„ AI é¢†åŸŸï¼ŒJlama ä¸º Java å¼€å‘è€…æä¾›äº†ä¸€ä¸ªå¼ºå¤§çš„æœ¬åœ°åŒ–è§£å†³æ–¹æ¡ˆï¼š

```mermaid
graph TB
    subgraph "ä¼ ç»Ÿæ–¹æ¡ˆçš„ç—›ç‚¹"
        A1[Pythonä¾èµ–] --> B1[é¢å¤–çš„è¿è¡Œæ—¶ç¯å¢ƒ]
        A2[è¿›ç¨‹é—´é€šä¿¡] --> B2[æ€§èƒ½æŸè€—]
        A3[éƒ¨ç½²å¤æ‚] --> B3[ç»´æŠ¤æˆæœ¬é«˜]
        A4[ç±»å‹ä¸å®‰å…¨] --> B4[é›†æˆå›°éš¾]
    end
    
    subgraph "Jlamaçš„ä¼˜åŠ¿"
        C1[çº¯Javaå®ç°] --> D1[ç»Ÿä¸€è¿è¡Œæ—¶]
        C2[æœ¬åœ°è°ƒç”¨] --> D2[é›¶å¼€é”€]
        C3[ç®€å•éƒ¨ç½²] --> D3[jaråŒ…å³å¯]
        C4[ç±»å‹å®‰å…¨] --> D4[æ˜“äºé›†æˆ]
    end
    
    style C1 fill:#4caf50
    style C2 fill:#4caf50
    style C3 fill:#4caf50
    style C4 fill:#4caf50
```

## æ ¸å¿ƒæ¶æ„

### æ•´ä½“æ¶æ„è®¾è®¡

Jlama é‡‡ç”¨æ¨¡å—åŒ–è®¾è®¡ï¼Œå„ç»„ä»¶èŒè´£æ¸…æ™°ï¼š

```mermaid
graph LR
    subgraph "åº”ç”¨å±‚"
        A1[CLIå·¥å…·]
        A2[REST API]
        A3[Javaåº”ç”¨]
    end
    
    subgraph "æ ¸å¿ƒå±‚ - jlama-core"
        B1[æ¨¡å‹åŠ è½½å™¨]
        B2[æ¨ç†å¼•æ“]
        B3[é‡åŒ–å¤„ç†]
        B4[æç¤ºç®¡ç†]
    end
    
    subgraph "åŠ é€Ÿå±‚ - jlama-native"
        C1[å‘é‡API]
        C2[SIMDä¼˜åŒ–]
        C3[Nativeä»£ç ]
    end
    
    subgraph "ç½‘ç»œå±‚ - jlama-net"
        D1[RESTæœåŠ¡]
        D2[åˆ†å¸ƒå¼åè°ƒ]
        D3[é›†ç¾¤Worker]
    end
    
    subgraph "æ•°æ®å±‚"
        E1[HuggingFace]
        E2[æœ¬åœ°æ¨¡å‹]
        E3[é‡åŒ–æ¨¡å‹]
    end
    
    A1 --> B1
    A2 --> B1
    A3 --> B1
    
    B1 --> B2
    B2 --> B3
    B2 --> B4
    
    B2 --> C1
    C1 --> C2
    C2 --> C3
    
    A2 --> D1
    D1 --> D2
    D2 --> D3
    
    B1 --> E1
    B1 --> E2
    B1 --> E3
    
    style B2 fill:#ff9800
    style C1 fill:#2196f3
    style D1 fill:#9c27b0
```

### æ¨¡å—è¯´æ˜

| æ¨¡å— | åŠŸèƒ½æè¿° | ä¸»è¦ç‰¹æ€§ |
|------|---------|---------|
| **jlama-core** | æ ¸å¿ƒæ¨ç†å¼•æ“ | æ¨¡å‹åŠ è½½ã€æ¨ç†æ‰§è¡Œã€é‡åŒ–å¤„ç† |
| **jlama-native** | æœ¬åœ°åŠ é€Ÿåº“ | SIMDä¼˜åŒ–ã€å‘é‡APIã€Nativeä»£ç  |
| **jlama-net** | ç½‘ç»œæœåŠ¡ | REST APIã€åˆ†å¸ƒå¼æ¨ç†ã€é›†ç¾¤ç®¡ç† |
| **jlama-cli** | å‘½ä»¤è¡Œå·¥å…· | æ¨¡å‹ä¸‹è½½ã€èŠå¤©ã€é‡åŒ–ã€APIæœåŠ¡ |
| **jlama-tests** | æµ‹è¯•å¥—ä»¶ | å•å…ƒæµ‹è¯•ã€é›†æˆæµ‹è¯•ã€æ€§èƒ½æµ‹è¯• |

## å¿«é€Ÿå¼€å§‹

### ç¯å¢ƒè¦æ±‚

```mermaid
graph TB
    subgraph "ç³»ç»Ÿè¦æ±‚"
        A[Java 21æˆ–æ›´é«˜ç‰ˆæœ¬]
        B[æ”¯æŒå‘é‡APIçš„JVM]
        C[è¶³å¤Ÿçš„å†…å­˜ç©ºé—´]
    end
    
    subgraph "å¯é€‰è¦æ±‚"
        D[CUDAæ”¯æŒ<br/>GPUåŠ é€Ÿ]
        E[Dockerç¯å¢ƒ<br/>å®¹å™¨éƒ¨ç½²]
        F[Maven/Gradle<br/>é¡¹ç›®æ„å»º]
    end
    
    A --> G[å¼€å§‹ä½¿ç”¨Jlama]
    B --> G
    C --> G
    
    style G fill:#4caf50
```

### å®‰è£… Jlama CLI

**æ–¹å¼ä¸€ï¼šä½¿ç”¨é¢„ç¼–è¯‘ç‰ˆæœ¬**

```bash
# ä¸‹è½½æœ€æ–°ç‰ˆæœ¬
curl -L https://github.com/tjake/Jlama/releases/latest/download/jlama-cli.jar -o jlama-cli.jar

# è®¾ç½®ç¯å¢ƒå˜é‡å¯ç”¨é¢„è§ˆç‰¹æ€§
export JDK_JAVA_OPTIONS="--add-modules jdk.incubator.vector --enable-preview"

# è¿è¡Œ Jlama
java -jar jlama-cli.jar
```

**æ–¹å¼äºŒï¼šä»æºç æ„å»º**

```bash
# å…‹éš†ä»“åº“
git clone https://github.com/tjake/Jlama.git
cd Jlama

# ä½¿ç”¨ Maven æ„å»º
./mvnw clean install

# è¿è¡Œ CLI
java -jar jlama-cli/target/jlama-cli.jar
```

**æ–¹å¼ä¸‰ï¼šä½¿ç”¨ Docker**

```bash
# æ‹‰å–é•œåƒ
docker pull tjake/jlama:latest

# è¿è¡Œå®¹å™¨
docker run -p 8080:8080 tjake/jlama:latest restapi llama-3.2-1b-instruct
```

### å‘½ä»¤è¡Œå·¥å…·æ¦‚è§ˆ

Jlama CLI æä¾›äº†ä¸°å¯Œçš„å‘½ä»¤ï¼š

```mermaid
graph TB
    A[jlama] --> B[æ¨ç†å‘½ä»¤]
    A --> C[åˆ†å¸ƒå¼æ¨ç†]
    A --> D[å…¶ä»–å·¥å…·]
    
    B --> B1[chat<br/>äº¤äº’å¼å¯¹è¯]
    B --> B2[restapi<br/>å¯åŠ¨APIæœåŠ¡]
    B --> B3[complete<br/>æ–‡æœ¬è¡¥å…¨]
    
    C --> C1[cluster-coordinator<br/>é›†ç¾¤åè°ƒå™¨]
    C --> C2[cluster-worker<br/>é›†ç¾¤å·¥ä½œèŠ‚ç‚¹]
    
    D --> D1[download<br/>ä¸‹è½½æ¨¡å‹]
    D --> D2[quantize<br/>æ¨¡å‹é‡åŒ–]
    D --> D3[list<br/>åˆ—å‡ºæ¨¡å‹]
    D --> D4[rm<br/>åˆ é™¤æ¨¡å‹]
    
    style B1 fill:#4caf50
    style B2 fill:#2196f3
    style B3 fill:#ff9800
    style C1 fill:#9c27b0
    style C2 fill:#9c27b0
```

## æ ¸å¿ƒåŠŸèƒ½è¯¦è§£

### 1. æ¨¡å‹ä¸‹è½½å’Œç®¡ç†

Jlama æ”¯æŒä» HuggingFace ç›´æ¥ä¸‹è½½å’Œç®¡ç†æ¨¡å‹ï¼š

```bash
# ä¸‹è½½æ¨¡å‹ï¼ˆä½¿ç”¨ owner/name æ ¼å¼ï¼‰
jlama download tjake/Llama-3.2-1B-Instruct-JQ4

# åˆ—å‡ºæœ¬åœ°æ¨¡å‹
jlama list

# åˆ é™¤æ¨¡å‹
jlama rm tjake/Llama-3.2-1B-Instruct-JQ4
```

**Jlama ç»´æŠ¤çš„é‡åŒ–æ¨¡å‹ä»“åº“**

æ‰€æœ‰å®˜æ–¹é‡åŒ–æ¨¡å‹éƒ½æ‰˜ç®¡åœ¨ https://huggingface.co/tjake

```mermaid
graph LR
    A[HuggingFace<br/>tjake] --> B[Llamaç³»åˆ—]
    A --> C[GPTç³»åˆ—]
    A --> D[å…¶ä»–æ¨¡å‹]
    
    B --> B1[Llama-3.2-1B-JQ4]
    B --> B2[Llama-3.2-3B-JQ4]
    B --> B3[Llama-3.1-8B-JQ4]
    
    C --> C1[GPT-2-Medium]
    C --> C2[GPT-Neoç³»åˆ—]
    
    D --> D1[Mistralç³»åˆ—]
    D --> D2[å…¶ä»–å¼€æºæ¨¡å‹]
    
    style A fill:#ff9800
```

### 2. äº¤äº’å¼èŠå¤©

å¯åŠ¨å‘½ä»¤è¡ŒèŠå¤©ç•Œé¢ï¼š

```bash
# åŸºæœ¬èŠå¤©
jlama chat tjake/Llama-3.2-1B-Instruct-JQ4

# æŒ‡å®šå·¥ä½œç›®å½•
jlama chat --model-directory ./models tjake/Llama-3.2-1B-Instruct-JQ4

# è®¾ç½®ç”Ÿæˆå‚æ•°
jlama chat --temperature 0.7 --max-tokens 512 tjake/Llama-3.2-1B-Instruct-JQ4
```

**èŠå¤©æµç¨‹**

```mermaid
sequenceDiagram
    participant U as ç”¨æˆ·
    participant CLI as Jlama CLI
    participant E as æ¨ç†å¼•æ“
    participant M as æ¨¡å‹
    
    U->>CLI: å¯åŠ¨èŠå¤©å‘½ä»¤
    CLI->>E: åŠ è½½æ¨¡å‹
    E->>M: åˆå§‹åŒ–æ¨¡å‹
    M-->>E: æ¨¡å‹å°±ç»ª
    E-->>CLI: å‡†å¤‡æ¥æ”¶è¾“å…¥
    
    loop å¯¹è¯å¾ªç¯
        U->>CLI: è¾“å…¥æ¶ˆæ¯
        CLI->>E: æ„å»ºæç¤ºè¯
        E->>M: æ‰§è¡Œæ¨ç†
        M-->>E: ç”Ÿæˆtokenæµ
        E-->>CLI: è¿”å›å“åº”
        CLI-->>U: æ˜¾ç¤ºå›å¤
    end
```

### 3. REST API æœåŠ¡

Jlama æä¾› OpenAI å…¼å®¹çš„ REST APIï¼š

```bash
# å¯åŠ¨ API æœåŠ¡
jlama restapi tjake/Llama-3.2-1B-Instruct-JQ4

# æŒ‡å®šç«¯å£
jlama restapi --port 8080 tjake/Llama-3.2-1B-Instruct-JQ4

# æŒ‡å®šæ•°æ®ç±»å‹
jlama restapi --model-type F32 --model-quantization I8 tjake/Llama-3.2-1B-Instruct-JQ4
```

**API æ¶æ„**

```mermaid
graph TB
    subgraph "å®¢æˆ·ç«¯"
        A[HTTPå®¢æˆ·ç«¯]
        B[OpenAI SDK]
        C[Langchain4j]
    end
    
    subgraph "Jlama REST API"
        D[/v1/chat/completions]
        E[/v1/completions]
        F[/v1/models]
    end
    
    subgraph "æ¨ç†å¼•æ“"
        G[æ¨¡å‹ç®¡ç†å™¨]
        H[æ¨ç†æ‰§è¡Œå™¨]
        I[å“åº”ç”Ÿæˆå™¨]
    end
    
    A --> D
    B --> D
    C --> D
    
    A --> E
    B --> E
    
    A --> F
    
    D --> G
    E --> G
    F --> G
    
    G --> H
    H --> I
    
    style D fill:#4caf50
    style E fill:#2196f3
    style F fill:#ff9800
```

**è°ƒç”¨ç¤ºä¾‹**

```bash
# ä½¿ç”¨ curl è°ƒç”¨ API
curl -X POST http://localhost:8080/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "tjake/Llama-3.2-1B-Instruct-JQ4",
    "messages": [
      {"role": "system", "content": "You are a helpful assistant."},
      {"role": "user", "content": "What is the capital of France?"}
    ],
    "temperature": 0.7,
    "max_tokens": 256
  }'
```

### 4. æ–‡æœ¬è¡¥å…¨

æ‰§è¡Œç®€å•çš„æ–‡æœ¬è¡¥å…¨ä»»åŠ¡ï¼š

```bash
# åŸºæœ¬è¡¥å…¨
jlama complete --prompt "Once upon a time" tjake/Llama-3.2-1B-Instruct-JQ4

# è®¾ç½®ç”Ÿæˆå‚æ•°
jlama complete \
  --prompt "The best way to learn programming is" \
  --temperature 0.0 \
  --max-tokens 256 \
  tjake/Llama-3.2-1B-Instruct-JQ4
```

### 5. æ¨¡å‹é‡åŒ–

Jlama æ”¯æŒå¯¹æ¨¡å‹è¿›è¡Œé‡åŒ–ä»¥å‡å°‘å†…å­˜å ç”¨ï¼š

```bash
# é‡åŒ–æ¨¡å‹
jlama quantize --quantization Q4 --output ./quantized llama-3.2-1b-instruct

# æ”¯æŒçš„é‡åŒ–ç±»å‹
# Q4: 4-bit é‡åŒ–
# Q8: 8-bit é‡åŒ–
# I8: INT8 é‡åŒ–
```

**é‡åŒ–æ•ˆæœå¯¹æ¯”**

```mermaid
graph LR
    subgraph "åŸå§‹æ¨¡å‹ FP32"
        A1[æ¨¡å‹å¤§å°: 4GB]
        A2[å†…å­˜å ç”¨: 4GB]
        A3[æ¨ç†é€Ÿåº¦: åŸºå‡†]
    end
    
    subgraph "Q8é‡åŒ–"
        B1[æ¨¡å‹å¤§å°: 1GB]
        B2[å†…å­˜å ç”¨: 1GB]
        B3[æ¨ç†é€Ÿåº¦: 1.2x]
        B4[ç²¾åº¦æŸå¤±: <1%]
    end
    
    subgraph "Q4é‡åŒ–"
        C1[æ¨¡å‹å¤§å°: 0.5GB]
        C2[å†…å­˜å ç”¨: 0.5GB]
        C3[æ¨ç†é€Ÿåº¦: 1.5x]
        C4[ç²¾åº¦æŸå¤±: <3%]
    end
    
    A1 --> B1
    B1 --> C1
    
    style C1 fill:#4caf50
    style C2 fill:#4caf50
    style C3 fill:#4caf50
```

## åˆ†å¸ƒå¼æ¨ç†

Jlama æ”¯æŒåˆ†å¸ƒå¼æ¨ç†ï¼Œå¯ä»¥åœ¨å¤šå°æœºå™¨ä¸ŠååŒå¤„ç†å¤§æ¨¡å‹ï¼š

### æ¶æ„è®¾è®¡

```mermaid
graph TB
    subgraph "å®¢æˆ·ç«¯å±‚"
        A[åº”ç”¨ç¨‹åº]
    end
    
    subgraph "åè°ƒå±‚"
        B[Cluster Coordinator<br/>é›†ç¾¤åè°ƒå™¨]
    end
    
    subgraph "å·¥ä½œèŠ‚ç‚¹å±‚"
        C1[Worker 1<br/>GPUèŠ‚ç‚¹]
        C2[Worker 2<br/>CPUèŠ‚ç‚¹]
        C3[Worker 3<br/>CPUèŠ‚ç‚¹]
        C4[Worker N<br/>...]
    end
    
    subgraph "æ¨¡å‹å±‚"
        D[åˆ†ç‰‡æ¨¡å‹<br/>Model Shards]
    end
    
    A -->|HTTPè¯·æ±‚| B
    
    B -->|ä»»åŠ¡åˆ†å‘| C1
    B -->|ä»»åŠ¡åˆ†å‘| C2
    B -->|ä»»åŠ¡åˆ†å‘| C3
    B -->|ä»»åŠ¡åˆ†å‘| C4
    
    C1 -->|åŠ è½½åˆ†ç‰‡| D
    C2 -->|åŠ è½½åˆ†ç‰‡| D
    C3 -->|åŠ è½½åˆ†ç‰‡| D
    C4 -->|åŠ è½½åˆ†ç‰‡| D
    
    C1 -->|è¿”å›ç»“æœ| B
    C2 -->|è¿”å›ç»“æœ| B
    C3 -->|è¿”å›ç»“æœ| B
    C4 -->|è¿”å›ç»“æœ| B
    
    B -->|èšåˆå“åº”| A
    
    style B fill:#ff9800
    style C1 fill:#4caf50
    style C2 fill:#4caf50
    style C3 fill:#4caf50
```

### å¯åŠ¨åˆ†å¸ƒå¼é›†ç¾¤

**æ­¥éª¤ 1ï¼šå¯åŠ¨åè°ƒå™¨**

```bash
# åœ¨ä¸»èŠ‚ç‚¹å¯åŠ¨åè°ƒå™¨
jlama cluster-coordinator \
  --host 0.0.0.0 \
  --port 8080 \
  --model-directory ./models \
  tjake/Llama-3.1-8B-Instruct-JQ4
```

**æ­¥éª¤ 2ï¼šå¯åŠ¨å·¥ä½œèŠ‚ç‚¹**

```bash
# åœ¨å·¥ä½œèŠ‚ç‚¹1å¯åŠ¨
jlama cluster-worker \
  --coordinator-host master.example.com \
  --coordinator-port 8080 \
  --worker-id worker-1

# åœ¨å·¥ä½œèŠ‚ç‚¹2å¯åŠ¨
jlama cluster-worker \
  --coordinator-host master.example.com \
  --coordinator-port 8080 \
  --worker-id worker-2

# æ›´å¤šå·¥ä½œèŠ‚ç‚¹...
```

**æ­¥éª¤ 3ï¼šä½¿ç”¨é›†ç¾¤ API**

```bash
# è°ƒç”¨é›†ç¾¤ APIï¼ˆä¸å•æœºAPIå…¼å®¹ï¼‰
curl -X POST http://master.example.com:8080/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "tjake/Llama-3.1-8B-Instruct-JQ4",
    "messages": [
      {"role": "user", "content": "Explain quantum computing"}
    ]
  }'
```

### åˆ†å¸ƒå¼æ¨ç†æµç¨‹

```mermaid
sequenceDiagram
    participant C as å®¢æˆ·ç«¯
    participant CO as åè°ƒå™¨
    participant W1 as Worker 1
    participant W2 as Worker 2
    participant W3 as Worker 3
    
    C->>CO: å‘é€æ¨ç†è¯·æ±‚
    CO->>CO: ä»»åŠ¡åˆ†è§£
    
    par å¹¶è¡Œå¤„ç†
        CO->>W1: åˆ†ç‰‡ä»»åŠ¡1
        CO->>W2: åˆ†ç‰‡ä»»åŠ¡2
        CO->>W3: åˆ†ç‰‡ä»»åŠ¡3
    end
    
    W1->>W1: æ¨ç†è®¡ç®—
    W2->>W2: æ¨ç†è®¡ç®—
    W3->>W3: æ¨ç†è®¡ç®—
    
    par è¿”å›ç»“æœ
        W1->>CO: è¿”å›ç»“æœ1
        W2->>CO: è¿”å›ç»“æœ2
        W3->>CO: è¿”å›ç»“æœ3
    end
    
    CO->>CO: èšåˆç»“æœ
    CO->>C: è¿”å›æœ€ç»ˆå“åº”
```

## Java åº”ç”¨é›†æˆ

### Maven ä¾èµ–é…ç½®

åœ¨ä½ çš„ `pom.xml` ä¸­æ·»åŠ ä¾èµ–ï¼š

```xml
<properties>
    <jlama.version>0.8.4</jlama.version>
</properties>

<dependencies>
    <!-- Jlama æ ¸å¿ƒåº“ -->
    <dependency>
        <groupId>com.github.tjake</groupId>
        <artifactId>jlama-core</artifactId>
        <version>${jlama.version}</version>
    </dependency>
    
    <!-- Jlama Native åŠ é€Ÿï¼ˆå¯é€‰ï¼‰ -->
    <dependency>
        <groupId>com.github.tjake</groupId>
        <artifactId>jlama-native</artifactId>
        <version>${jlama.version}</version>
        <!-- ä½¿ç”¨ os-maven-plugin è‡ªåŠ¨æ£€æµ‹ç³»ç»Ÿ -->
        <classifier>${os.detected.name}-${os.detected.arch}</classifier>
    </dependency>
</dependencies>

<build>
    <extensions>
        <!-- ç”¨äºæ£€æµ‹æ“ä½œç³»ç»Ÿå’Œæ¶æ„ -->
        <extension>
            <groupId>kr.motd.maven</groupId>
            <artifactId>os-maven-plugin</artifactId>
            <version>1.7.1</version>
        </extension>
    </extensions>
    
    <plugins>
        <plugin>
            <groupId>org.apache.maven.plugins</groupId>
            <artifactId>maven-compiler-plugin</artifactId>
            <version>3.11.0</version>
            <configuration>
                <release>21</release>
                <compilerArgs>
                    <arg>--enable-preview</arg>
                    <arg>--add-modules</arg>
                    <arg>jdk.incubator.vector</arg>
                </compilerArgs>
            </configuration>
        </plugin>
    </plugins>
</build>
```

### Gradle é…ç½®

```gradle
plugins {
    id 'java'
}

java {
    toolchain {
        languageVersion = JavaLanguageVersion.of(21)
    }
}

dependencies {
    implementation 'com.github.tjake:jlama-core:0.8.4'
    implementation "com.github.tjake:jlama-native:0.8.4:${osdetector.os}-${osdetector.arch}"
}

tasks.withType(JavaCompile) {
    options.compilerArgs += ['--enable-preview', '--add-modules', 'jdk.incubator.vector']
}

tasks.withType(Test) {
    jvmArgs += ['--enable-preview', '--add-modules', 'jdk.incubator.vector']
}
```

### åŸºç¡€ä½¿ç”¨ç¤ºä¾‹

```java
import com.github.tjake.jlama.model.AbstractModel;
import com.github.tjake.jlama.model.ModelSupport;
import com.github.tjake.jlama.safetensors.DType;
import com.github.tjake.jlama.safetensors.prompt.PromptContext;
import com.github.tjake.jlama.util.Downloader;

import java.io.File;
import java.io.IOException;
import java.util.UUID;

public class BasicExample {
    public void basicInference() throws IOException {
        String model = "tjake/Llama-3.2-1B-Instruct-JQ4";
        String workingDirectory = "./models";
        String prompt = "What is the best season to plant avocados?";
        
        // 1. ä¸‹è½½æ¨¡å‹ï¼ˆå¦‚æœæœ¬åœ°ä¸å­˜åœ¨ï¼‰
        File localModelPath = new Downloader(workingDirectory, model)
            .huggingFaceModel();
        
        // 2. åŠ è½½æ¨¡å‹
        // å‚æ•°è¯´æ˜ï¼š
        // - localModelPath: æ¨¡å‹è·¯å¾„
        // - DType.F32: å·¥ä½œç²¾åº¦ï¼ˆFP32ï¼‰
        // - DType.I8: å†…å­˜ç²¾åº¦ï¼ˆINT8é‡åŒ–ï¼‰
        AbstractModel m = ModelSupport.loadModel(
            localModelPath, 
            DType.F32, 
            DType.I8
        );
        
        // 3. æ„å»ºæç¤ºè¯ä¸Šä¸‹æ–‡
        PromptContext ctx;
        if (m.promptSupport().isPresent()) {
            // å¦‚æœæ¨¡å‹æ”¯æŒèŠå¤©æç¤ºï¼Œä½¿ç”¨ç»“æ„åŒ–æç¤º
            ctx = m.promptSupport()
                .get()
                .builder()
                .addSystemMessage("You are a helpful chatbot who writes short responses.")
                .addUserMessage(prompt)
                .build();
        } else {
            // å¦åˆ™ä½¿ç”¨çº¯æ–‡æœ¬æç¤º
            ctx = PromptContext.of(prompt);
        }
        
        System.out.println("Prompt: " + ctx.getPrompt() + "\n");
        
        // 4. æ‰§è¡Œæ¨ç†
        // å‚æ•°è¯´æ˜ï¼š
        // - UUID.randomUUID(): ä¼šè¯ID
        // - ctx: æç¤ºè¯ä¸Šä¸‹æ–‡
        // - 0.0f: temperatureï¼ˆ0è¡¨ç¤ºç¡®å®šæ€§è¾“å‡ºï¼‰
        // - 256: æœ€å¤§ç”Ÿæˆtokenæ•°
        // - (s, f) -> {}: tokenå›è°ƒï¼ˆå¯ç”¨äºæµå¼è¾“å‡ºï¼‰
        var response = m.generate(
            UUID.randomUUID(), 
            ctx, 
            0.0f, 
            256, 
            (token, probability) -> {}
        );
        
        System.out.println(response.responseText);
    }
}
```

### Builder API ç¤ºä¾‹

Jlama æä¾›äº†æ›´ä¼˜é›…çš„ Builder APIï¼š

```java
public class BuilderExample {
    public void builderInference() throws IOException {
        String model = "tjake/Llama-3.2-1B-Instruct-JQ4";
        String workingDirectory = "./models";
        
        // ä¸‹è½½å¹¶åŠ è½½æ¨¡å‹
        File localModelPath = new Downloader(workingDirectory, model)
            .huggingFaceModel();
        AbstractModel m = ModelSupport.loadModel(
            localModelPath, 
            DType.F32, 
            DType.I8
        );
        
        // ä½¿ç”¨ Builder API æ‰§è¡Œæ¨ç†
        var response = m.generateBuilder()
            .session(UUID.randomUUID())  // ä¼šè¯IDï¼ˆå¯é€‰ï¼Œé»˜è®¤éšæœºç”Ÿæˆï¼‰
            .prompt("Explain quantum computing in simple terms")  // ç›´æ¥æä¾›æ–‡æœ¬
            .ntokens(512)  // æœ€å¤§tokenæ•°ï¼ˆé»˜è®¤256ï¼‰
            .temperature(0.7f)  // æ¸©åº¦å‚æ•°ï¼ˆé»˜è®¤0.0ï¼‰
            .onTokenWithTimings((token, timing) -> {
                // æµå¼è¾“å‡ºå›è°ƒ
                System.out.print(token);
            })
            .generate();
        
        System.out.println("\n\nFinal response: " + response.responseText);
    }
}
```

### ç®€åŒ–çš„æç¤ºè¯æ„å»º

```java
public class SimplifiedPromptExample {
    public void simplifiedPrompt() throws IOException {
        String model = "tjake/Llama-3.2-1B-Instruct-JQ4";
        
        File localModelPath = new Downloader("./models", model)
            .huggingFaceModel();
        AbstractModel m = ModelSupport.loadModel(
            localModelPath, 
            DType.F32, 
            DType.I8
        );
        
        // ä½¿ç”¨ç®€åŒ–çš„ prompt() æ–¹æ³•
        var ctx = m.prompt()
            .addSystemMessage("You are a helpful coding assistant.")
            .addUserMessage("Write a Java function to reverse a string")
            .build();  // è‡ªåŠ¨å¤„ç†æ˜¯å¦æ”¯æŒç»“æ„åŒ–æç¤º
        
        var response = m.generateBuilder()
            .promptContext(ctx)
            .temperature(0.3f)
            .ntokens(512)
            .generate();
        
        System.out.println(response.responseText);
    }
}
```

### å¤šè½®å¯¹è¯ç¤ºä¾‹

```java
import java.util.ArrayList;
import java.util.List;
import java.util.Scanner;

public class MultiTurnChatExample {
    public void interactiveChat() throws IOException {
        String model = "tjake/Llama-3.2-1B-Instruct-JQ4";
        
        // åŠ è½½æ¨¡å‹
        File localModelPath = new Downloader("./models", model)
            .huggingFaceModel();
        AbstractModel m = ModelSupport.loadModel(
            localModelPath, 
            DType.F32, 
            DType.I8
        );
        
        // ä¼šè¯å†å²
        List<Message> history = new ArrayList<>();
        Scanner scanner = new Scanner(System.in);
        UUID sessionId = UUID.randomUUID();
        
        System.out.println("Chat started. Type 'exit' to quit.\n");
        
        while (true) {
            System.out.print("You: ");
            String userInput = scanner.nextLine();
            
            if ("exit".equalsIgnoreCase(userInput)) {
                break;
            }
            
            // æ„å»ºåŒ…å«å†å²çš„æç¤ºè¯
            var promptBuilder = m.prompt()
                .addSystemMessage("You are a helpful assistant.");
            
            // æ·»åŠ å†å²å¯¹è¯
            for (Message msg : history) {
                if (msg.role.equals("user")) {
                    promptBuilder.addUserMessage(msg.content);
                } else {
                    promptBuilder.addAssistantMessage(msg.content);
                }
            }
            
            // æ·»åŠ å½“å‰ç”¨æˆ·è¾“å…¥
            promptBuilder.addUserMessage(userInput);
            var ctx = promptBuilder.build();
            
            // ç”Ÿæˆå›å¤
            System.out.print("Assistant: ");
            var response = m.generateBuilder()
                .session(sessionId)
                .promptContext(ctx)
                .temperature(0.7f)
                .ntokens(256)
                .onTokenWithTimings((token, timing) -> {
                    System.out.print(token);
                })
                .generate();
            System.out.println("\n");
            
            // æ›´æ–°å†å²
            history.add(new Message("user", userInput));
            history.add(new Message("assistant", response.responseText));
        }
        
        scanner.close();
    }
    
    static class Message {
        String role;
        String content;
        
        Message(String role, String content) {
            this.role = role;
            this.content = content;
        }
    }
}
```

## Langchain4j é›†æˆ

Jlama æä¾›äº† Langchain4j çš„å®˜æ–¹é›†æˆï¼Œè®©ä½ å¯ä»¥åœ¨ Langchain4j åº”ç”¨ä¸­æ— ç¼ä½¿ç”¨ Jlamaï¼š

```mermaid
graph LR
    subgraph "Langchain4jåº”ç”¨"
        A[AIæœåŠ¡]
        B[æç¤ºæ¨¡æ¿]
        C[é“¾å¼è°ƒç”¨]
    end
    
    subgraph "Jlamaé›†æˆ"
        D[JlamaChatModel]
        E[JlamaStreamingChatModel]
        F[JlamaEmbeddingModel]
    end
    
    subgraph "Jlamaå¼•æ“"
        G[æ¨¡å‹æ¨ç†]
        H[å‘é‡ç”Ÿæˆ]
    end
    
    A --> D
    B --> D
    C --> D
    
    A --> E
    
    D --> G
    E --> G
    F --> H
    
    style D fill:#4caf50
    style E fill:#2196f3
    style F fill:#ff9800
```

### åŸºç¡€é›†æˆç¤ºä¾‹

```java
import dev.langchain4j.model.chat.ChatLanguageModel;
import dev.langchain4j.model.jlama.JlamaChatModel;
import dev.langchain4j.service.AiServices;

public class Langchain4jExample {
    
    interface Assistant {
        String chat(String message);
    }
    
    public void basicLangchain4j() {
        // åˆ›å»º Jlama èŠå¤©æ¨¡å‹
        ChatLanguageModel model = JlamaChatModel.builder()
            .modelName("tjake/Llama-3.2-1B-Instruct-JQ4")
            .modelDirectory("./models")
            .temperature(0.7)
            .maxTokens(256)
            .build();
        
        // åˆ›å»º AI æœåŠ¡
        Assistant assistant = AiServices.create(Assistant.class, model);
        
        // ä½¿ç”¨åŠ©æ‰‹
        String response = assistant.chat("What is machine learning?");
        System.out.println(response);
    }
}
```

### æµå¼å“åº”ç¤ºä¾‹

```java
import dev.langchain4j.model.chat.StreamingChatLanguageModel;
import dev.langchain4j.model.jlama.JlamaStreamingChatModel;

public class StreamingExample {
    public void streamingChat() {
        StreamingChatLanguageModel model = JlamaStreamingChatModel.builder()
            .modelName("tjake/Llama-3.2-1B-Instruct-JQ4")
            .modelDirectory("./models")
            .temperature(0.7)
            .build();
        
        model.generate("Tell me a story about a robot", 
            new StreamingResponseHandler<AiMessage>() {
                @Override
                public void onNext(String token) {
                    System.out.print(token);
                }
                
                @Override
                public void onComplete(Response<AiMessage> response) {
                    System.out.println("\n\nGeneration complete!");
                }
                
                @Override
                public void onError(Throwable error) {
                    error.printStackTrace();
                }
            });
    }
}
```

## é«˜çº§ç‰¹æ€§

### 1. è‡ªå®šä¹‰æ•°æ®ç±»å‹

Jlama æ”¯æŒå¤šç§æ•°æ®ç±»å‹ç»„åˆä»¥ä¼˜åŒ–æ€§èƒ½å’Œå†…å­˜ï¼š

```java
public class DataTypeExample {
    public void differentDataTypes() throws IOException {
        File modelPath = new Downloader("./models", "tjake/Llama-3.2-1B-Instruct-JQ4")
            .huggingFaceModel();
        
        // é…ç½®1ï¼šFP32 å·¥ä½œç²¾åº¦ + FP32 å†…å­˜
        // - æœ€é«˜ç²¾åº¦ï¼Œæœ€å¤§å†…å­˜å ç”¨
        AbstractModel m1 = ModelSupport.loadModel(
            modelPath, 
            DType.F32,  // å·¥ä½œç²¾åº¦
            DType.F32   // å†…å­˜ç²¾åº¦
        );
        
        // é…ç½®2ï¼šFP32 å·¥ä½œç²¾åº¦ + INT8 å†…å­˜é‡åŒ–
        // - å¹³è¡¡ç²¾åº¦å’Œå†…å­˜ï¼ˆæ¨èï¼‰
        AbstractModel m2 = ModelSupport.loadModel(
            modelPath, 
            DType.F32, 
            DType.I8
        );
        
        // é…ç½®3ï¼šBF16 å·¥ä½œç²¾åº¦ + INT8 å†…å­˜é‡åŒ–
        // - æ›´å¿«çš„æ¨ç†é€Ÿåº¦
        AbstractModel m3 = ModelSupport.loadModel(
            modelPath, 
            DType.BF16, 
            DType.I8
        );
    }
}
```

**æ•°æ®ç±»å‹å¯¹æ¯”**

```mermaid
graph TB
    subgraph "ç²¾åº¦ vs æ€§èƒ½"
        A[FP32/FP32<br/>æœ€é«˜ç²¾åº¦<br/>æœ€æ…¢é€Ÿåº¦<br/>æœ€å¤§å†…å­˜]
        B[FP32/I8<br/>é«˜ç²¾åº¦<br/>ä¸­ç­‰é€Ÿåº¦<br/>ä¸­ç­‰å†…å­˜]
        C[BF16/I8<br/>è‰¯å¥½ç²¾åº¦<br/>æœ€å¿«é€Ÿåº¦<br/>æœ€å°å†…å­˜]
    end
    
    A -->|é™ä½å†…å­˜| B
    B -->|æå‡é€Ÿåº¦| C
    
    style A fill:#e74c3c
    style B fill:#f39c12
    style C fill:#27ae60
```

### 2. æç¤ºè¯æ¨¡æ¿

```java
public class PromptTemplateExample {
    public void templateExample() throws IOException {
        File modelPath = new Downloader("./models", "tjake/Llama-3.2-1B-Instruct-JQ4")
            .huggingFaceModel();
        AbstractModel model = ModelSupport.loadModel(modelPath, DType.F32, DType.I8);
        
        // å®šä¹‰æç¤ºè¯æ¨¡æ¿
        String systemTemplate = """
            You are an expert %s programmer.
            Write clean, efficient, and well-documented code.
            Follow %s best practices.
            """;
        
        String userTemplate = """
            Task: %s
            
            Requirements:
            %s
            """;
        
        // å¡«å……æ¨¡æ¿
        String language = "Java";
        String bestPractices = "SOLID principles and clean code";
        String task = "Create a REST API for user management";
        String requirements = """
            - CRUD operations for users
            - Input validation
            - Exception handling
            - Unit tests
            """;
        
        var ctx = model.prompt()
            .addSystemMessage(String.format(systemTemplate, language, bestPractices))
            .addUserMessage(String.format(userTemplate, task, requirements))
            .build();
        
        var response = model.generateBuilder()
            .promptContext(ctx)
            .temperature(0.3f)
            .ntokens(1024)
            .generate();
        
        System.out.println(response.responseText);
    }
}
```

### 3. æ‰¹é‡æ¨ç†

```java
import java.util.List;
import java.util.concurrent.CompletableFuture;
import java.util.stream.Collectors;

public class BatchInferenceExample {
    public void batchInference() throws IOException {
        File modelPath = new Downloader("./models", "tjake/Llama-3.2-1B-Instruct-JQ4")
            .huggingFaceModel();
        AbstractModel model = ModelSupport.loadModel(modelPath, DType.F32, DType.I8);
        
        // å‡†å¤‡æ‰¹é‡æç¤º
        List<String> prompts = List.of(
            "What is Java?",
            "Explain object-oriented programming",
            "What are design patterns?",
            "Describe the Spring Framework"
        );
        
        // å¹¶è¡Œå¤„ç†
        List<CompletableFuture<String>> futures = prompts.stream()
            .map(prompt -> CompletableFuture.supplyAsync(() -> {
                try {
                    var ctx = model.prompt()
                        .addUserMessage(prompt)
                        .build();
                    
                    var response = model.generateBuilder()
                        .session(UUID.randomUUID())
                        .promptContext(ctx)
                        .temperature(0.0f)
                        .ntokens(128)
                        .generate();
                    
                    return response.responseText;
                } catch (Exception e) {
                    return "Error: " + e.getMessage();
                }
            }))
            .collect(Collectors.toList());
        
        // ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
        List<String> results = futures.stream()
            .map(CompletableFuture::join)
            .collect(Collectors.toList());
        
        // è¾“å‡ºç»“æœ
        for (int i = 0; i < prompts.size(); i++) {
            System.out.println("Prompt: " + prompts.get(i));
            System.out.println("Response: " + results.get(i));
            System.out.println("---");
        }
    }
}
```

### 4. æ€§èƒ½ç›‘æ§

```java
public class PerformanceMonitoringExample {
    public void monitorPerformance() throws IOException {
        File modelPath = new Downloader("./models", "tjake/Llama-3.2-1B-Instruct-JQ4")
            .huggingFaceModel();
        AbstractModel model = ModelSupport.loadModel(modelPath, DType.F32, DType.I8);
        
        String prompt = "Explain machine learning";
        var ctx = model.prompt().addUserMessage(prompt).build();
        
        // è®°å½•å¼€å§‹æ—¶é—´
        long startTime = System.currentTimeMillis();
        long startMemory = Runtime.getRuntime().totalMemory() - Runtime.getRuntime().freeMemory();
        
        // Token è®¡æ•°å™¨
        int[] tokenCount = {0};
        
        var response = model.generateBuilder()
            .promptContext(ctx)
            .temperature(0.7f)
            .ntokens(256)
            .onTokenWithTimings((token, timing) -> {
                tokenCount[0]++;
                // timing åŒ…å«æ¯ä¸ª token çš„ç”Ÿæˆæ—¶é—´
            })
            .generate();
        
        // è®¡ç®—æ€§èƒ½æŒ‡æ ‡
        long endTime = System.currentTimeMillis();
        long endMemory = Runtime.getRuntime().totalMemory() - Runtime.getRuntime().freeMemory();
        
        long totalTime = endTime - startTime;
        long memoryUsed = (endMemory - startMemory) / (1024 * 1024); // MB
        double tokensPerSecond = (tokenCount[0] * 1000.0) / totalTime;
        
        System.out.println("=== Performance Metrics ===");
        System.out.println("Total time: " + totalTime + " ms");
        System.out.println("Tokens generated: " + tokenCount[0]);
        System.out.println("Tokens/second: " + String.format("%.2f", tokensPerSecond));
        System.out.println("Memory used: " + memoryUsed + " MB");
        System.out.println("Response: " + response.responseText);
    }
}
```

## ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²

### Docker éƒ¨ç½²

**Dockerfile ç¤ºä¾‹**

```dockerfile
FROM eclipse-temurin:21-jdk-alpine

# å®‰è£…å¿…è¦çš„å·¥å…·
RUN apk add --no-cache curl

# è®¾ç½®å·¥ä½œç›®å½•
WORKDIR /app

# å¤åˆ¶ Jlama jar æ–‡ä»¶
COPY jlama-cli.jar /app/jlama-cli.jar

# åˆ›å»ºæ¨¡å‹ç›®å½•
RUN mkdir -p /app/models

# è®¾ç½® Java é€‰é¡¹
ENV JDK_JAVA_OPTIONS="--add-modules jdk.incubator.vector --enable-preview"

# æš´éœ²ç«¯å£
EXPOSE 8080

# å¥åº·æ£€æŸ¥
HEALTHCHECK --interval=30s --timeout=10s --start-period=60s --retries=3 \
  CMD curl -f http://localhost:8080/v1/models || exit 1

# å¯åŠ¨å‘½ä»¤ï¼ˆé€šè¿‡ç¯å¢ƒå˜é‡é…ç½®ï¼‰
ENTRYPOINT ["java", "-jar", "/app/jlama-cli.jar"]
CMD ["restapi", "--port", "8080", "--model-directory", "/app/models"]
```

**docker-compose.yml**

```yaml
version: '3.8'

services:
  jlama:
    image: jlama:latest
    build: .
    ports:
      - "8080:8080"
    environment:
      - JDK_JAVA_OPTIONS=--add-modules jdk.incubator.vector --enable-preview
      - MODEL_NAME=tjake/Llama-3.2-1B-Instruct-JQ4
    volumes:
      - ./models:/app/models
      - ./config:/app/config
    command: ["restapi", "--port", "8080", "${MODEL_NAME}"]
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/v1/models"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
```

### Kubernetes éƒ¨ç½²

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: jlama-deployment
  labels:
    app: jlama
spec:
  replicas: 3
  selector:
    matchLabels:
      app: jlama
  template:
    metadata:
      labels:
        app: jlama
    spec:
      containers:
      - name: jlama
        image: jlama:latest
        ports:
        - containerPort: 8080
        env:
        - name: JDK_JAVA_OPTIONS
          value: "--add-modules jdk.incubator.vector --enable-preview"
        - name: MODEL_NAME
          value: "tjake/Llama-3.2-1B-Instruct-JQ4"
        volumeMounts:
        - name: models
          mountPath: /app/models
        resources:
          requests:
            memory: "4Gi"
            cpu: "2000m"
          limits:
            memory: "8Gi"
            cpu: "4000m"
        livenessProbe:
          httpGet:
            path: /v1/models
            port: 8080
          initialDelaySeconds: 60
          periodSeconds: 30
        readinessProbe:
          httpGet:
            path: /v1/models
            port: 8080
          initialDelaySeconds: 30
          periodSeconds: 10
      volumes:
      - name: models
        persistentVolumeClaim:
          claimName: jlama-models-pvc
---
apiVersion: v1
kind: Service
metadata:
  name: jlama-service
spec:
  selector:
    app: jlama
  ports:
  - protocol: TCP
    port: 80
    targetPort: 8080
  type: LoadBalancer
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: jlama-models-pvc
spec:
  accessModes:
  - ReadWriteMany
  resources:
    requests:
      storage: 50Gi
```

### æ€§èƒ½è°ƒä¼˜

```mermaid
graph TB
    subgraph "æ€§èƒ½ä¼˜åŒ–ç­–ç•¥"
        A[JVM ä¼˜åŒ–]
        B[æ¨¡å‹ä¼˜åŒ–]
        C[ç¡¬ä»¶ä¼˜åŒ–]
        D[å¹¶å‘ä¼˜åŒ–]
    end
    
    A --> A1[å¢å¤§å †å†…å­˜<br/>-Xmx8g]
    A --> A2[ä½¿ç”¨G1GC<br/>-XX:+UseG1GC]
    A --> A3[å¯ç”¨å‘é‡API<br/>--add-modules]
    
    B --> B1[ä½¿ç”¨é‡åŒ–æ¨¡å‹<br/>Q4/Q8]
    B --> B2[é€‰æ‹©åˆé€‚ç²¾åº¦<br/>FP32/BF16/I8]
    B --> B3[æ¨¡å‹åˆ†ç‰‡<br/>åˆ†å¸ƒå¼æ¨ç†]
    
    C --> C1[ä½¿ç”¨GPUåŠ é€Ÿ]
    C --> C2[å¢åŠ CPUæ ¸å¿ƒ]
    C --> C3[ä½¿ç”¨é«˜é€Ÿå­˜å‚¨]
    
    D --> D1[è¿æ¥æ± ç®¡ç†]
    D --> D2[è¯·æ±‚æ‰¹å¤„ç†]
    D --> D3[å¼‚æ­¥å¤„ç†]
    
    style A fill:#e74c3c
    style B fill:#3498db
    style C fill:#2ecc71
    style D fill:#f39c12
```

**JVM ä¼˜åŒ–å‚æ•°**

```bash
# å¯åŠ¨ Jlama æ—¶çš„ JVM ä¼˜åŒ–å‚æ•°
java \
  -Xmx8g \
  -Xms8g \
  -XX:+UseG1GC \
  -XX:MaxGCPauseMillis=200 \
  -XX:+UseStringDeduplication \
  --add-modules jdk.incubator.vector \
  --enable-preview \
  -jar jlama-cli.jar restapi tjake/Llama-3.2-1B-Instruct-JQ4
```

## æœ€ä½³å®è·µ

### 1. æ¨¡å‹é€‰æ‹©æŒ‡å—

```mermaid
graph TB
    A[é€‰æ‹©æ¨¡å‹] --> B{åº”ç”¨åœºæ™¯}
    
    B -->|èŠå¤©å¯¹è¯| C[Llama-3.2-1B-Instruct]
    B -->|ä»£ç ç”Ÿæˆ| D[CodeLlamaç³»åˆ—]
    B -->|æ–‡æœ¬è¡¥å…¨| E[GPT-Neoç³»åˆ—]
    B -->|å¤šè¯­è¨€| F[Mistralç³»åˆ—]
    
    C --> G{èµ„æºé™åˆ¶}
    D --> G
    E --> G
    F --> G
    
    G -->|å†…å­˜<2GB| H[1Bé‡åŒ–æ¨¡å‹<br/>Q4é‡åŒ–]
    G -->|å†…å­˜2-4GB| I[1B-3Bæ¨¡å‹<br/>Q8é‡åŒ–]
    G -->|å†…å­˜4-8GB| J[3B-7Bæ¨¡å‹<br/>Q8é‡åŒ–]
    G -->|å†…å­˜>8GB| K[8B+æ¨¡å‹<br/>Q4/Q8é‡åŒ–]
    
    style H fill:#27ae60
    style I fill:#f39c12
    style J fill:#e74c3c
```

### 2. é”™è¯¯å¤„ç†

```java
public class ErrorHandlingExample {
    public void robustInference() {
        try {
            // æ¨¡å‹åŠ è½½
            File modelPath = new Downloader("./models", "tjake/Llama-3.2-1B-Instruct-JQ4")
                .huggingFaceModel();
            
            AbstractModel model = ModelSupport.loadModel(modelPath, DType.F32, DType.I8);
            
            // æ¨ç†æ‰§è¡Œ
            var ctx = model.prompt()
                .addUserMessage("Your question here")
                .build();
            
            var response = model.generateBuilder()
                .promptContext(ctx)
                .temperature(0.7f)
                .ntokens(256)
                .onTokenWithTimings((token, timing) -> {
                    // å¯ä»¥åœ¨è¿™é‡Œæ·»åŠ è¶…æ—¶æ£€æµ‹
                    if (timing > 5000) { // 5ç§’è¶…æ—¶
                        throw new RuntimeException("Token generation timeout");
                    }
                })
                .generate();
            
            System.out.println(response.responseText);
            
        } catch (IOException e) {
            System.err.println("Model loading failed: " + e.getMessage());
            // å¤„ç†æ¨¡å‹åŠ è½½å¤±è´¥
            e.printStackTrace();
            
        } catch (OutOfMemoryError e) {
            System.err.println("Out of memory. Consider:");
            System.err.println("1. Using a smaller model");
            System.err.println("2. Increasing JVM heap size (-Xmx)");
            System.err.println("3. Using more aggressive quantization (Q4)");
            
        } catch (RuntimeException e) {
            System.err.println("Inference error: " + e.getMessage());
            // å¤„ç†æ¨ç†é”™è¯¯
            e.printStackTrace();
            
        } catch (Exception e) {
            System.err.println("Unexpected error: " + e.getMessage());
            e.printStackTrace();
        }
    }
}
```

### 3. èµ„æºç®¡ç†

```java
public class ResourceManagementExample {
    private static final int MAX_CONCURRENT_REQUESTS = 10;
    private static final Semaphore semaphore = new Semaphore(MAX_CONCURRENT_REQUESTS);
    
    public void managedInference(String prompt) throws IOException, InterruptedException {
        // è·å–è®¸å¯ï¼ˆé™åˆ¶å¹¶å‘ï¼‰
        semaphore.acquire();
        
        try {
            File modelPath = new Downloader("./models", "tjake/Llama-3.2-1B-Instruct-JQ4")
                .huggingFaceModel();
            
            // ä½¿ç”¨ try-with-resources ç®¡ç†èµ„æº
            AbstractModel model = ModelSupport.loadModel(modelPath, DType.F32, DType.I8);
            
            var ctx = model.prompt()
                .addUserMessage(prompt)
                .build();
            
            var response = model.generateBuilder()
                .promptContext(ctx)
                .temperature(0.7f)
                .ntokens(256)
                .generate();
            
            System.out.println(response.responseText);
            
        } finally {
            // é‡Šæ”¾è®¸å¯
            semaphore.release();
            
            // å»ºè®®è¿›è¡Œåƒåœ¾å›æ”¶ï¼ˆå¦‚æœå†…å­˜ç´§å¼ ï¼‰
            System.gc();
        }
    }
}
```

### 4. æ—¥å¿—å’Œç›‘æ§

```java
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

public class LoggingExample {
    private static final Logger logger = LoggerFactory.getLogger(LoggingExample.class);
    
    public void loggedInference() throws IOException {
        String modelName = "tjake/Llama-3.2-1B-Instruct-JQ4";
        logger.info("Starting inference with model: {}", modelName);
        
        try {
            long startTime = System.currentTimeMillis();
            
            File modelPath = new Downloader("./models", modelName)
                .huggingFaceModel();
            logger.debug("Model path: {}", modelPath.getAbsolutePath());
            
            AbstractModel model = ModelSupport.loadModel(modelPath, DType.F32, DType.I8);
            logger.info("Model loaded successfully");
            
            var ctx = model.prompt()
                .addUserMessage("Test prompt")
                .build();
            
            int[] tokenCount = {0};
            var response = model.generateBuilder()
                .promptContext(ctx)
                .temperature(0.7f)
                .ntokens(256)
                .onTokenWithTimings((token, timing) -> {
                    tokenCount[0]++;
                    if (tokenCount[0] % 10 == 0) {
                        logger.debug("Generated {} tokens", tokenCount[0]);
                    }
                })
                .generate();
            
            long endTime = System.currentTimeMillis();
            long duration = endTime - startTime;
            
            logger.info("Inference completed in {} ms", duration);
            logger.info("Generated {} tokens", tokenCount[0]);
            logger.info("Tokens/second: {}", (tokenCount[0] * 1000.0) / duration);
            logger.debug("Response: {}", response.responseText);
            
        } catch (Exception e) {
            logger.error("Inference failed", e);
            throw e;
        }
    }
}
```

## å¸¸è§é—®é¢˜

### Q1: å¦‚ä½•é€‰æ‹©åˆé€‚çš„é‡åŒ–çº§åˆ«ï¼Ÿ

```mermaid
graph LR
    A[é€‰æ‹©é‡åŒ–çº§åˆ«] --> B{ä¼˜å…ˆçº§}
    
    B -->|ç²¾åº¦ä¼˜å…ˆ| C[Q8é‡åŒ–<br/>ç²¾åº¦æŸå¤±<1%<br/>å†…å­˜å ç”¨25%]
    B -->|å¹³è¡¡| D[Q4é‡åŒ–<br/>ç²¾åº¦æŸå¤±<3%<br/>å†…å­˜å ç”¨12.5%]
    B -->|é€Ÿåº¦ä¼˜å…ˆ| E[Q4+æ¿€è¿›é‡åŒ–<br/>ç²¾åº¦æŸå¤±<5%<br/>å†…å­˜å ç”¨<10%]
    
    style C fill:#27ae60
    style D fill:#f39c12
    style E fill:#e74c3c
```

### Q2: OutOfMemoryError å¦‚ä½•è§£å†³ï¼Ÿ

1. **å¢åŠ  JVM å †å†…å­˜**
```bash
java -Xmx8g -jar jlama-cli.jar restapi model-name
```

2. **ä½¿ç”¨æ›´å°çš„æ¨¡å‹æˆ–æ›´æ¿€è¿›çš„é‡åŒ–**
```bash
# ä½¿ç”¨ Q4 é‡åŒ–è€Œä¸æ˜¯ Q8
jlama download tjake/Llama-3.2-1B-Instruct-JQ4
```

3. **å‡å°‘å¹¶å‘è¯·æ±‚æ•°**
```java
// é™åˆ¶åŒæ—¶å¤„ç†çš„è¯·æ±‚æ•°é‡
Semaphore semaphore = new Semaphore(5); // æœ€å¤š5ä¸ªå¹¶å‘è¯·æ±‚
```

### Q3: å¦‚ä½•æå‡æ¨ç†é€Ÿåº¦ï¼Ÿ

1. **ä½¿ç”¨ Native åŠ é€Ÿåº“**
```xml
<dependency>
    <groupId>com.github.tjake</groupId>
    <artifactId>jlama-native</artifactId>
    <classifier>${os.detected.name}-${os.detected.arch}</classifier>
</dependency>
```

2. **ä½¿ç”¨ BF16 ç²¾åº¦**
```java
AbstractModel model = ModelSupport.loadModel(
    modelPath, 
    DType.BF16,  // ä½¿ç”¨ BF16 è€Œä¸æ˜¯ FP32
    DType.I8
);
```

3. **å¯ç”¨åˆ†å¸ƒå¼æ¨ç†**
```bash
# ä½¿ç”¨å¤šå°æœºå™¨ååŒæ¨ç†å¤§æ¨¡å‹
jlama cluster-coordinator model-name
```

### Q4: å¦‚ä½•å¤„ç†é•¿ä¸Šä¸‹æ–‡ï¼Ÿ

```java
public void longContextInference() throws IOException {
    File modelPath = new Downloader("./models", "tjake/Llama-3.2-1B-Instruct-JQ4")
        .huggingFaceModel();
    AbstractModel model = ModelSupport.loadModel(modelPath, DType.F32, DType.I8);
    
    // å¯¹äºé•¿ä¸Šä¸‹æ–‡ï¼Œåˆ†æ®µå¤„ç†
    String longContext = "...very long text...";
    int chunkSize = 2000; // æ¯æ¬¡å¤„ç† 2000 å­—ç¬¦
    
    List<String> chunks = splitIntoChunks(longContext, chunkSize);
    List<String> summaries = new ArrayList<>();
    
    // å¯¹æ¯ä¸ªåˆ†æ®µè¿›è¡Œæ€»ç»“
    for (String chunk : chunks) {
        var ctx = model.prompt()
            .addSystemMessage("Summarize the following text concisely:")
            .addUserMessage(chunk)
            .build();
        
        var response = model.generateBuilder()
            .promptContext(ctx)
            .temperature(0.3f)
            .ntokens(128)
            .generate();
        
        summaries.add(response.responseText);
    }
    
    // å¯¹æ‰€æœ‰æ€»ç»“è¿›è¡Œæœ€ç»ˆæ±‡æ€»
    String finalSummary = String.join("\n", summaries);
    // ...
}

private List<String> splitIntoChunks(String text, int chunkSize) {
    List<String> chunks = new ArrayList<>();
    for (int i = 0; i < text.length(); i += chunkSize) {
        chunks.add(text.substring(i, Math.min(text.length(), i + chunkSize)));
    }
    return chunks;
}
```

## å¼€å‘æŒ‡å—

### è´¡çŒ®ä»£ç 

å¦‚æœä½ æƒ³ä¸º Jlama è´¡çŒ®ä»£ç ï¼Œè¯·å‚è€ƒå®˜æ–¹çš„å¼€å‘æŒ‡å—ï¼š

1. **Fork ä»“åº“**
```bash
git clone https://github.com/YOUR_USERNAME/Jlama.git
cd Jlama
```

2. **æ„å»ºé¡¹ç›®**
```bash
./mvnw clean install
```

3. **è¿è¡Œæµ‹è¯•**
```bash
./mvnw test
```

4. **æäº¤ Pull Request**

### é¡¹ç›®ç»“æ„

```
Jlama/
â”œâ”€â”€ jlama-core/          # æ ¸å¿ƒæ¨ç†å¼•æ“
â”‚   â”œâ”€â”€ src/main/java/
â”‚   â”‚   â”œâ”€â”€ model/       # æ¨¡å‹å®šä¹‰
â”‚   â”‚   â”œâ”€â”€ tensor/      # å¼ é‡æ“ä½œ
â”‚   â”‚   â””â”€â”€ math/        # æ•°å­¦è¿ç®—
â”‚   â””â”€â”€ pom.xml
â”œâ”€â”€ jlama-native/        # Native åŠ é€Ÿ
â”‚   â”œâ”€â”€ src/main/c/      # C/C++ ä»£ç 
â”‚   â””â”€â”€ pom.xml
â”œâ”€â”€ jlama-net/           # ç½‘ç»œæœåŠ¡
â”‚   â”œâ”€â”€ src/main/java/
â”‚   â”‚   â”œâ”€â”€ api/         # REST API
â”‚   â”‚   â””â”€â”€ cluster/     # åˆ†å¸ƒå¼åè°ƒ
â”‚   â””â”€â”€ pom.xml
â”œâ”€â”€ jlama-cli/           # å‘½ä»¤è¡Œå·¥å…·
â”‚   â””â”€â”€ pom.xml
â”œâ”€â”€ jlama-tests/         # æµ‹è¯•å¥—ä»¶
â”‚   â””â”€â”€ pom.xml
â””â”€â”€ pom.xml              # çˆ¶ POM
```

## è·¯çº¿å›¾

Jlama çš„æœªæ¥å‘å±•æ–¹å‘ï¼š

```mermaid
timeline
    title Jlama å‘å±•è·¯çº¿å›¾
    2024 Q4 : æ ¸å¿ƒåŠŸèƒ½å®Œå–„
           : é‡åŒ–æ”¯æŒ
           : åŸºç¡€ REST API
    2025 Q1 : åˆ†å¸ƒå¼æ¨ç†
           : Langchain4j é›†æˆ
           : æ€§èƒ½ä¼˜åŒ–
    2025 Q2 : LoRA æ”¯æŒ
           : æ›´å¤šæ¨¡å‹æ”¯æŒ
           : GPU åŠ é€Ÿ
    2025 Q3 : GraalVM åŸç”Ÿé•œåƒ
           : è¾¹ç¼˜è®¾å¤‡ä¼˜åŒ–
           : ä¼ä¸šçº§ç‰¹æ€§
```

### è®¡åˆ’ä¸­çš„åŠŸèƒ½

- âœ… æ”¯æŒæ›´å¤šæ¨¡å‹æ¶æ„
- âœ… çº¯ Java tokenizer
- âœ… é‡åŒ–æ”¯æŒï¼ˆQ4, Q8ï¼‰
- â³ LoRA å¾®è°ƒæ”¯æŒ
- â³ GraalVM Native Image æ”¯æŒ
- â³ åˆ†å¸ƒå¼æ¨ç†ä¼˜åŒ–
- ğŸ“‹ GPU åŠ é€Ÿï¼ˆCUDAï¼‰
- ğŸ“‹ æ›´å¤šé‡åŒ–ç®—æ³•
- ğŸ“‹ æ¨¡å‹å‹ç¼©æŠ€æœ¯

## ç¤¾åŒºå’Œæ”¯æŒ

### è·å–å¸®åŠ©

- **GitHub Issues**: [https://github.com/tjake/Jlama/issues](https://github.com/tjake/Jlama/issues)
- **è®¨è®ºåŒº**: [https://github.com/tjake/Jlama/discussions](https://github.com/tjake/Jlama/discussions)
- **Star é¡¹ç›®**: â­ [ç»™é¡¹ç›®åŠ æ˜Ÿ](https://github.com/tjake/Jlama)

### ç›¸å…³èµ„æº

- **å®˜æ–¹ä»“åº“**: [https://github.com/tjake/Jlama](https://github.com/tjake/Jlama)
- **æ¨¡å‹ä»“åº“**: [https://huggingface.co/tjake](https://huggingface.co/tjake)
- **Langchain4j**: [https://github.com/langchain4j/langchain4j](https://github.com/langchain4j/langchain4j)
- **å¼€å‘æŒ‡å—**: [DEVELOPER_GUIDE.md](https://github.com/tjake/Jlama/blob/main/DEVELOPER_GUIDE.md)

## æ€»ç»“

Jlama ä¸º Java ç”Ÿæ€å¸¦æ¥äº†å¼ºå¤§çš„æœ¬åœ° LLM æ¨ç†èƒ½åŠ›ï¼š

### æ ¸å¿ƒä¼˜åŠ¿

1. **çº¯ Java å®ç°** - æ— éœ€é¢å¤–çš„ Python è¿è¡Œæ—¶
2. **é«˜æ€§èƒ½** - åˆ©ç”¨ Java 21 å‘é‡ API å’Œ SIMD ä¼˜åŒ–
3. **æ˜“äºé›†æˆ** - ä¸ç°æœ‰ Java åº”ç”¨æ— ç¼é›†æˆ
4. **ç”Ÿäº§å°±ç»ª** - æ”¯æŒ REST APIã€åˆ†å¸ƒå¼æ¨ç†ã€å®¹å™¨åŒ–éƒ¨ç½²
5. **ç”Ÿæ€å‹å¥½** - Langchain4j é›†æˆã€OpenAI å…¼å®¹ API

### é€‚ç”¨åœºæ™¯

- **ä¼ä¸šåº”ç”¨** - åœ¨ Java ä¼ä¸šåº”ç”¨ä¸­é›†æˆ AI èƒ½åŠ›
- **å¾®æœåŠ¡æ¶æ„** - ä½œä¸ºç‹¬ç«‹çš„æ¨ç†æœåŠ¡éƒ¨ç½²
- **è¾¹ç¼˜è®¡ç®—** - åœ¨èµ„æºå—é™è®¾å¤‡ä¸Šè¿è¡Œ
- **ç§æœ‰åŒ–éƒ¨ç½²** - æœ¬åœ°éƒ¨ç½²ï¼Œä¿æŠ¤æ•°æ®éšç§
- **AI åº”ç”¨å¼€å‘** - å¿«é€ŸåŸå‹å’Œäº§å“å¼€å‘

### å¼€å§‹ä½¿ç”¨

```bash
# 1. ä¸‹è½½ Jlama
curl -L https://github.com/tjake/Jlama/releases/latest/download/jlama-cli.jar -o jlama-cli.jar

# 2. å¯ç”¨ Java é¢„è§ˆç‰¹æ€§
export JDK_JAVA_OPTIONS="--add-modules jdk.incubator.vector --enable-preview"

# 3. å¯åŠ¨ REST API æœåŠ¡
java -jar jlama-cli.jar restapi tjake/Llama-3.2-1B-Instruct-JQ4

# 4. å¼€å§‹ä½¿ç”¨ï¼
curl -X POST http://localhost:8080/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{"model":"tjake/Llama-3.2-1B-Instruct-JQ4","messages":[{"role":"user","content":"Hello!"}]}'
```

é€šè¿‡ Jlamaï¼ŒJava å¼€å‘è€…å¯ä»¥è½»æ¾åœ°åœ¨è‡ªå·±ç†Ÿæ‚‰çš„æŠ€æœ¯æ ˆä¸­æ„å»ºå¼ºå¤§çš„ AI åº”ç”¨ï¼Œæ— éœ€å­¦ä¹ æ–°çš„è¯­è¨€æˆ–æ¡†æ¶ã€‚ç«‹å³å¼€å§‹ä½ çš„ Java AI ä¹‹æ—…å§ï¼

---

**å¼•ç”¨å’Œå‚è€ƒ**

å¦‚æœä½ åœ¨ç ”ç©¶æˆ–é¡¹ç›®ä¸­ä½¿ç”¨äº† Jlamaï¼Œè¯·å¼•ç”¨ï¼š

```bibtex
@misc{jlama2024,
    title = {Jlama: A modern Java inference engine for large language models},
    url = {https://github.com/tjake/jlama},
    author = {T Jake Luciani},
    month = {January},
    year = {2024}
}
```

## License

Jlama é‡‡ç”¨ Apache License 2.0 å¼€æºåè®®ã€‚
