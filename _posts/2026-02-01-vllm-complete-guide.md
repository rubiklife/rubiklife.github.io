---
title: "vLLM å®Œæ•´æŒ‡å—ï¼šé«˜æ€§èƒ½å¤§è¯­è¨€æ¨¡å‹æ¨ç†ä¸æœåŠ¡å¼•æ“"
date: 2026-02-01T10:00:00+08:00
categories:
  - AIå·¥å…·
tags:
  - LLM
  - æ¨ç†å¼•æ“
toc: true
mermaid: true
---

## vLLM ç®€ä»‹

vLLM æ˜¯ä¸€ä¸ªé«˜ååé‡ã€å†…å­˜é«˜æ•ˆçš„å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¨ç†å’ŒæœåŠ¡å¼•æ“ï¼Œæœ€åˆç”±åŠ å·å¤§å­¦ä¼¯å…‹åˆ©åˆ†æ ¡ Sky Computing Lab å¼€å‘ï¼Œç°å·²å‘å±•æˆä¸ºç¤¾åŒºé©±åŠ¨çš„å¼€æºé¡¹ç›®ã€‚vLLM ä»¥å…¶å“è¶Šçš„æ€§èƒ½å’Œæ˜“ç”¨æ€§ï¼Œæˆä¸º LLM éƒ¨ç½²çš„é¦–é€‰æ–¹æ¡ˆä¹‹ä¸€ã€‚

```mermaid
mindmap
  root((vLLM))
    æ ¸å¿ƒæŠ€æœ¯
      PagedAttention
      è¿ç»­æ‰¹å¤„ç†
      CUDA/HIP å›¾ä¼˜åŒ–
      FlashAttention é›†æˆ
      æŠ•æœºè§£ç 
      åˆ†å—é¢„å¡«å……
    æ€§èƒ½ä¼˜åŠ¿
      æœ€å…ˆè¿›çš„æœåŠ¡ååé‡
      é«˜æ•ˆå†…å­˜ç®¡ç†
      å¿«é€Ÿæ¨¡å‹æ‰§è¡Œ
      æµå¼è¾“å‡º
    å¤šæ ·åŒ–æ”¯æŒ
      é‡åŒ–æ”¯æŒ
        GPTQ
        AWQ
        INT4/INT8/FP8
        AutoRound
      ç¡¬ä»¶æ”¯æŒ
        NVIDIA GPU
        AMD CPU/GPU
        Intel CPU/GPU
        TPU
        ARM CPU
      æ¨¡å‹ç±»å‹
        Transformer LLMs
        MoE æ¨¡å‹
        å¤šæ¨¡æ€æ¨¡å‹
        åµŒå…¥æ¨¡å‹
    æ˜“ç”¨æ€§
      HuggingFace é›†æˆ
      OpenAI API å…¼å®¹
      åˆ†å¸ƒå¼æ¨ç†
      LoRA æ”¯æŒ
      å‰ç¼€ç¼“å­˜
```

### ä¸ºä»€ä¹ˆé€‰æ‹© vLLMï¼Ÿ

vLLM è§£å†³äº†ä¼ ç»Ÿ LLM æ¨ç†æœåŠ¡çš„è¯¸å¤šç—›ç‚¹ï¼š

```mermaid
graph TB
    subgraph "ä¼ ç»Ÿæ¨ç†å¼•æ“çš„æŒ‘æˆ˜"
        A1[å†…å­˜æ•ˆç‡ä½ä¸‹] --> B1[KV Cache æµªè´¹]
        A2[ååé‡å—é™] --> B2[æ‰¹å¤„ç†ä¸è¿ç»­]
        A3[ç¡¬ä»¶åˆ©ç”¨ç‡ä½] --> B3[GPU ç©ºé—²æ—¶é—´é•¿]
        A4[éƒ¨ç½²å¤æ‚] --> B4[å…¼å®¹æ€§å·®]
    end
    
    subgraph "vLLMçš„è§£å†³æ–¹æ¡ˆ"
        C1[PagedAttention] --> D1[å†…å­˜åˆ©ç”¨ç‡æå‡]
        C2[è¿ç»­æ‰¹å¤„ç†] --> D2[ååé‡æœ€å¤§åŒ–]
        C3[CUDAå›¾ä¼˜åŒ–] --> D3[GPUåˆ©ç”¨ç‡æå‡]
        C4[OpenAI API] --> D4[æ— ç¼è¿ç§»]
    end
    
    B1 --> D1
    B2 --> D2
    B3 --> D3
    B4 --> D4
    
    style A1 fill:#ff6b6b
    style A2 fill:#ff6b6b
    style A3 fill:#ff6b6b
    style A4 fill:#ff6b6b
    style C1 fill:#51cf66
    style C2 fill:#51cf66
    style C3 fill:#51cf66
    style C4 fill:#51cf66
```

## æ ¸å¿ƒæŠ€æœ¯åŸç†

### PagedAttention æœºåˆ¶

PagedAttention æ˜¯ vLLM çš„æ ¸å¿ƒåˆ›æ–°ï¼Œå®ƒå€Ÿé‰´äº†æ“ä½œç³»ç»Ÿè™šæ‹Ÿå†…å­˜çš„æ€æƒ³ï¼Œå°† KV Cache åˆ†é¡µç®¡ç†ï¼š

```mermaid
graph LR
    subgraph "ä¼ ç»Ÿ KV Cache"
        A1[Request 1<br/>è¿ç»­å†…å­˜å—] --> B1[å¤§é‡å†…å­˜ç¢ç‰‡]
        A2[Request 2<br/>è¿ç»­å†…å­˜å—] --> B1
        A3[Request 3<br/>è¿ç»­å†…å­˜å—] --> B1
    end
    
    subgraph "PagedAttention"
        C1[Request 1<br/>é¡µè¡¨] --> D1[ç‰©ç†é¡µ 1]
        C1 --> D2[ç‰©ç†é¡µ 2]
        C2[Request 2<br/>é¡µè¡¨] --> D3[ç‰©ç†é¡µ 3]
        C2 --> D4[ç‰©ç†é¡µ 4]
        C3[Request 3<br/>é¡µè¡¨] --> D2
        C3 --> D5[ç‰©ç†é¡µ 5]
        
        D1 -.å…±äº«é¡µ.-> D2
    end
    
    B1 --> E1[å†…å­˜æµªè´¹<br/>æœ€é«˜å¯è¾¾60%]
    D5 --> E2[æ¥è¿‘é›¶æµªè´¹<br/>å…±äº«æå‡æ•ˆç‡]
    
    style A1 fill:#ff6b6b
    style A2 fill:#ff6b6b
    style A3 fill:#ff6b6b
    style C1 fill:#51cf66
    style C2 fill:#51cf66
    style C3 fill:#51cf66
    style E1 fill:#ff6b6b
    style E2 fill:#51cf66
```

**PagedAttention ä¼˜åŠ¿ï¼š**

- **å†…å­˜æ•ˆç‡**ï¼šå°†å†…å­˜æµªè´¹ä» 60% é™ä½åˆ°æ¥è¿‘ 0%
- **å…±äº«æœºåˆ¶**ï¼šç›¸åŒå‰ç¼€çš„è¯·æ±‚å¯ä»¥å…±äº«å†…å­˜é¡µ
- **åŠ¨æ€åˆ†é…**ï¼šæŒ‰éœ€åˆ†é…å’Œé‡Šæ”¾å†…å­˜é¡µ
- **çµæ´»æ€§**ï¼šæ”¯æŒå¯å˜é•¿åº¦çš„åºåˆ—

### è¿ç»­æ‰¹å¤„ç†æ¶æ„

vLLM å®ç°äº†çœŸæ­£çš„è¿ç»­æ‰¹å¤„ç†ï¼ˆContinuous Batchingï¼‰ï¼Œæ— éœ€ç­‰å¾…æ•´ä¸ªæ‰¹æ¬¡å®Œæˆï¼š

```mermaid
sequenceDiagram
    participant Client as å®¢æˆ·ç«¯
    participant Scheduler as è°ƒåº¦å™¨
    participant Engine as æ¨ç†å¼•æ“
    participant KVCache as KV Cache
    
    Client->>Scheduler: è¯·æ±‚ 1 åˆ°è¾¾
    Scheduler->>Engine: å¼€å§‹å¤„ç†è¯·æ±‚ 1
    Engine->>KVCache: åˆ†é…é¡µè¡¨
    
    Client->>Scheduler: è¯·æ±‚ 2 åˆ°è¾¾
    Scheduler->>Engine: åŠ¨æ€åŠ å…¥æ‰¹æ¬¡
    Engine->>KVCache: åˆ†é…æ–°é¡µè¡¨
    
    Engine->>Engine: å¹¶è¡Œç”Ÿæˆ Token
    
    Engine->>Client: è¯·æ±‚ 1 å®Œæˆ
    Engine->>KVCache: é‡Šæ”¾è¯·æ±‚ 1 é¡µè¡¨
    
    Client->>Scheduler: è¯·æ±‚ 3 åˆ°è¾¾
    Scheduler->>Engine: å¡«è¡¥æ‰¹æ¬¡ç©ºä½
    
    Engine->>Client: è¯·æ±‚ 2 å®Œæˆ
    Engine->>Client: è¯·æ±‚ 3 å®Œæˆ
    
    Note over Scheduler,Engine: æ‰¹æ¬¡å¤§å°åŠ¨æ€å˜åŒ–<br/>æ— éœ€ç­‰å¾…æ‰€æœ‰è¯·æ±‚å®Œæˆ
```

## å®‰è£…éƒ¨ç½²

### ç¯å¢ƒè¦æ±‚

```mermaid
graph TD
    A[vLLM å®‰è£…] --> B{ç¡¬ä»¶é€‰æ‹©}
    B -->|æ¨è| C[NVIDIA GPU]
    B --> D[AMD GPU]
    B --> E[Intel GPU/CPU]
    B --> F[TPU]
    
    C --> C1[CUDA 12.1+]
    C --> C2[è®¡ç®—èƒ½åŠ› 7.0+]
    C --> C3[è‡³å°‘ 16GB æ˜¾å­˜]
    
    D --> D1[ROCm 6.0+]
    E --> E1[XPU é©±åŠ¨]
    F --> F1[Cloud TPU v3+]
    
    A --> G[Python ç¯å¢ƒ]
    G --> G1[Python 3.8-3.12]
    G --> G2[PyTorch 2.0+]
    
    style A fill:#4dabf7
    style C fill:#51cf66
```

### å¿«é€Ÿå®‰è£…

**æ–¹æ³•ä¸€ï¼šä½¿ç”¨ pip å®‰è£…ï¼ˆæ¨èï¼‰**

```bash
# åŸºç¡€å®‰è£…
pip install vllm

# æŒ‡å®š CUDA ç‰ˆæœ¬
pip install vllm --extra-index-url https://download.pytorch.org/whl/cu121

# ä»æºç å®‰è£…ï¼ˆè·å–æœ€æ–°åŠŸèƒ½ï¼‰
pip install git+https://github.com/vllm-project/vllm.git
```

**æ–¹æ³•äºŒï¼šä½¿ç”¨ Docker**

```bash
# æ‹‰å–å®˜æ–¹é•œåƒ
docker pull vllm/vllm-openai:latest

# è¿è¡Œå®¹å™¨
docker run --runtime nvidia --gpus all \
    -v ~/.cache/huggingface:/root/.cache/huggingface \
    -p 8000:8000 \
    --ipc=host \
    vllm/vllm-openai:latest \
    --model meta-llama/Llama-2-7b-chat-hf
```

**æ–¹æ³•ä¸‰ï¼šä½¿ç”¨ Conda**

```bash
# åˆ›å»ºç¯å¢ƒ
conda create -n vllm python=3.11 -y
conda activate vllm

# å®‰è£…ä¾èµ–
conda install pytorch pytorch-cuda=12.1 -c pytorch -c nvidia
pip install vllm
```

### éªŒè¯å®‰è£…

```python
import vllm
print(f"vLLM ç‰ˆæœ¬: {vllm.__version__}")

# æ£€æŸ¥ GPU å¯ç”¨æ€§
from vllm import LLM
print("vLLM å®‰è£…æˆåŠŸï¼")
```

## å¿«é€Ÿå¼€å§‹

### åŸºç¡€æ¨ç†ç¤ºä¾‹

**ç¦»çº¿æ‰¹é‡æ¨ç†**

```python
from vllm import LLM, SamplingParams

# åˆå§‹åŒ–æ¨¡å‹
llm = LLM(
    model="meta-llama/Llama-2-7b-chat-hf",
    trust_remote_code=True,
    tensor_parallel_size=1  # GPU æ•°é‡
)

# è®¾ç½®é‡‡æ ·å‚æ•°
sampling_params = SamplingParams(
    temperature=0.7,
    top_p=0.95,
    top_k=50,
    max_tokens=256,
    stop=["</s>"]
)

# å‡†å¤‡æç¤ºè¯
prompts = [
    "ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ",
    "è§£é‡Šä¸€ä¸‹æœºå™¨å­¦ä¹ å’Œæ·±åº¦å­¦ä¹ çš„åŒºåˆ«ã€‚",
    "Python çš„ä¸»è¦ç‰¹ç‚¹æ˜¯ä»€ä¹ˆï¼Ÿ"
]

# æ‰¹é‡ç”Ÿæˆ
outputs = llm.generate(prompts, sampling_params)

# å¤„ç†è¾“å‡º
for output in outputs:
    prompt = output.prompt
    generated_text = output.outputs[0].text
    print(f"æç¤ºè¯: {prompt}")
    print(f"ç”Ÿæˆ: {generated_text}\n")
```

### åœ¨çº¿æœåŠ¡éƒ¨ç½²

**å¯åŠ¨ OpenAI å…¼å®¹çš„ API æœåŠ¡å™¨**

```bash
# åŸºç¡€å¯åŠ¨
python -m vllm.entrypoints.openai.api_server \
    --model meta-llama/Llama-2-7b-chat-hf \
    --port 8000

# é«˜çº§é…ç½®
python -m vllm.entrypoints.openai.api_server \
    --model meta-llama/Llama-2-7b-chat-hf \
    --host 0.0.0.0 \
    --port 8000 \
    --tensor-parallel-size 2 \
    --dtype float16 \
    --max-model-len 4096 \
    --gpu-memory-utilization 0.9 \
    --enable-prefix-caching
```

**å®¢æˆ·ç«¯è°ƒç”¨ç¤ºä¾‹**

```python
from openai import OpenAI

# è¿æ¥åˆ° vLLM æœåŠ¡å™¨
client = OpenAI(
    api_key="EMPTY",  # vLLM ä¸éœ€è¦ API key
    base_url="http://localhost:8000/v1"
)

# Chat Completions API
response = client.chat.completions.create(
    model="meta-llama/Llama-2-7b-chat-hf",
    messages=[
        {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæœ‰å¸®åŠ©çš„AIåŠ©æ‰‹ã€‚"},
        {"role": "user", "content": "è¯·ä»‹ç»ä¸€ä¸‹ vLLM çš„ä¼˜åŠ¿ã€‚"}
    ],
    temperature=0.7,
    max_tokens=512,
    stream=True  # å¯ç”¨æµå¼è¾“å‡º
)

# å¤„ç†æµå¼å“åº”
for chunk in response:
    if chunk.choices[0].delta.content:
        print(chunk.choices[0].delta.content, end="", flush=True)
```

### æµç¨‹ç¤ºæ„å›¾

```mermaid
flowchart TD
    A[å¼€å§‹] --> B{éƒ¨ç½²æ–¹å¼}
    
    B -->|ç¦»çº¿æ¨ç†| C[å¯¼å…¥ LLM ç±»]
    C --> D[é…ç½®æ¨¡å‹å‚æ•°]
    D --> E[è®¾ç½®é‡‡æ ·å‚æ•°]
    E --> F[æ‰¹é‡ç”Ÿæˆæ–‡æœ¬]
    F --> G[å¤„ç†è¾“å‡ºç»“æœ]
    
    B -->|åœ¨çº¿æœåŠ¡| H[å¯åŠ¨ API Server]
    H --> I[é…ç½®æœåŠ¡å‚æ•°]
    I --> J[å®¢æˆ·ç«¯è¿æ¥]
    J --> K[å‘é€è¯·æ±‚]
    K --> L{æµå¼è¾“å‡º?}
    L -->|æ˜¯| M[é€å—æ¥æ”¶]
    L -->|å¦| N[ä¸€æ¬¡æ€§æ¥æ”¶]
    
    M --> O[å¤„ç†å®Œæˆ]
    N --> O
    G --> O
    
    style C fill:#4dabf7
    style H fill:#4dabf7
    style O fill:#51cf66
```

## é«˜çº§ç‰¹æ€§

### 1. åˆ†å¸ƒå¼æ¨ç†

vLLM æ”¯æŒå¤šç§å¹¶è¡Œç­–ç•¥ï¼Œå®ç°å¤§æ¨¡å‹çš„é«˜æ•ˆæ¨ç†ï¼š

```mermaid
graph TB
    subgraph "å¹¶è¡Œç­–ç•¥"
        A[vLLM åˆ†å¸ƒå¼æ¨ç†]
        
        A --> B[å¼ é‡å¹¶è¡Œ<br/>Tensor Parallel]
        A --> C[æµæ°´çº¿å¹¶è¡Œ<br/>Pipeline Parallel]
        A --> D[æ•°æ®å¹¶è¡Œ<br/>Data Parallel]
        A --> E[ä¸“å®¶å¹¶è¡Œ<br/>Expert Parallel]
        
        B --> B1[åŒä¸€å±‚åˆ†å¸ƒåˆ°å¤š GPU]
        B --> B2[é€‚åˆå•èŠ‚ç‚¹å¤šå¡]
        
        C --> C1[ä¸åŒå±‚åˆ†å¸ƒåˆ°å¤š GPU]
        C --> C2[é€‚åˆè¶…å¤§æ¨¡å‹]
        
        D --> D1[å®Œæ•´å‰¯æœ¬åœ¨å¤š GPU]
        D --> D2[æå‡ååé‡]
        
        E --> E1[MoE ä¸“å®¶åˆ†å¸ƒ]
        E --> E2[é€‚åˆ Mixtral ç­‰]
    end
    
    style A fill:#4dabf7
    style B fill:#51cf66
    style C fill:#51cf66
    style D fill:#51cf66
    style E fill:#51cf66
```

**å¼ é‡å¹¶è¡Œç¤ºä¾‹**

```python
from vllm import LLM, SamplingParams

# ä½¿ç”¨ 4 ä¸ª GPU è¿›è¡Œå¼ é‡å¹¶è¡Œ
llm = LLM(
    model="meta-llama/Llama-2-70b-chat-hf",
    tensor_parallel_size=4,  # 4 è·¯å¼ é‡å¹¶è¡Œ
    dtype="float16",
    max_model_len=4096
)

prompts = ["ä½ çš„æç¤ºè¯"]
sampling_params = SamplingParams(temperature=0.7, max_tokens=256)
outputs = llm.generate(prompts, sampling_params)
```

**æµæ°´çº¿å¹¶è¡Œç¤ºä¾‹**

```python
# ä½¿ç”¨ 8 ä¸ª GPUï¼š2 è·¯æµæ°´çº¿ Ã— 4 è·¯å¼ é‡å¹¶è¡Œ
llm = LLM(
    model="meta-llama/Llama-2-70b-chat-hf",
    tensor_parallel_size=4,
    pipeline_parallel_size=2,  # 2 è·¯æµæ°´çº¿å¹¶è¡Œ
    dtype="float16"
)
```

**API æœåŠ¡å™¨åˆ†å¸ƒå¼éƒ¨ç½²**

```bash
# å¯åŠ¨åˆ†å¸ƒå¼æœåŠ¡
python -m vllm.entrypoints.openai.api_server \
    --model meta-llama/Llama-2-70b-chat-hf \
    --tensor-parallel-size 4 \
    --pipeline-parallel-size 2 \
    --port 8000
```

### 2. é‡åŒ–åŠ é€Ÿ

vLLM æ”¯æŒå¤šç§é‡åŒ–æ–¹æ³•ï¼Œå‡å°‘æ˜¾å­˜å ç”¨å¹¶æå‡æ¨ç†é€Ÿåº¦ï¼š

```mermaid
graph LR
    A[é‡åŒ–æ–¹æ³•] --> B[GPTQ]
    A --> C[AWQ]
    A --> D[INT4/INT8]
    A --> E[FP8]
    A --> F[AutoRound]
    
    B --> B1[4-bit æƒé‡é‡åŒ–]
    B --> B2[TheBloke é¢„é‡åŒ–æ¨¡å‹]
    
    C --> C1[æ¿€æ´»æ„ŸçŸ¥æƒé‡é‡åŒ–]
    C --> C2[ç²¾åº¦æŸå¤±æ›´å°]
    
    D --> D1[æ•´æ•°é‡åŒ–]
    D --> D2[æè‡´å‹ç¼©]
    
    E --> E1[8-bit æµ®ç‚¹]
    E --> E2[H100/Ada æ¶æ„ä¼˜åŒ–]
    
    F --> F1[è‡ªåŠ¨æƒé‡èˆå…¥]
    F --> F2[å¹³è¡¡ç²¾åº¦ä¸æ€§èƒ½]
    
    style A fill:#4dabf7
    style B fill:#51cf66
    style C fill:#51cf66
    style D fill:#51cf66
    style E fill:#51cf66
    style F fill:#51cf66
```

**GPTQ é‡åŒ–ç¤ºä¾‹**

```python
from vllm import LLM, SamplingParams

# åŠ è½½ GPTQ é‡åŒ–æ¨¡å‹
llm = LLM(
    model="TheBloke/Llama-2-70B-Chat-GPTQ",
    quantization="gptq",
    dtype="float16",
    gpu_memory_utilization=0.9
)

prompts = ["ä½ çš„æç¤ºè¯"]
outputs = llm.generate(prompts, SamplingParams(temperature=0.7))
```

**AWQ é‡åŒ–ç¤ºä¾‹**

```python
# åŠ è½½ AWQ é‡åŒ–æ¨¡å‹
llm = LLM(
    model="TheBloke/Llama-2-70B-Chat-AWQ",
    quantization="awq",
    dtype="float16"
)
```

**FP8 é‡åŒ–ç¤ºä¾‹**

```python
# ä½¿ç”¨ FP8 é‡åŒ–ï¼ˆéœ€è¦ H100 æˆ– Ada æ¶æ„ GPUï¼‰
llm = LLM(
    model="meta-llama/Llama-2-70b-chat-hf",
    quantization="fp8",
    dtype="float16"
)
```

### 3. LoRA é€‚é…å™¨æ”¯æŒ

vLLM æ”¯æŒåŠ¨æ€åŠ è½½å¤šä¸ª LoRA é€‚é…å™¨ï¼Œæ— éœ€é‡å¯æœåŠ¡ï¼š

```python
from vllm import LLM, SamplingParams
from vllm.lora.request import LoRARequest

# åˆå§‹åŒ–åŸºç¡€æ¨¡å‹
llm = LLM(
    model="meta-llama/Llama-2-7b-hf",
    enable_lora=True,
    max_lora_rank=64,
    max_loras=8  # æœ€å¤šåŒæ—¶åŠ è½½ 8 ä¸ª LoRA
)

# å‡†å¤‡ä¸åŒçš„ LoRA é€‚é…å™¨è¯·æ±‚
lora_request_1 = LoRARequest(
    lora_name="chinese_adapter",
    lora_int_id=1,
    lora_local_path="/path/to/chinese_lora"
)

lora_request_2 = LoRARequest(
    lora_name="math_adapter",
    lora_int_id=2,
    lora_local_path="/path/to/math_lora"
)

# ä½¿ç”¨ä¸åŒçš„ LoRA ç”Ÿæˆ
outputs = llm.generate(
    ["ä½ å¥½ï¼Œä¸–ç•Œï¼"],
    SamplingParams(temperature=0.7),
    lora_request=lora_request_1
)
```

**LoRA API æœåŠ¡å™¨**

```bash
# å¯åŠ¨æ”¯æŒ LoRA çš„æœåŠ¡å™¨
python -m vllm.entrypoints.openai.api_server \
    --model meta-llama/Llama-2-7b-hf \
    --enable-lora \
    --lora-modules chinese=/path/to/chinese_lora \
                   math=/path/to/math_lora \
    --max-lora-rank 64
```

```python
# å®¢æˆ·ç«¯è°ƒç”¨
from openai import OpenAI

client = OpenAI(base_url="http://localhost:8000/v1", api_key="EMPTY")

response = client.chat.completions.create(
    model="chinese",  # ä½¿ç”¨ chinese LoRA
    messages=[{"role": "user", "content": "ä½ å¥½"}]
)
```

### 4. å‰ç¼€ç¼“å­˜

å‰ç¼€ç¼“å­˜å¯ä»¥æ˜¾è‘—æå‡ç›¸åŒå‰ç¼€è¯·æ±‚çš„å¤„ç†é€Ÿåº¦ï¼š

```mermaid
sequenceDiagram
    participant Client as å®¢æˆ·ç«¯
    participant Cache as å‰ç¼€ç¼“å­˜
    participant Engine as æ¨ç†å¼•æ“
    
    Client->>Cache: è¯·æ±‚ 1: ç³»ç»Ÿæç¤º + é—®é¢˜ A
    Cache->>Cache: æœªå‘½ä¸­ç¼“å­˜
    Cache->>Engine: å®Œæ•´å¤„ç†
    Engine->>Cache: ç¼“å­˜ç³»ç»Ÿæç¤ºçš„ KV
    Cache->>Client: è¿”å›ç»“æœ
    
    Client->>Cache: è¯·æ±‚ 2: ç›¸åŒç³»ç»Ÿæç¤º + é—®é¢˜ B
    Cache->>Cache: å‘½ä¸­å‰ç¼€ç¼“å­˜
    Cache->>Engine: ä»…å¤„ç†é—®é¢˜ B
    Engine->>Client: å¿«é€Ÿè¿”å›ç»“æœ
    
    Note over Cache,Engine: æ€§èƒ½æå‡ï¼š<br/>- å‡å°‘è®¡ç®—é‡ 50-90%<br/>- é™ä½å»¶è¿Ÿ 40-80%<br/>- æå‡ååé‡ 2-5x
```

**å¯ç”¨å‰ç¼€ç¼“å­˜**

```bash
# API æœåŠ¡å™¨å¯ç”¨å‰ç¼€ç¼“å­˜
python -m vllm.entrypoints.openai.api_server \
    --model meta-llama/Llama-2-7b-chat-hf \
    --enable-prefix-caching \
    --port 8000
```

```python
# Python API å¯ç”¨å‰ç¼€ç¼“å­˜
llm = LLM(
    model="meta-llama/Llama-2-7b-chat-hf",
    enable_prefix_caching=True
)
```

### 5. å¤šæ¨¡æ€æ¨¡å‹æ”¯æŒ

vLLM æ”¯æŒè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ï¼š

```python
from vllm import LLM, SamplingParams

# åŠ è½½å¤šæ¨¡æ€æ¨¡å‹ï¼ˆå¦‚ LLaVAï¼‰
llm = LLM(
    model="llava-hf/llava-1.5-7b-hf",
    trust_remote_code=True
)

# å‡†å¤‡å›¾åƒå’Œæ–‡æœ¬è¾“å…¥
prompts = [
    {
        "prompt": "USER: <image>\nè¿™å¼ å›¾ç‰‡é‡Œæœ‰ä»€ä¹ˆï¼Ÿ\nASSISTANT:",
        "multi_modal_data": {
            "image": "https://example.com/image.jpg"
        }
    }
]

outputs = llm.generate(prompts, SamplingParams(temperature=0.7, max_tokens=256))
for output in outputs:
    print(output.outputs[0].text)
```

### 6. æŠ•æœºè§£ç 

æŠ•æœºè§£ç ï¼ˆSpeculative Decodingï¼‰é€šè¿‡å°æ¨¡å‹é¢„æµ‹åŠ é€Ÿå¤§æ¨¡å‹ç”Ÿæˆï¼š

```mermaid
graph LR
    A[ç›®æ ‡å¤§æ¨¡å‹<br/>70B] --> B[æ­£å¸¸ç”Ÿæˆé€Ÿåº¦]
    
    C[æŠ•æœºè§£ç ] --> D[è‰ç¨¿å°æ¨¡å‹<br/>7B]
    C --> E[ç›®æ ‡å¤§æ¨¡å‹<br/>70B]
    
    D --> F[å¿«é€Ÿç”Ÿæˆå¤šä¸ª Token]
    F --> E
    E --> G[å¹¶è¡ŒéªŒè¯]
    G --> H[æ¥å—æ­£ç¡®çš„ Token]
    
    H --> I[2-3x åŠ é€Ÿ]
    
    style B fill:#ff6b6b
    style I fill:#51cf66
```

```python
# ä½¿ç”¨æŠ•æœºè§£ç 
llm = LLM(
    model="meta-llama/Llama-2-70b-chat-hf",
    speculative_model="meta-llama/Llama-2-7b-chat-hf",  # è‰ç¨¿æ¨¡å‹
    num_speculative_tokens=5,  # æ¯æ¬¡é¢„æµ‹ 5 ä¸ª token
    use_v2_block_manager=True
)
```

## æ€§èƒ½ä¼˜åŒ–å®è·µ

### å‚æ•°è°ƒä¼˜æŒ‡å—

```mermaid
graph TD
    A[æ€§èƒ½ä¼˜åŒ–] --> B[å†…å­˜ä¼˜åŒ–]
    A --> C[ååé‡ä¼˜åŒ–]
    A --> D[å»¶è¿Ÿä¼˜åŒ–]
    
    B --> B1[--gpu-memory-utilization]
    B --> B2[--max-model-len]
    B --> B3[--enable-chunked-prefill]
    
    C --> C1[--max-num-seqs]
    C --> C2[--max-num-batched-tokens]
    C --> C3[è¿ç»­æ‰¹å¤„ç†]
    
    D --> D1[--enable-prefix-caching]
    D --> D2[æŠ•æœºè§£ç ]
    D --> D3[é‡åŒ–]
    
    style A fill:#4dabf7
    style B fill:#51cf66
    style C fill:#51cf66
    style D fill:#51cf66
```

**å…³é”®å‚æ•°è¯´æ˜**

```bash
python -m vllm.entrypoints.openai.api_server \
    --model meta-llama/Llama-2-70b-chat-hf \
    \
    # GPU å†…å­˜åˆ©ç”¨ç‡ï¼ˆé»˜è®¤ 0.9ï¼‰
    --gpu-memory-utilization 0.95 \
    \
    # æœ€å¤§æ¨¡å‹é•¿åº¦ï¼ˆè¶Šå°å†…å­˜å ç”¨è¶Šå°‘ï¼‰
    --max-model-len 4096 \
    \
    # æœ€å¤§å¹¶å‘åºåˆ—æ•°ï¼ˆå½±å“ååé‡ï¼‰
    --max-num-seqs 256 \
    \
    # æœ€å¤§æ‰¹æ¬¡ token æ•°
    --max-num-batched-tokens 8192 \
    \
    # å¯ç”¨å‰ç¼€ç¼“å­˜
    --enable-prefix-caching \
    \
    # å¯ç”¨åˆ†å—é¢„å¡«å……
    --enable-chunked-prefill \
    \
    # å¼ é‡å¹¶è¡Œ
    --tensor-parallel-size 4 \
    \
    # æ•°æ®ç±»å‹
    --dtype float16
```

### æ€§èƒ½åŸºå‡†æµ‹è¯•

**ååé‡æµ‹è¯•**

```python
import time
from vllm import LLM, SamplingParams

llm = LLM(model="meta-llama/Llama-2-7b-chat-hf")
sampling_params = SamplingParams(temperature=0.7, max_tokens=256)

# å‡†å¤‡å¤§é‡è¯·æ±‚
prompts = ["æµ‹è¯•æç¤ºè¯ " + str(i) for i in range(1000)]

start_time = time.time()
outputs = llm.generate(prompts, sampling_params)
end_time = time.time()

total_tokens = sum(len(output.outputs[0].token_ids) for output in outputs)
throughput = total_tokens / (end_time - start_time)

print(f"ååé‡: {throughput:.2f} tokens/ç§’")
print(f"æ€»è€—æ—¶: {end_time - start_time:.2f} ç§’")
```

**å»¶è¿Ÿæµ‹è¯•**

```python
import time
from openai import OpenAI

client = OpenAI(base_url="http://localhost:8000/v1", api_key="EMPTY")

latencies = []
for i in range(100):
    start = time.time()
    response = client.chat.completions.create(
        model="meta-llama/Llama-2-7b-chat-hf",
        messages=[{"role": "user", "content": "ä½ å¥½"}],
        max_tokens=50
    )
    latency = time.time() - start
    latencies.append(latency)

print(f"å¹³å‡å»¶è¿Ÿ: {sum(latencies) / len(latencies):.3f} ç§’")
print(f"P50 å»¶è¿Ÿ: {sorted(latencies)[50]:.3f} ç§’")
print(f"P99 å»¶è¿Ÿ: {sorted(latencies)[99]:.3f} ç§’")
```

### ç›‘æ§ä¸è°ƒè¯•

**å¯ç”¨è¯¦ç»†æ—¥å¿—**

```bash
# è®¾ç½®æ—¥å¿—çº§åˆ«
export VLLM_LOGGING_LEVEL=DEBUG

# å¯ç”¨æ€§èƒ½åˆ†æ
export VLLM_PROFILING_ENABLED=1

python -m vllm.entrypoints.openai.api_server \
    --model meta-llama/Llama-2-7b-chat-hf \
    --log-level debug
```

**ç›‘æ§æŒ‡æ ‡**

```python
# è·å–æ¨¡å‹ç»Ÿè®¡ä¿¡æ¯
from vllm import LLM

llm = LLM(model="meta-llama/Llama-2-7b-chat-hf")

# æŸ¥çœ‹ GPU å†…å­˜ä½¿ç”¨
import torch
print(f"GPU å†…å­˜å·²åˆ†é…: {torch.cuda.memory_allocated() / 1024**3:.2f} GB")
print(f"GPU å†…å­˜ç¼“å­˜: {torch.cuda.memory_reserved() / 1024**3:.2f} GB")
```

## å®é™…åº”ç”¨åœºæ™¯

### åœºæ™¯ 1ï¼šèŠå¤©æœºå™¨äººæœåŠ¡

```mermaid
flowchart LR
    A[ç”¨æˆ·è¯·æ±‚] --> B[è´Ÿè½½å‡è¡¡å™¨]
    B --> C1[vLLM å®ä¾‹ 1]
    B --> C2[vLLM å®ä¾‹ 2]
    B --> C3[vLLM å®ä¾‹ N]
    
    C1 --> D[Redis ç¼“å­˜]
    C2 --> D
    C3 --> D
    
    D --> E[å“åº”è¿”å›]
    
    subgraph "vLLM é…ç½®"
        C1
        F[å‰ç¼€ç¼“å­˜å¯ç”¨]
        G[æµå¼è¾“å‡º]
        H[4-bit é‡åŒ–]
    end
    
    style B fill:#4dabf7
    style D fill:#ffd43b
    style E fill:#51cf66
```

**éƒ¨ç½²é…ç½®**

```bash
# å¯åŠ¨èŠå¤©æœåŠ¡
python -m vllm.entrypoints.openai.api_server \
    --model meta-llama/Llama-2-13b-chat-hf \
    --host 0.0.0.0 \
    --port 8000 \
    --enable-prefix-caching \
    --quantization awq \
    --max-model-len 4096 \
    --max-num-seqs 128
```

### åœºæ™¯ 2ï¼šæ‰¹é‡å†…å®¹ç”Ÿæˆ

```python
from vllm import LLM, SamplingParams
import json

# åŠ è½½æ¨¡å‹
llm = LLM(
    model="meta-llama/Llama-2-70b-chat-hf",
    tensor_parallel_size=4,
    dtype="float16"
)

# å‡†å¤‡æ‰¹é‡ä»»åŠ¡
with open("prompts.json", "r") as f:
    tasks = json.load(f)

# æ‰¹é‡ç”Ÿæˆ
sampling_params = SamplingParams(
    temperature=0.8,
    top_p=0.95,
    max_tokens=512,
    stop=["</s>", "\n\n"]
)

outputs = llm.generate([task["prompt"] for task in tasks], sampling_params)

# ä¿å­˜ç»“æœ
results = []
for task, output in zip(tasks, outputs):
    results.append({
        "id": task["id"],
        "prompt": task["prompt"],
        "generated": output.outputs[0].text
    })

with open("results.json", "w") as f:
    json.dump(results, f, ensure_ascii=False, indent=2)
```

### åœºæ™¯ 3ï¼šå¤šç§Ÿæˆ·æœåŠ¡

```mermaid
sequenceDiagram
    participant T1 as ç§Ÿæˆ· 1
    participant T2 as ç§Ÿæˆ· 2
    participant T3 as ç§Ÿæˆ· 3
    participant Gateway as API ç½‘å…³
    participant vLLM as vLLM æœåŠ¡
    participant LoRA as LoRA ç®¡ç†
    
    T1->>Gateway: è¯·æ±‚ï¼ˆç§Ÿæˆ· 1 LoRAï¼‰
    T2->>Gateway: è¯·æ±‚ï¼ˆç§Ÿæˆ· 2 LoRAï¼‰
    T3->>Gateway: è¯·æ±‚ï¼ˆç§Ÿæˆ· 3 LoRAï¼‰
    
    Gateway->>vLLM: è·¯ç”±è¯·æ±‚
    vLLM->>LoRA: åŠ¨æ€åŠ è½½ LoRA
    LoRA->>vLLM: è¿”å›é€‚é…å™¨
    vLLM->>vLLM: å¹¶è¡Œæ¨ç†
    
    vLLM->>Gateway: è¿”å›ç»“æœ
    Gateway->>T1: å“åº”
    Gateway->>T2: å“åº”
    Gateway->>T3: å“åº”
    
    Note over vLLM,LoRA: å…±äº«åŸºç¡€æ¨¡å‹<br/>åŠ¨æ€åˆ‡æ¢ LoRA
```

**å¤šç§Ÿæˆ·éƒ¨ç½²**

```bash
# å¯åŠ¨å¤š LoRA æœåŠ¡
python -m vllm.entrypoints.openai.api_server \
    --model meta-llama/Llama-2-13b-hf \
    --enable-lora \
    --lora-modules \
        tenant1=/path/to/lora1 \
        tenant2=/path/to/lora2 \
        tenant3=/path/to/lora3 \
    --max-loras 10 \
    --max-lora-rank 64
```

### åœºæ™¯ 4ï¼šåµŒå…¥å‘é‡æœåŠ¡

```python
from vllm import LLM

# åŠ è½½åµŒå…¥æ¨¡å‹
llm = LLM(
    model="intfloat/e5-mistral-7b-instruct",
    task="embed",  # åµŒå…¥ä»»åŠ¡
    trust_remote_code=True
)

# ç”ŸæˆåµŒå…¥å‘é‡
texts = [
    "ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ",
    "æ·±åº¦å­¦ä¹ çš„åº”ç”¨é¢†åŸŸ",
    "è‡ªç„¶è¯­è¨€å¤„ç†æŠ€æœ¯"
]

outputs = llm.encode(texts)

# è¾“å‡ºåµŒå…¥å‘é‡
for text, output in zip(texts, outputs):
    embedding = output.outputs.embedding
    print(f"æ–‡æœ¬: {text}")
    print(f"å‘é‡ç»´åº¦: {len(embedding)}")
    print(f"å‘é‡: {embedding[:5]}...\n")
```

## å¸¸è§é—®é¢˜ä¸è§£å†³æ–¹æ¡ˆ

### Q1: å†…å­˜æº¢å‡ºï¼ˆOOMï¼‰é”™è¯¯

**é—®é¢˜è¡¨ç°ï¼š**
```
torch.cuda.OutOfMemoryError: CUDA out of memory
```

**è§£å†³æ–¹æ¡ˆï¼š**

```mermaid
graph TD
    A[OOM é”™è¯¯] --> B{è¯Šæ–­åŸå› }
    
    B --> C[æ¨¡å‹å¤ªå¤§]
    B --> D[åºåˆ—å¤ªé•¿]
    B --> E[æ‰¹æ¬¡å¤ªå¤§]
    
    C --> C1[ä½¿ç”¨é‡åŒ–]
    C --> C2[å¢åŠ  GPU æ•°é‡]
    C --> C3[é™ä½ç²¾åº¦]
    
    D --> D1[å‡å° max-model-len]
    D --> D2[å¯ç”¨åˆ†å—é¢„å¡«å……]
    
    E --> E1[å‡å° max-num-seqs]
    E --> E2[å‡å° max-num-batched-tokens]
    
    C1 --> F[é—®é¢˜è§£å†³]
    C2 --> F
    C3 --> F
    D1 --> F
    D2 --> F
    E1 --> F
    E2 --> F
    
    style A fill:#ff6b6b
    style F fill:#51cf66
```

```bash
# ä¼˜åŒ–é…ç½®
python -m vllm.entrypoints.openai.api_server \
    --model meta-llama/Llama-2-70b-chat-hf \
    --quantization awq \  # ä½¿ç”¨é‡åŒ–
    --gpu-memory-utilization 0.85 \  # é™ä½æ˜¾å­˜åˆ©ç”¨ç‡
    --max-model-len 2048 \  # å‡å°æœ€å¤§é•¿åº¦
    --max-num-seqs 64 \  # å‡å°å¹¶å‘æ•°
    --tensor-parallel-size 4  # å¤šå¡åˆ†å¸ƒ
```

### Q2: æ¨ç†é€Ÿåº¦æ…¢

**è¯Šæ–­å·¥å…·ï¼š**

```python
# å¯ç”¨æ€§èƒ½åˆ†æ
llm = LLM(
    model="meta-llama/Llama-2-7b-chat-hf",
    enforce_eager=False,  # å¯ç”¨ CUDA å›¾ä¼˜åŒ–
    enable_prefix_caching=True,
    enable_chunked_prefill=True
)
```

**ä¼˜åŒ–æ£€æŸ¥æ¸…å•ï¼š**

- âœ… ä½¿ç”¨ `float16` æˆ– `bfloat16` è€Œé `float32`
- âœ… å¯ç”¨ CUDA å›¾ï¼ˆè®¾ç½® `enforce_eager=False`ï¼‰
- âœ… å¯ç”¨å‰ç¼€ç¼“å­˜ï¼ˆ`enable_prefix_caching=True`ï¼‰
- âœ… ä½¿ç”¨é€‚å½“çš„é‡åŒ–æ–¹æ³•
- âœ… è°ƒæ•´ `max-num-batched-tokens`
- âœ… è€ƒè™‘æŠ•æœºè§£ç 

### Q3: æ¨¡å‹åŠ è½½å¤±è´¥

**å¸¸è§é”™è¯¯ï¼š**

```python
# é”™è¯¯ï¼šæ¨¡å‹ä¸æ”¯æŒ
ValueError: Model architecture is not supported

# è§£å†³ï¼šæ£€æŸ¥æ”¯æŒçš„æ¨¡å‹åˆ—è¡¨
from vllm import ModelRegistry
print(ModelRegistry.get_supported_archs())
```

**æ¨¡å‹é…ç½®é—®é¢˜ï¼š**

```python
# ä½¿ç”¨ trust_remote_code
llm = LLM(
    model="your-custom-model",
    trust_remote_code=True,  # å…è®¸æ‰§è¡Œè‡ªå®šä¹‰ä»£ç 
    download_dir="/path/to/cache",  # æŒ‡å®šç¼“å­˜ç›®å½•
    tokenizer_mode="auto"  # è‡ªåŠ¨æ£€æµ‹ tokenizer
)
```

### Q4: API å…¼å®¹æ€§é—®é¢˜

**OpenAI SDK ç‰ˆæœ¬å†²çªï¼š**

```bash
# å®‰è£…å…¼å®¹çš„ OpenAI SDK
pip install openai>=1.0.0
```

**è‡ªå®šä¹‰å‚æ•°ä¼ é€’ï¼š**

```python
from openai import OpenAI

client = OpenAI(base_url="http://localhost:8000/v1", api_key="EMPTY")

# vLLM ç‰¹å®šå‚æ•°
response = client.chat.completions.create(
    model="meta-llama/Llama-2-7b-chat-hf",
    messages=[{"role": "user", "content": "ä½ å¥½"}],
    temperature=0.7,
    max_tokens=256,
    extra_body={
        "repetition_penalty": 1.1,  # vLLM æ‰©å±•å‚æ•°
        "top_k": 50,
        "min_p": 0.1
    }
)
```

### Q5: åˆ†å¸ƒå¼æ¨ç†é…ç½®

**å¤š GPU é€šä¿¡é—®é¢˜ï¼š**

```bash
# æ£€æŸ¥ NCCL ç¯å¢ƒ
export NCCL_DEBUG=INFO
export NCCL_P2P_DISABLE=0

# Ray é…ç½®ï¼ˆvLLM ä½¿ç”¨ Ray è¿›è¡Œåˆ†å¸ƒå¼ï¼‰
export RAY_DEDUP_LOGS=0

python -m vllm.entrypoints.openai.api_server \
    --model meta-llama/Llama-2-70b-chat-hf \
    --tensor-parallel-size 4
```

## ä¸å…¶ä»–æ¡†æ¶å¯¹æ¯”

```mermaid
graph TB
    subgraph "æ¡†æ¶å¯¹æ¯”"
        A[æ¨ç†æ¡†æ¶é€‰æ‹©]
        
        A --> B[vLLM]
        A --> C[TGI]
        A --> D[DeepSpeed]
        A --> E[TensorRT-LLM]
        
        B --> B1[âœ… æœ€é«˜ååé‡]
        B --> B2[âœ… PagedAttention]
        B --> B3[âœ… æ˜“ç”¨æ€§æœ€ä½³]
        B --> B4[âœ… ç¤¾åŒºæ´»è·ƒ]
        
        C --> C1[âœ… HuggingFace é›†æˆ]
        C --> C2[âš ï¸ ååé‡è¾ƒä½]
        
        D --> D1[âœ… è®­ç»ƒ+æ¨ç†]
        D --> D2[âš ï¸ é…ç½®å¤æ‚]
        
        E --> E1[âœ… å»¶è¿Ÿæœ€ä½]
        E --> E2[âš ï¸ ç¼–è¯‘å¤æ‚]
    end
    
    style B fill:#51cf66
```

**æ€§èƒ½å¯¹æ¯”ï¼ˆå‚è€ƒæ•°æ®ï¼‰ï¼š**

| æ¡†æ¶ | ååé‡ | å»¶è¿Ÿ | æ˜“ç”¨æ€§ | å†…å­˜æ•ˆç‡ |
|------|--------|------|--------|----------|
| **vLLM** | â­â­â­â­â­ | â­â­â­â­ | â­â­â­â­â­ | â­â­â­â­â­ |
| TGI | â­â­â­ | â­â­â­â­ | â­â­â­â­ | â­â­â­ |
| DeepSpeed | â­â­â­â­ | â­â­â­ | â­â­â­ | â­â­â­â­ |
| TensorRT-LLM | â­â­â­â­ | â­â­â­â­â­ | â­â­ | â­â­â­â­ |

## æœ€ä½³å®è·µæ€»ç»“

### ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²å»ºè®®

```mermaid
flowchart TD
    A[ç”Ÿäº§éƒ¨ç½²] --> B[ç¡¬ä»¶é€‰æ‹©]
    A --> C[é…ç½®ä¼˜åŒ–]
    A --> D[ç›‘æ§è¿ç»´]
    
    B --> B1[NVIDIA A100/H100]
    B --> B2[é«˜é€Ÿ NVLink]
    B --> B3[å……è¶³å†…å­˜]
    
    C --> C1[å¯ç”¨é‡åŒ–]
    C --> C2[å‰ç¼€ç¼“å­˜]
    C --> C3[åˆ†å¸ƒå¼æ¨ç†]
    
    D --> D1[Prometheus ç›‘æ§]
    D --> D2[æ—¥å¿—èšåˆ]
    D --> D3[è´Ÿè½½å‡è¡¡]
    
    style A fill:#4dabf7
    style B1 fill:#51cf66
    style C1 fill:#51cf66
    style D1 fill:#51cf66
```

**æ¨èé…ç½®æ¨¡æ¿ï¼š**

```bash
# ç”Ÿäº§ç¯å¢ƒå¯åŠ¨è„šæœ¬
#!/bin/bash

export CUDA_VISIBLE_DEVICES=0,1,2,3
export VLLM_LOGGING_LEVEL=INFO

python -m vllm.entrypoints.openai.api_server \
    --model meta-llama/Llama-2-70b-chat-hf \
    --host 0.0.0.0 \
    --port 8000 \
    --tensor-parallel-size 4 \
    --dtype float16 \
    --quantization awq \
    --max-model-len 4096 \
    --max-num-seqs 256 \
    --max-num-batched-tokens 8192 \
    --gpu-memory-utilization 0.90 \
    --enable-prefix-caching \
    --enable-chunked-prefill \
    --disable-log-requests \
    --served-model-name llama2-70b \
    2>&1 | tee -a vllm.log
```

### æ€§èƒ½è°ƒä¼˜æ¸…å•

**å†…å­˜ä¼˜åŒ–ï¼š**
- [ ] ä½¿ç”¨é‡åŒ–ï¼ˆAWQ/GPTQï¼‰å‡å°‘æ˜¾å­˜å ç”¨
- [ ] è°ƒæ•´ `--gpu-memory-utilization`ï¼ˆå»ºè®® 0.85-0.95ï¼‰
- [ ] é™åˆ¶ `--max-model-len` åˆ°å®é™…éœ€è¦çš„é•¿åº¦
- [ ] å¯ç”¨ `--enable-chunked-prefill`

**ååé‡ä¼˜åŒ–ï¼š**
- [ ] å¢åŠ  `--max-num-seqs`ï¼ˆæ ¹æ®æ˜¾å­˜è°ƒæ•´ï¼‰
- [ ] è°ƒæ•´ `--max-num-batched-tokens`
- [ ] ä½¿ç”¨å¤š GPU å¼ é‡å¹¶è¡Œ
- [ ] å¯ç”¨è¿ç»­æ‰¹å¤„ç†ï¼ˆé»˜è®¤å¼€å¯ï¼‰

**å»¶è¿Ÿä¼˜åŒ–ï¼š**
- [ ] å¯ç”¨ `--enable-prefix-caching`
- [ ] ä½¿ç”¨æŠ•æœºè§£ç 
- [ ] å‡å° `--max-num-seqs`ï¼ˆç‰ºç‰²ååé‡ï¼‰
- [ ] ä½¿ç”¨ CUDA å›¾ï¼ˆé»˜è®¤å¼€å¯ï¼‰

**ç¨³å®šæ€§ä¼˜åŒ–ï¼š**
- [ ] è®¾ç½®åˆç†çš„è¶…æ—¶æ—¶é—´
- [ ] é…ç½®å¥åº·æ£€æŸ¥ç«¯ç‚¹
- [ ] å¯ç”¨è¯·æ±‚é™æµ
- [ ] éƒ¨ç½²å¤šå®ä¾‹è´Ÿè½½å‡è¡¡

## èµ„æºé“¾æ¥

### å®˜æ–¹èµ„æº

- ğŸ“š **å®˜æ–¹æ–‡æ¡£**: [https://docs.vllm.ai](https://docs.vllm.ai)
- ğŸ’» **GitHub ä»“åº“**: [https://github.com/vllm-project/vllm](https://github.com/vllm-project/vllm)
- ğŸŒ **å®˜æ–¹ç½‘ç«™**: [https://vllm.ai](https://vllm.ai)
- ğŸ“ **è®ºæ–‡**: [Efficient Memory Management for Large Language Model Serving with PagedAttention](https://arxiv.org/abs/2309.06180)

### ç¤¾åŒºèµ„æº

- ğŸ’¬ **ç”¨æˆ·è®ºå›**: [vLLM Forum](https://github.com/vllm-project/vllm/discussions)
- ğŸ¯ **å¼€å‘è€… Slack**: [åŠ å…¥é¢‘é“](https://join.slack.com/t/vllm-workspace)
- ğŸ› **é—®é¢˜åé¦ˆ**: [GitHub Issues](https://github.com/vllm-project/vllm/issues)
- ğŸ” **å®‰å…¨æŠ¥å‘Š**: [Security Advisories](https://github.com/vllm-project/vllm/security/advisories)

### å­¦ä¹ èµ„æº

- ğŸ“– **DeepWiki**: [https://deepwiki.com/vllm-project/vllm](https://deepwiki.com/vllm-project/vllm)
- ğŸ¬ **è§†é¢‘æ•™ç¨‹**: [vLLM Events](https://vllm.ai/events)
- ğŸ“° **å®˜æ–¹åšå®¢**: [vLLM Blog](https://blog.vllm.ai)
- ğŸ“¦ **æ¨¡å‹åˆ—è¡¨**: [æ”¯æŒçš„æ¨¡å‹](https://docs.vllm.ai/en/latest/models/supported_models.html)

### ç›¸å…³å·¥å…·

- ğŸ¤— **Hugging Face**: æ¨¡å‹ä¸‹è½½ä¸ç®¡ç†
- âš¡ **Ray**: åˆ†å¸ƒå¼è®¡ç®—æ¡†æ¶
- ğŸ”¥ **FlashAttention**: é«˜æ•ˆæ³¨æ„åŠ›æœºåˆ¶
- ğŸ“Š **Prometheus**: æ€§èƒ½ç›‘æ§

## æ€»ç»“

vLLM é€šè¿‡åˆ›æ–°çš„ PagedAttention æŠ€æœ¯å’Œè¿ç»­æ‰¹å¤„ç†æœºåˆ¶ï¼Œä¸º LLM æ¨ç†æœåŠ¡æ ‘ç«‹äº†æ–°çš„æ€§èƒ½æ ‡æ†ã€‚æ— è®ºæ˜¯å°è§„æ¨¡åŸå‹å¼€å‘è¿˜æ˜¯å¤§è§„æ¨¡ç”Ÿäº§éƒ¨ç½²ï¼ŒvLLM éƒ½æä¾›äº†ç®€å•æ˜“ç”¨ä¸”é«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚

**æ ¸å¿ƒä¼˜åŠ¿å›é¡¾ï¼š**

```mermaid
mindmap
  root((vLLM))
    æŠ€æœ¯åˆ›æ–°
      PagedAttention
      è¿ç»­æ‰¹å¤„ç†
      CUDAå›¾ä¼˜åŒ–
    æ€§èƒ½å“è¶Š
      2-10x ååé‡æå‡
      å†…å­˜åˆ©ç”¨ç‡ 90%+
      æ¯«ç§’çº§å»¶è¿Ÿ
    ç”Ÿæ€å®Œå–„
      OpenAI API å…¼å®¹
      HuggingFace é›†æˆ
      ä¸°å¯Œçš„ç¡¬ä»¶æ”¯æŒ
    ç¤¾åŒºæ´»è·ƒ
      2000+ è´¡çŒ®è€…
      66k+ GitHub Stars
      å¿«é€Ÿè¿­ä»£æ›´æ–°
```

**é€‰æ‹© vLLM çš„ç†ç”±ï¼š**

1. ğŸš€ **æ€§èƒ½é¢†å…ˆ**ï¼šä¸šç•Œæœ€é«˜çš„æ¨ç†ååé‡
2. ğŸ’° **æˆæœ¬ä¼˜åŒ–**ï¼šæ›´é«˜çš„ç¡¬ä»¶åˆ©ç”¨ç‡é™ä½éƒ¨ç½²æˆæœ¬
3. ğŸ”§ **æ˜“äºä½¿ç”¨**ï¼šç®€æ´çš„ API å’Œå®Œå–„çš„æ–‡æ¡£
4. ğŸŒ **å¹¿æ³›æ”¯æŒ**ï¼šæ”¯æŒä¸»æµæ¨¡å‹å’Œç¡¬ä»¶å¹³å°
5. ğŸ”„ **æŒç»­åˆ›æ–°**ï¼šæ´»è·ƒçš„ç¤¾åŒºå’Œå¿«é€Ÿçš„åŠŸèƒ½è¿­ä»£

å¼€å§‹ä½¿ç”¨ vLLMï¼Œè®©ä½ çš„ LLM åº”ç”¨è¿è¡Œå¾—æ›´å¿«ã€æ›´çœã€æ›´å¼ºå¤§ï¼

---

**ç›¸å…³é˜…è¯»ï¼š**
- [LangChain4j å®Œæ•´æŒ‡å—]({% post_url 2026-01-21-langchain4j-complete-guide %})
- [DeepLearning4j å®Œæ•´æŒ‡å—]({% post_url 2026-01-15-deeplearning4j-complete-guide %})
- [Apache OpenNLP å®Œæ•´æŒ‡å—]({% post_url 2026-01-09-apache-opennlp-complete-guide %})

