---
title: "LeRobot å®Œæ•´æŒ‡å—ï¼šHugging Face æœºå™¨äººå­¦ä¹ åº“å…¨è§£æ"
date: 2026-02-08 10:00:00 +0800
categories:
  - AIå·¥å…·
tags:
  - Robotics
  - Machine Learning
toc: true
mermaid: true
---

## LeRobot ç®€ä»‹

LeRobot æ˜¯ Hugging Face æ¨å‡ºçš„å¼€æºæœºå™¨äººå­¦ä¹ åº“ï¼Œæ—¨åœ¨é€šè¿‡ç«¯åˆ°ç«¯å­¦ä¹ è®© AI æœºå™¨äººæŠ€æœ¯å˜å¾—æ›´åŠ æ˜“ç”¨ã€‚å®ƒæä¾›äº†æ¨¡å‹ã€æ•°æ®é›†å’Œå·¥å…·ï¼Œé™ä½äº†æœºå™¨äººå­¦ä¹ çš„é—¨æ§›ï¼Œä½¿æ¯ä¸ªäººéƒ½èƒ½è´¡çŒ®å’Œå—ç›Šäºå…±äº«çš„æ•°æ®é›†å’Œé¢„è®­ç»ƒæ¨¡å‹ã€‚

```mermaid
mindmap
  root((LeRobot))
    ç¡¬ä»¶æ§åˆ¶
      ç»Ÿä¸€Robotæ¥å£
      å¤šå¹³å°æ”¯æŒ
      é¥æ“ä½œè®¾å¤‡
      ä½æˆæœ¬æœºæ¢°è‡‚
      äººå½¢æœºå™¨äºº
    æ•°æ®ç®¡ç†
      LeRobotDatasetæ ¼å¼
      Parquet+MP4å­˜å‚¨
      HF Hubé›†æˆ
      æ•°æ®æµå¼ä¼ è¾“
      å¯è§†åŒ–å·¥å…·
    æ¨¡å‹è®­ç»ƒ
      æ¨¡ä»¿å­¦ä¹ 
        ACT
        Diffusion
        VQ-BeT
      å¼ºåŒ–å­¦ä¹ 
        HIL-SERL
        TDMPC
      VLAæ¨¡å‹
        Pi0.5
        GR00T
        SmolVLA
    ç”Ÿæ€ç³»ç»Ÿ
      PyTorchåŸç”Ÿ
      å¼€æºç¤¾åŒº
      åŸºå‡†æµ‹è¯•
        LIBERO
        MetaWorld
      æ‰©å±•æ€§å¼º
```

### æ ¸å¿ƒç‰¹æ€§

```mermaid
graph LR
    A[LeRobotæ ¸å¿ƒç‰¹æ€§] --> B[ç¡¬ä»¶æ— å…³æ¥å£]
    A --> C[æ ‡å‡†åŒ–æ•°æ®æ ¼å¼]
    A --> D[å‰æ²¿ç­–ç•¥æ¨¡å‹]
    A --> E[å¼€æºç”Ÿæ€æ”¯æŒ]
    
    B --> B1[PythonåŸç”Ÿ]
    B --> B2[è·¨å¹³å°æ§åˆ¶]
    B --> B3[ä»ä½æˆæœ¬åˆ°é«˜ç«¯]
    
    C --> C1[é«˜æ•ˆå­˜å‚¨]
    C --> C2[æµå¼ä¼ è¾“]
    C --> C3[HF Hubæ‰˜ç®¡]
    
    D --> D1[å³æ’å³ç”¨]
    D --> D2[çœŸå®ä¸–ç•ŒéªŒè¯]
    D --> D3[æŒç»­æ›´æ–°]
    
    E --> E1[ç¤¾åŒºé©±åŠ¨]
    E --> E2[æ˜“äºæ‰©å±•]
    E --> E3[å®Œæ•´æ–‡æ¡£]
    
    style A fill:#f9f,stroke:#333,stroke-width:4px
```

### ä¸ºä»€ä¹ˆé€‰æ‹© LeRobotï¼Ÿ

åœ¨æœºå™¨äººå­¦ä¹ é¢†åŸŸï¼ŒLeRobot è§£å†³äº†ä»¥ä¸‹æ ¸å¿ƒç—›ç‚¹ï¼š

```mermaid
graph TB
    subgraph "ä¼ ç»ŸæŒ‘æˆ˜"
        A1[ç¡¬ä»¶ç¢ç‰‡åŒ–] --> B1[éš¾ä»¥å¤ç°]
        A2[æ•°æ®æ ¼å¼ä¸ç»Ÿä¸€] --> B2[éš¾ä»¥å…±äº«]
        A3[æ¨¡å‹å®ç°å¤æ‚] --> B3[é—¨æ§›é«˜]
        A4[ç¼ºä¹æ ‡å‡†å·¥å…·] --> B4[æ•ˆç‡ä½]
    end
    
    subgraph "LeRobotè§£å†³æ–¹æ¡ˆ"
        C1[ç»Ÿä¸€Robotæ¥å£] --> D1[è½»æ¾åˆ‡æ¢å¹³å°]
        C2[LeRobotDatasetæ ¼å¼] --> D2[æ•°æ®å³æ’å³ç”¨]
        C3[é¢„è®­ç»ƒæ¨¡å‹åº“] --> D3[å¼€ç®±å³ç”¨]
        C4[å®Œæ•´å·¥å…·é“¾] --> D4[ç«¯åˆ°ç«¯æ”¯æŒ]
    end
    
    B1 -.-> C1
    B2 -.-> C2
    B3 -.-> C3
    B4 -.-> C4
    
    style A1 fill:#faa
    style A2 fill:#faa
    style A3 fill:#faa
    style A4 fill:#faa
    style C1 fill:#afa
    style C2 fill:#afa
    style C3 fill:#afa
    style C4 fill:#afa
```

## å¿«é€Ÿå¼€å§‹

### å®‰è£…

LeRobot å¯ä»¥ç›´æ¥ä» PyPI å®‰è£…ï¼š

```bash
# åŸºç¡€å®‰è£…
pip install lerobot

# éªŒè¯å®‰è£…
lerobot-info
```

### ç³»ç»Ÿè¦æ±‚

```mermaid
graph LR
    A[ç³»ç»Ÿè¦æ±‚] --> B[Python 3.8+]
    A --> C[PyTorch 1.13+]
    A --> D[CUDAå¯é€‰]
    A --> E[è¶³å¤Ÿå­˜å‚¨ç©ºé—´]
    
    B --> B1[æ¨è3.10+]
    C --> C1[æ”¯æŒCPU/GPU]
    D --> D1[åŠ é€Ÿè®­ç»ƒæ¨ç†]
    E --> E1[è§†é¢‘æ•°æ®é›†]
    
    style A fill:#ff9
```

### ç¬¬ä¸€ä¸ªç¤ºä¾‹

```python
from lerobot.datasets.lerobot_dataset import LeRobotDataset

# ä» Hugging Face Hub åŠ è½½æ•°æ®é›†
dataset = LeRobotDataset("lerobot/aloha_mobile_cabinet")

# è®¿é—®æ•°æ®ï¼ˆè‡ªåŠ¨å¤„ç†è§†é¢‘è§£ç ï¼‰
episode_index = 0
print(f"åŠ¨ä½œç»´åº¦: {dataset[episode_index]['action'].shape}")
print(f"è§‚å¯Ÿé”®: {dataset[episode_index].keys()}")

# éå†æ•°æ®é›†
for i in range(min(5, len(dataset))):
    sample = dataset[i]
    print(f"æ ·æœ¬ {i}: åŠ¨ä½œ={sample['action'][:3]}")
```

## æœºå™¨äººæ§åˆ¶

### Robot æ¥å£æ¶æ„

LeRobot æä¾›äº†ç»Ÿä¸€çš„ `Robot` ç±»æ¥å£ï¼Œå°†æ§åˆ¶é€»è¾‘ä¸ç¡¬ä»¶ç»†èŠ‚è§£è€¦ï¼š

```mermaid
graph TB
    subgraph "åº”ç”¨å±‚"
        A[ç”¨æˆ·ä»£ç ] --> B[ç»Ÿä¸€Robot API]
    end
    
    subgraph "æŠ½è±¡å±‚"
        B --> C[RobotåŸºç±»]
        C --> D[é…ç½®ç®¡ç†]
        C --> E[çŠ¶æ€åŒæ­¥]
        C --> F[åŠ¨ä½œæ§åˆ¶]
    end
    
    subgraph "ç¡¬ä»¶å±‚"
        D --> G[SO100]
        D --> H[Koch]
        D --> I[Reachy2]
        D --> J[è‡ªå®šä¹‰æœºå™¨äºº]
        
        E --> G
        E --> H
        E --> I
        E --> J
        
        F --> G
        F --> H
        F --> I
        F --> J
    end
    
    style B fill:#9cf
    style C fill:#fcf
```

### åŸºç¡€æ§åˆ¶ç¤ºä¾‹

```python
from lerobot.robots.myrobot import MyRobot
from lerobot.common.robot_config import RobotConfig

# 1. åˆ›å»ºæœºå™¨äººé…ç½®
config = RobotConfig(
    robot_type="myrobot",
    control_frequency=30,  # æ§åˆ¶é¢‘ç‡ 30Hz
    cameras={
        "front": {"resolution": (640, 480), "fps": 30},
        "wrist": {"resolution": (320, 240), "fps": 30}
    }
)

# 2. è¿æ¥æœºå™¨äºº
robot = MyRobot(config=config)
robot.connect()

# 3. è¯»å–è§‚å¯Ÿ
observation = robot.get_observation()
print(f"å…³èŠ‚ä½ç½®: {observation['joint_positions']}")
print(f"ç›¸æœºå›¾åƒ: {observation['cameras']['front'].shape}")

# 4. å‘é€åŠ¨ä½œ
action = {
    'joint_positions': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6],
    'gripper': 1.0  # æ‰“å¼€å¤¹çˆª
}
robot.send_action(action)

# 5. æ–­å¼€è¿æ¥
robot.disconnect()
```

### é¥æ“ä½œç¤ºä¾‹

```python
from lerobot.robots.follower import FollowerRobot
from lerobot.robots.leader import LeaderRobot
import time

# åˆ›å»ºä¸»ä»æœºå™¨äºº
leader = LeaderRobot(config=leader_config)
follower = FollowerRobot(config=follower_config)

leader.connect()
follower.connect()

try:
    # é¥æ“ä½œå¾ªç¯
    for _ in range(1000):
        # ä»ä¸»æ§è®¾å¤‡è¯»å–åŠ¨ä½œ
        leader_obs = leader.get_observation()
        action = leader_obs['joint_positions']
        
        # å‘é€åˆ°ä»åŠ¨æœºå™¨äºº
        follower.send_action({'joint_positions': action})
        
        time.sleep(1/30)  # 30Hz æ§åˆ¶é¢‘ç‡
        
finally:
    leader.disconnect()
    follower.disconnect()
```

### æ”¯æŒçš„ç¡¬ä»¶å¹³å°

```mermaid
graph LR
    A[æ”¯æŒçš„ç¡¬ä»¶] --> B[æœºå™¨äººå¹³å°]
    A --> C[é¥æ“ä½œè®¾å¤‡]
    A --> D[æ‰©å±•æ€§]
    
    B --> B1[SO100 ä½æˆæœ¬è‡‚]
    B --> B2[LeKiwi]
    B --> B3[Koch]
    B --> B4[HopeJR]
    B --> B5[Reachy2 äººå½¢]
    B --> B6[Unitree G1]
    B --> B7[OpenARM]
    
    C --> C1[æ¸¸æˆæ‰‹æŸ„]
    C --> C2[é”®ç›˜]
    C --> C3[æ‰‹æœº]
    C --> C4[ä¸»æ§æœºæ¢°è‡‚]
    
    D --> D1[è‡ªå®šä¹‰Robotç±»]
    D --> D2[å®ç°æ ‡å‡†æ¥å£]
    D --> D3[åˆ©ç”¨å®Œæ•´å·¥å…·é“¾]
    
    style A fill:#f96
```

## LeRobotDataset æ•°æ®é›†

### æ•°æ®æ ¼å¼æ¶æ„

LeRobot ä½¿ç”¨æ ‡å‡†åŒ–çš„ LeRobotDataset æ ¼å¼æ¥è§£å†³æœºå™¨äººæ•°æ®ç¢ç‰‡åŒ–é—®é¢˜ï¼š

```mermaid
graph TB
    subgraph "LeRobotDatasetç»“æ„"
        A[æ•°æ®é›†æ ¹ç›®å½•] --> B[è§†é¢‘æ•°æ®]
        A --> C[çŠ¶æ€/åŠ¨ä½œæ•°æ®]
        A --> D[å…ƒæ•°æ®]
        
        B --> B1[MP4/å›¾åƒåºåˆ—]
        B --> B2[å¤šç›¸æœºåŒæ­¥]
        B --> B3[é«˜æ•ˆå‹ç¼©]
        
        C --> C1[Parquetæ–‡ä»¶]
        C --> C2[å…³èŠ‚ä½ç½®]
        C --> C3[åŠ¨ä½œåºåˆ—]
        C --> C4[æ—¶é—´æˆ³]
        
        D --> D1[info.json]
        D --> D2[stats.json]
        D --> D3[episode_data_index.safetensors]
    end
    
    style A fill:#9f9
    style B fill:#9cf
    style C fill:#fcf
    style D fill:#ffc
```

### åŠ è½½å’Œä½¿ç”¨æ•°æ®é›†

```python
from lerobot.datasets.lerobot_dataset import LeRobotDataset
import torch

# 1. ä» Hub åŠ è½½æ•°æ®é›†
dataset = LeRobotDataset(
    repo_id="lerobot/aloha_mobile_cabinet",
    split="train"
)

# 2. æŸ¥çœ‹æ•°æ®é›†ä¿¡æ¯
print(f"æ•°æ®é›†å¤§å°: {len(dataset)}")
print(f"ç‰¹å¾åˆ—: {dataset.features}")
print(f"å›åˆæ•°: {dataset.num_episodes}")
print(f"å¸§æ•°: {dataset.num_frames}")

# 3. è®¿é—®å•ä¸ªæ ·æœ¬
sample = dataset[0]
print(f"å¯ç”¨é”®: {sample.keys()}")
print(f"åŠ¨ä½œå½¢çŠ¶: {sample['action'].shape}")
print(f"çŠ¶æ€å½¢çŠ¶: {sample['observation.state'].shape}")

# 4. è®¿é—®å›¾åƒæ•°æ®
if 'observation.images.front' in sample:
    image = sample['observation.images.front']
    print(f"å›¾åƒå½¢çŠ¶: {image.shape}")  # [H, W, C]

# 5. æŒ‰å›åˆè®¿é—®
episode_0 = dataset.get_episode(0)
print(f"å›åˆ0é•¿åº¦: {len(episode_0)}")

# 6. æ‰¹é‡åŠ è½½
from torch.utils.data import DataLoader

dataloader = DataLoader(
    dataset,
    batch_size=32,
    shuffle=True,
    num_workers=4
)

for batch in dataloader:
    actions = batch['action']
    states = batch['observation.state']
    # è®­ç»ƒæ¨¡å‹...
    break
```

### æ•°æ®é›†æ“ä½œå·¥å…·

```mermaid
graph LR
    A[æ•°æ®é›†å·¥å…·] --> B[åˆ é™¤å›åˆ]
    A --> C[æ•°æ®é›†åˆ‡åˆ†]
    A --> D[ç‰¹å¾ç®¡ç†]
    A --> E[åˆå¹¶æ•°æ®é›†]
    
    B --> B1[æŒ‰ç´¢å¼•åˆ é™¤]
    B --> B2[æŒ‰æ¡ä»¶è¿‡æ»¤]
    
    C --> C1[æŒ‰ç´¢å¼•åˆ‡åˆ†]
    C --> C2[æŒ‰æ¯”ä¾‹åˆ‡åˆ†]
    C --> C3[train/val/test]
    
    D --> D1[æ·»åŠ ç‰¹å¾]
    D --> D2[åˆ é™¤ç‰¹å¾]
    D --> D3[é‡å‘½åç‰¹å¾]
    
    E --> E1[å¤šæ•°æ®é›†åˆå¹¶]
    E --> E2[ä¿æŒä¸€è‡´æ€§]
    
    style A fill:#f9c
```

```python
from lerobot.datasets.lerobot_dataset import LeRobotDataset

# åŠ è½½æ•°æ®é›†
dataset = LeRobotDataset("lerobot/pusht")

# 1. åˆ é™¤ç‰¹å®šå›åˆ
dataset.delete_episodes([0, 5, 10])

# 2. åˆ‡åˆ†æ•°æ®é›†
train_dataset, val_dataset = dataset.split([0.8, 0.2])

# 3. æŒ‰ç´¢å¼•åˆ‡åˆ†
train_dataset = dataset[:8000]
val_dataset = dataset[8000:]

# 4. æ·»åŠ æ–°ç‰¹å¾
dataset.add_feature(
    name="observation.new_sensor",
    dtype="float32",
    shape=(10,)
)

# 5. åˆ é™¤ç‰¹å¾
dataset.remove_feature("observation.unused_camera")

# 6. åˆå¹¶å¤šä¸ªæ•°æ®é›†
from lerobot.datasets.utils import merge_datasets

merged = merge_datasets([
    "lerobot/aloha_mobile_cabinet",
    "lerobot/aloha_mobile_chair"
])

# 7. ä¿å­˜åˆ° Hub
dataset.push_to_hub(
    repo_id="my-username/my-robot-dataset",
    private=False
)
```

### åˆ›å»ºè‡ªå®šä¹‰æ•°æ®é›†

```python
from lerobot.datasets.lerobot_dataset import LeRobotDataset
from pathlib import Path
import numpy as np

# 1. åˆ›å»ºæ–°æ•°æ®é›†
dataset = LeRobotDataset.create(
    repo_id="my-username/custom-robot-data",
    fps=30,
    robot_type="my_robot",
    features={
        "observation.state": {"dtype": "float32", "shape": (7,)},
        "observation.images.front": {"dtype": "video", "shape": (480, 640, 3)},
        "action": {"dtype": "float32", "shape": (7,)},
    }
)

# 2. æ·»åŠ å›åˆæ•°æ®
for episode_idx in range(10):
    episode_data = []
    for frame_idx in range(100):
        frame = {
            "observation.state": np.random.randn(7),
            "observation.images.front": np.random.randint(0, 255, (480, 640, 3), dtype=np.uint8),
            "action": np.random.randn(7),
            "timestamp": frame_idx / 30.0,
        }
        episode_data.append(frame)
    
    dataset.add_episode(episode_data)

# 3. è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
dataset.consolidate()

# 4. ä¿å­˜
dataset.save("./my_robot_dataset")
```

### æµè§ˆ Hub ä¸Šçš„æ•°æ®é›†

```python
from huggingface_hub import list_datasets

# æœç´¢ LeRobot æ•°æ®é›†
datasets = list_datasets(
    author="lerobot",
    sort="downloads",
    direction=-1
)

for ds in datasets:
    print(f"ğŸ“¦ {ds.id}")
    print(f"   ä¸‹è½½é‡: {ds.downloads}")
    print(f"   æ›´æ–°æ—¶é—´: {ds.lastModified}")
    print()
```

## ç­–ç•¥æ¨¡å‹è®­ç»ƒ

### æ¨¡å‹æ¶æ„æ€»è§ˆ

```mermaid
graph TB
    subgraph "LeRobotç­–ç•¥æ¨¡å‹"
        A[ç­–ç•¥ç±»å‹] --> B[æ¨¡ä»¿å­¦ä¹ ]
        A --> C[å¼ºåŒ–å­¦ä¹ ]
        A --> D[VLAæ¨¡å‹]
        
        B --> B1[ACT<br/>Action Chunking Transformer]
        B --> B2[Diffusion<br/>æ‰©æ•£ç­–ç•¥]
        B --> B3[VQ-BeT<br/>å‘é‡é‡åŒ–è¡Œä¸ºTransformer]
        
        C --> C1[HIL-SERL<br/>äººåœ¨ç¯å¼ºåŒ–å­¦ä¹ ]
        C --> C2[TDMPC<br/>æ—¶åºå·®åˆ†æ¨¡å‹é¢„æµ‹æ§åˆ¶]
        
        D --> D1[Pi0.5<br/>Physical Intelligence]
        D --> D2[GR00T N1.5<br/>NVIDIAé€šç”¨æœºå™¨äºº]
        D --> D3[SmolVLA<br/>å°å‹è§†è§‰è¯­è¨€åŠ¨ä½œ]
        D --> D4[XVLA<br/>è·¨ä½“ç°VLA]
    end
    
    style B fill:#afa
    style C fill:#faa
    style D fill:#aaf
```

### ACT æ¨¡å‹è®­ç»ƒ

ACT (Action Chunking with Transformers) æ˜¯ä¸€ç§æµè¡Œçš„æ¨¡ä»¿å­¦ä¹ æ–¹æ³•ï¼š

```python
from lerobot.scripts.train import train

# ä½¿ç”¨é…ç½®è®­ç»ƒ ACT æ¨¡å‹
train(
    policy_name="act",
    dataset_repo_id="lerobot/aloha_mobile_cabinet",
    output_dir="./outputs/act_aloha",
    training_steps=100000,
    batch_size=32,
    learning_rate=1e-4,
    eval_freq=5000,
    save_freq=10000,
    num_workers=4,
    device="cuda",
    seed=42
)
```

æˆ–ä½¿ç”¨å‘½ä»¤è¡Œï¼š

```bash
lerobot-train \
  --policy=act \
  --dataset.repo_id=lerobot/aloha_mobile_cabinet \
  --training.num_steps=100000 \
  --training.batch_size=32 \
  --training.learning_rate=1e-4 \
  --training.eval_freq=5000 \
  --training.save_freq=10000 \
  --output_dir=./outputs/act_aloha \
  --device=cuda
```

### è‡ªå®šä¹‰è®­ç»ƒé…ç½®

```python
from lerobot.common.policies.act.configuration_act import ACTConfig
from lerobot.common.policies.act.modeling_act import ACTPolicy
import torch

# 1. åˆ›å»ºè‡ªå®šä¹‰é…ç½®
config = ACTConfig(
    # æ¨¡å‹æ¶æ„
    input_shapes={
        "observation.state": [14],
        "observation.images.front": [3, 480, 640],
    },
    output_shapes={
        "action": [14],
    },
    # ACT ç‰¹å®šå‚æ•°
    chunk_size=100,
    n_obs_steps=1,
    dim_model=512,
    n_heads=8,
    dim_feedforward=3200,
    n_encoder_layers=4,
    n_decoder_layers=7,
    # è®­ç»ƒå‚æ•°
    lr=1e-5,
    weight_decay=1e-4,
    kl_weight=10,
)

# 2. åˆ›å»ºæ¨¡å‹
policy = ACTPolicy(config)

# 3. å‡†å¤‡æ•°æ®
from lerobot.datasets.lerobot_dataset import LeRobotDataset
from torch.utils.data import DataLoader

dataset = LeRobotDataset("lerobot/aloha_mobile_cabinet")
dataloader = DataLoader(
    dataset,
    batch_size=32,
    shuffle=True,
    num_workers=4
)

# 4. è®­ç»ƒå¾ªç¯
optimizer = torch.optim.AdamW(
    policy.parameters(),
    lr=config.lr,
    weight_decay=config.weight_decay
)

policy.train()
policy.to("cuda")

for epoch in range(100):
    for batch_idx, batch in enumerate(dataloader):
        # ç§»åŠ¨åˆ°è®¾å¤‡
        batch = {k: v.to("cuda") for k, v in batch.items()}
        
        # å‰å‘ä¼ æ’­
        output = policy(batch)
        loss = output["loss"]
        
        # åå‘ä¼ æ’­
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        
        if batch_idx % 100 == 0:
            print(f"Epoch {epoch}, Batch {batch_idx}, Loss: {loss.item():.4f}")
    
    # ä¿å­˜æ£€æŸ¥ç‚¹
    if (epoch + 1) % 10 == 0:
        torch.save(
            policy.state_dict(),
            f"./checkpoints/act_epoch_{epoch+1}.pt"
        )
```

### Diffusion ç­–ç•¥è®­ç»ƒ

```bash
lerobot-train \
  --policy=diffusion \
  --dataset.repo_id=lerobot/pusht \
  --policy.n_action_steps=8 \
  --policy.num_inference_steps=10 \
  --policy.down_dims='[256, 512, 1024]' \
  --training.num_steps=200000 \
  --training.batch_size=64 \
  --output_dir=./outputs/diffusion_pusht
```

### è®­ç»ƒç›‘æ§

```mermaid
graph LR
    A[è®­ç»ƒç›‘æ§] --> B[TensorBoard]
    A --> C[W&Bé›†æˆ]
    A --> D[æŒ‡æ ‡è®°å½•]
    
    B --> B1[æŸå¤±æ›²çº¿]
    B --> B2[å­¦ä¹ ç‡]
    B --> B3[æ¢¯åº¦èŒƒæ•°]
    
    C --> C1[å®éªŒè·Ÿè¸ª]
    C --> C2[æ¨¡å‹ç‰ˆæœ¬]
    C --> C3[è¶…å‚æ•°]
    
    D --> D1[è®­ç»ƒæŸå¤±]
    D --> D2[éªŒè¯æŸå¤±]
    D --> D3[æˆåŠŸç‡]
    D --> D4[æ¨ç†æ—¶é—´]
    
    style A fill:#f9f
```

```python
# å¯ç”¨ Weights & Biases è·Ÿè¸ª
lerobot-train \
  --policy=act \
  --dataset.repo_id=lerobot/aloha_mobile_cabinet \
  --wandb.enable=true \
  --wandb.project=lerobot-experiments \
  --wandb.entity=my-username \
  --wandb.run_name=act-aloha-v1

# æˆ–åœ¨ä»£ç ä¸­å¯ç”¨
import wandb

wandb.init(
    project="lerobot-experiments",
    name="act-aloha-v1",
    config={
        "policy": "act",
        "dataset": "lerobot/aloha_mobile_cabinet",
        "batch_size": 32,
        "learning_rate": 1e-4,
    }
)
```

### å¤š GPU è®­ç»ƒ

```bash
# ä½¿ç”¨ torchrun è¿›è¡Œåˆ†å¸ƒå¼è®­ç»ƒ
torchrun --nproc_per_node=4 \
  lerobot-train \
  --policy=act \
  --dataset.repo_id=lerobot/aloha_mobile_cabinet \
  --training.batch_size=128 \
  --training.num_steps=100000
```

```python
# åœ¨ä»£ç ä¸­å¯ç”¨ DDP
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP

# åˆå§‹åŒ–åˆ†å¸ƒå¼ç¯å¢ƒ
dist.init_process_group(backend="nccl")
local_rank = int(os.environ["LOCAL_RANK"])
torch.cuda.set_device(local_rank)

# åŒ…è£…æ¨¡å‹
policy = ACTPolicy(config)
policy = policy.to(local_rank)
policy = DDP(policy, device_ids=[local_rank])

# ä½¿ç”¨åˆ†å¸ƒå¼é‡‡æ ·å™¨
from torch.utils.data.distributed import DistributedSampler

sampler = DistributedSampler(dataset)
dataloader = DataLoader(
    dataset,
    batch_size=32,
    sampler=sampler,
    num_workers=4
)
```

## æ¨¡å‹æ¨ç†ä¸è¯„ä¼°

### æ¨ç†æµç¨‹

```mermaid
sequenceDiagram
    participant U as ç”¨æˆ·
    participant M as ç­–ç•¥æ¨¡å‹
    participant R as æœºå™¨äºº
    participant E as ç¯å¢ƒ
    
    U->>M: åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
    M->>M: åˆå§‹åŒ–
    
    loop æ§åˆ¶å¾ªç¯
        R->>M: è·å–è§‚å¯Ÿ
        M->>M: æ¨ç†åŠ¨ä½œ
        M->>R: å‘é€åŠ¨ä½œ
        R->>E: æ‰§è¡ŒåŠ¨ä½œ
        E->>R: æ›´æ–°çŠ¶æ€
    end
    
    U->>M: è¯„ä¼°æ€§èƒ½
    M->>U: è¿”å›æŒ‡æ ‡
```

### å®æ—¶æ¨ç†

```python
from lerobot.common.policies.act.modeling_act import ACTPolicy
from lerobot.robots.myrobot import MyRobot
import torch

# 1. åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
policy = ACTPolicy.from_pretrained("lerobot/act_aloha_mobile_cabinet")
policy.eval()
policy.to("cuda")

# 2. è¿æ¥æœºå™¨äºº
robot = MyRobot(config=robot_config)
robot.connect()

# 3. æ¨ç†å¾ªç¯
try:
    for step in range(1000):
        # è·å–è§‚å¯Ÿ
        obs = robot.get_observation()
        
        # å‡†å¤‡è¾“å…¥
        observation = {
            "observation.state": torch.tensor(obs["joint_positions"]).unsqueeze(0).to("cuda"),
            "observation.images.front": torch.tensor(obs["cameras"]["front"]).permute(2, 0, 1).unsqueeze(0).to("cuda"),
        }
        
        # æ¨ç†
        with torch.no_grad():
            output = policy.select_action(observation)
            action = output["action"][0].cpu().numpy()
        
        # æ‰§è¡ŒåŠ¨ä½œ
        robot.send_action({"joint_positions": action})
        
        # æ§åˆ¶é¢‘ç‡
        time.sleep(1/30)
        
finally:
    robot.disconnect()
```

### æ‰¹é‡è¯„ä¼°

```bash
# åœ¨ä»¿çœŸç¯å¢ƒä¸­è¯„ä¼°ç­–ç•¥
lerobot-eval \
  --policy.path=lerobot/act_aloha_mobile_cabinet \
  --env.type=aloha \
  --env.task=mobile_cabinet \
  --eval.n_episodes=50 \
  --eval.max_steps=500 \
  --output_dir=./eval_results
```

### LIBERO åŸºå‡†æµ‹è¯•

```bash
# åœ¨ LIBERO åŸºå‡†ä¸Šè¯„ä¼°
lerobot-eval \
  --policy.path=lerobot/pi0_libero_finetuned \
  --env.type=libero \
  --env.task=libero_object \
  --eval.n_episodes=10 \
  --eval.max_steps=600
```

### MetaWorld åŸºå‡†æµ‹è¯•

```bash
# åœ¨ MetaWorld ä¸Šè¯„ä¼°
lerobot-eval \
  --policy.path=lerobot/tdmpc_metaworld \
  --env.type=metaworld \
  --env.task=reach-v2 \
  --eval.n_episodes=20
```

### è‡ªå®šä¹‰è¯„ä¼°è„šæœ¬

```python
from lerobot.common.policies.act.modeling_act import ACTPolicy
from lerobot.envs import make_env
import numpy as np

# 1. åŠ è½½æ¨¡å‹å’Œç¯å¢ƒ
policy = ACTPolicy.from_pretrained("lerobot/act_pusht")
policy.eval()
policy.to("cuda")

env = make_env("pusht", render_mode="rgb_array")

# 2. è¯„ä¼°å‡½æ•°
def evaluate_policy(policy, env, n_episodes=10):
    success_count = 0
    episode_rewards = []
    
    for episode in range(n_episodes):
        obs, info = env.reset()
        episode_reward = 0
        done = False
        
        while not done:
            # å‡†å¤‡è§‚å¯Ÿ
            observation = {
                "observation.state": torch.tensor(obs["agent_pos"]).unsqueeze(0).to("cuda"),
                "observation.images.top": torch.tensor(obs["pixels"]).permute(2, 0, 1).unsqueeze(0).to("cuda"),
            }
            
            # æ¨ç†
            with torch.no_grad():
                action = policy.select_action(observation)["action"][0].cpu().numpy()
            
            # æ‰§è¡Œ
            obs, reward, terminated, truncated, info = env.step(action)
            done = terminated or truncated
            episode_reward += reward
        
        episode_rewards.append(episode_reward)
        if info.get("is_success", False):
            success_count += 1
        
        print(f"Episode {episode+1}: Reward={episode_reward:.2f}, Success={info.get('is_success', False)}")
    
    return {
        "success_rate": success_count / n_episodes,
        "mean_reward": np.mean(episode_rewards),
        "std_reward": np.std(episode_rewards),
    }

# 3. è¿è¡Œè¯„ä¼°
results = evaluate_policy(policy, env, n_episodes=50)
print(f"\nè¯„ä¼°ç»“æœ:")
print(f"æˆåŠŸç‡: {results['success_rate']:.2%}")
print(f"å¹³å‡å¥–åŠ±: {results['mean_reward']:.2f} Â± {results['std_reward']:.2f}")
```

### å¯è§†åŒ–è¯„ä¼°ç»“æœ

```python
import matplotlib.pyplot as plt
import seaborn as sns

# å¯è§†åŒ–æˆåŠŸç‡
def plot_success_rates(results_dict):
    """
    results_dict: {"æ¨¡å‹åç§°": æˆåŠŸç‡}
    """
    plt.figure(figsize=(10, 6))
    models = list(results_dict.keys())
    success_rates = list(results_dict.values())
    
    sns.barplot(x=models, y=success_rates)
    plt.ylabel("æˆåŠŸç‡")
    plt.title("ä¸åŒæ¨¡å‹åœ¨ä»»åŠ¡ä¸Šçš„æˆåŠŸç‡æ¯”è¾ƒ")
    plt.ylim(0, 1)
    plt.xticks(rotation=45)
    plt.tight_layout()
    plt.savefig("success_rates.png", dpi=300)
    plt.show()

# å¯è§†åŒ–è®­ç»ƒæ›²çº¿
def plot_training_curves(log_file):
    import pandas as pd
    
    df = pd.read_csv(log_file)
    
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))
    
    # è®­ç»ƒæŸå¤±
    axes[0, 0].plot(df["step"], df["train_loss"])
    axes[0, 0].set_xlabel("æ­¥æ•°")
    axes[0, 0].set_ylabel("è®­ç»ƒæŸå¤±")
    axes[0, 0].set_title("è®­ç»ƒæŸå¤±æ›²çº¿")
    
    # éªŒè¯æŸå¤±
    axes[0, 1].plot(df["step"], df["val_loss"])
    axes[0, 1].set_xlabel("æ­¥æ•°")
    axes[0, 1].set_ylabel("éªŒè¯æŸå¤±")
    axes[0, 1].set_title("éªŒè¯æŸå¤±æ›²çº¿")
    
    # æˆåŠŸç‡
    axes[1, 0].plot(df["step"], df["success_rate"])
    axes[1, 0].set_xlabel("æ­¥æ•°")
    axes[1, 0].set_ylabel("æˆåŠŸç‡")
    axes[1, 0].set_title("è¯„ä¼°æˆåŠŸç‡")
    
    # å­¦ä¹ ç‡
    axes[1, 1].plot(df["step"], df["learning_rate"])
    axes[1, 1].set_xlabel("æ­¥æ•°")
    axes[1, 1].set_ylabel("å­¦ä¹ ç‡")
    axes[1, 1].set_title("å­¦ä¹ ç‡è°ƒåº¦")
    
    plt.tight_layout()
    plt.savefig("training_curves.png", dpi=300)
    plt.show()
```

## æ•°æ®æ”¶é›†

### æ•°æ®æ”¶é›†æµç¨‹

```mermaid
graph TB
    subgraph "æ•°æ®æ”¶é›†ç®¡çº¿"
        A[å¼€å§‹] --> B[åˆå§‹åŒ–æœºå™¨äºº]
        B --> C[åˆå§‹åŒ–æ•°æ®é›†]
        C --> D[å¼€å§‹å›åˆ]
        
        D --> E[é¥æ“ä½œ/è‡ªä¸»æ§åˆ¶]
        E --> F[è®°å½•è§‚å¯Ÿ]
        F --> G[è®°å½•åŠ¨ä½œ]
        G --> H[ä¿å­˜å¸§]
        
        H --> I{å›åˆç»“æŸ?}
        I -->|å¦| E
        I -->|æ˜¯| J[ä¿å­˜å›åˆ]
        
        J --> K{ç»§ç»­æ”¶é›†?}
        K -->|æ˜¯| D
        K -->|å¦| L[æ•´åˆæ•°æ®é›†]
        
        L --> M[ä¸Šä¼ åˆ°Hub]
        M --> N[ç»“æŸ]
    end
    
    style A fill:#afa
    style N fill:#faa
    style M fill:#aaf
```

### é¥æ“ä½œæ•°æ®æ”¶é›†

```python
from lerobot.robots.leader import LeaderRobot
from lerobot.robots.follower import FollowerRobot
from lerobot.datasets.lerobot_dataset import LeRobotDataset
import numpy as np

# 1. åˆå§‹åŒ–æœºå™¨äºº
leader = LeaderRobot(config=leader_config)
follower = FollowerRobot(config=follower_config)

leader.connect()
follower.connect()

# 2. åˆ›å»ºæ•°æ®é›†
dataset = LeRobotDataset.create(
    repo_id="my-username/teleoperation-data",
    fps=30,
    robot_type="my_robot",
    features={
        "observation.state": {"dtype": "float32", "shape": (14,)},
        "observation.images.front": {"dtype": "video", "shape": (480, 640, 3)},
        "observation.images.wrist": {"dtype": "video", "shape": (240, 320, 3)},
        "action": {"dtype": "float32", "shape": (14,)},
    }
)

# 3. æ•°æ®æ”¶é›†å¾ªç¯
n_episodes = 50

try:
    for episode_idx in range(n_episodes):
        print(f"\nå¼€å§‹æ”¶é›†å›åˆ {episode_idx + 1}/{n_episodes}")
        print("å‡†å¤‡å°±ç»ªåæŒ‰ Enter å¼€å§‹...")
        input()
        
        episode_data = []
        start_time = time.time()
        
        while True:
            # è·å–ä¸»æ§åŠ¨ä½œ
            leader_obs = leader.get_observation()
            action = leader_obs["joint_positions"]
            
            # æ‰§è¡ŒåŠ¨ä½œå¹¶è·å–è§‚å¯Ÿ
            follower.send_action({"joint_positions": action})
            follower_obs = follower.get_observation()
            
            # è®°å½•æ•°æ®å¸§
            frame = {
                "observation.state": follower_obs["joint_positions"],
                "observation.images.front": follower_obs["cameras"]["front"],
                "observation.images.wrist": follower_obs["cameras"]["wrist"],
                "action": action,
                "timestamp": time.time() - start_time,
            }
            episode_data.append(frame)
            
            # æ£€æŸ¥æ˜¯å¦ç»“æŸï¼ˆä¾‹å¦‚æŒ‰é”®æˆ–è¾¾åˆ°æœ€å¤§é•¿åº¦ï¼‰
            if len(episode_data) >= 500 or check_stop_condition():
                break
            
            time.sleep(1/30)
        
        # æ·»åŠ å›åˆåˆ°æ•°æ®é›†
        dataset.add_episode(episode_data)
        print(f"å›åˆ {episode_idx + 1} å®Œæˆï¼Œæ”¶é›†äº† {len(episode_data)} å¸§")
        
finally:
    leader.disconnect()
    follower.disconnect()

# 4. æ•´åˆå¹¶ä¿å­˜
dataset.consolidate()
dataset.save("./teleoperation_data")

# 5. ä¸Šä¼ åˆ° Hub
dataset.push_to_hub(
    repo_id="my-username/teleoperation-data",
    private=False
)
```

### ä½¿ç”¨å‘½ä»¤è¡Œæ”¶é›†æ•°æ®

```bash
# ä½¿ç”¨å†…ç½®è„šæœ¬æ”¶é›†æ•°æ®
lerobot-record \
  --robot-type=so100 \
  --robot-config=configs/robot/so100.yaml \
  --repo-id=my-username/so100-pick-place \
  --num-episodes=100 \
  --episode-length=500 \
  --fps=30 \
  --control-mode=teleoperation
```

### ä»ç°æœ‰æ ¼å¼è½¬æ¢

```python
from lerobot.datasets.convert import convert_dataset

# ä»å…¶ä»–æ ¼å¼è½¬æ¢åˆ° LeRobotDataset
convert_dataset(
    input_format="rlds",  # æˆ– "robomimic", "d4rl" ç­‰
    input_path="./original_dataset",
    output_repo_id="my-username/converted-dataset",
    fps=30,
    robot_type="my_robot",
)
```

## å®é™…åº”ç”¨åœºæ™¯

### åœºæ™¯1: ç§»åŠ¨æ“ä½œä»»åŠ¡

```mermaid
graph LR
    A[ALOHAç§»åŠ¨æ“ä½œ] --> B[æ•°æ®æ”¶é›†]
    A --> C[æ¨¡å‹è®­ç»ƒ]
    A --> D[éƒ¨ç½²åº”ç”¨]
    
    B --> B1[é¥æ“ä½œæ¼”ç¤º]
    B --> B2[å¤šè§’åº¦ç›¸æœº]
    B --> B3[çŠ¶æ€è®°å½•]
    
    C --> C1[ACTç­–ç•¥]
    C --> C2[100kæ­¥è®­ç»ƒ]
    C --> C3[æ•°æ®å¢å¼º]
    
    D --> D1[å®æ—¶æ¨ç†]
    D --> D2[é—­ç¯æ§åˆ¶]
    D --> D3[å®‰å…¨ç›‘æ§]
    
    style A fill:#9f9
```

**ç¤ºä¾‹ä»£ç ï¼š**

```python
# 1. æ”¶é›†æ•°æ®
lerobot-record \
  --robot-type=aloha \
  --repo-id=my-username/aloha-mobile-cabinet \
  --num-episodes=50

# 2. è®­ç»ƒæ¨¡å‹
lerobot-train \
  --policy=act \
  --dataset.repo_id=my-username/aloha-mobile-cabinet \
  --training.num_steps=100000 \
  --output_dir=./models/aloha_cabinet

# 3. è¯„ä¼°
lerobot-eval \
  --policy.path=./models/aloha_cabinet \
  --env.type=aloha \
  --env.task=mobile_cabinet \
  --eval.n_episodes=10

# 4. éƒ¨ç½²
lerobot-deploy \
  --policy.path=./models/aloha_cabinet \
  --robot-type=aloha \
  --control-frequency=30
```

### åœºæ™¯2: ä½æˆæœ¬æœºæ¢°è‡‚å­¦ä¹ 

```mermaid
graph TB
    subgraph "SO-100 å·¥ä½œæµ"
        A[SO-100æœºæ¢°è‡‚] --> B[ç¡¬ä»¶ç»„è£…]
        B --> C[è½¯ä»¶é…ç½®]
        C --> D[é¥æ“ä½œè®­ç»ƒ]
        D --> E[ç­–ç•¥å­¦ä¹ ]
        E --> F[è‡ªä¸»æ‰§è¡Œ]
        
        B --> B1[200ç¾å…ƒæˆæœ¬]
        C --> C1[LeRoboté›†æˆ]
        D --> D1[æ‰‹æœºé¥æ§]
        E --> E1[Diffusionç­–ç•¥]
        F --> F1[å®é™…åº”ç”¨]
    end
    
    style A fill:#f96
    style F fill:#9f6
```

**å®Œæ•´ç¤ºä¾‹ï¼š**

```python
from lerobot.robots.so100 import SO100Robot
from lerobot.common.policies.diffusion import DiffusionPolicy

# 1. è¿æ¥ SO-100
robot = SO100Robot(port="/dev/ttyUSB0")
robot.connect()

# 2. æ ¡å‡†
robot.calibrate()

# 3. æ”¶é›†æ¼”ç¤ºæ•°æ®
dataset = collect_demonstrations(
    robot=robot,
    control_device="phone",  # ä½¿ç”¨æ‰‹æœºé¥æ§
    num_episodes=30,
    task="pick_and_place"
)

# 4. è®­ç»ƒç­–ç•¥
policy = train_policy(
    policy_type="diffusion",
    dataset=dataset,
    training_steps=50000
)

# 5. éƒ¨ç½²
deploy_policy(
    policy=policy,
    robot=robot,
    safety_checks=True
)
```

### åœºæ™¯3: äººå½¢æœºå™¨äººæ§åˆ¶

```mermaid
graph LR
    A[Reachy 2 äººå½¢æœºå™¨äºº] --> B[VLAæ¨¡å‹]
    A --> C[å¤šæ¨¡æ€æ„ŸçŸ¥]
    A --> D[å¤æ‚æ“ä½œ]
    
    B --> B1[GR00T N1.5]
    B --> B2[è§†è§‰è¯­è¨€ç†è§£]
    B --> B3[é›¶æ ·æœ¬æ³›åŒ–]
    
    C --> C1[åŒè‡‚åè°ƒ]
    C --> C2[è§†è§‰åé¦ˆ]
    C --> C3[åŠ›è§‰ä¼ æ„Ÿ]
    
    D --> D1[ç²¾ç»†æ“ä½œ]
    D --> D2[äººæœºäº¤äº’]
    D --> D3[è‡ªé€‚åº”è¡Œä¸º]
    
    style A fill:#aaf
```

**ä½¿ç”¨ VLA æ¨¡å‹ï¼š**

```python
from lerobot.common.policies.groot import GR00TPolicy
from lerobot.robots.reachy2 import Reachy2Robot

# 1. åŠ è½½é¢„è®­ç»ƒ VLA æ¨¡å‹
policy = GR00TPolicy.from_pretrained("lerobot/groot_n1.5")
policy.eval()
policy.to("cuda")

# 2. è¿æ¥ Reachy 2
robot = Reachy2Robot()
robot.connect()

# 3. è¯­è¨€æ¡ä»¶æ§åˆ¶
task_instruction = "è¯·æ‹¿èµ·æ¡Œå­ä¸Šçš„çº¢è‰²æ–¹å—ï¼Œæ”¾åˆ°è“è‰²ç›’å­é‡Œ"

while True:
    # è·å–è§‚å¯Ÿï¼ˆåŒ…æ‹¬è§†è§‰ï¼‰
    obs = robot.get_observation()
    
    # VLA æ¨ç†
    with torch.no_grad():
        action = policy.predict(
            images=obs["cameras"],
            states=obs["joint_positions"],
            instruction=task_instruction
        )
    
    # æ‰§è¡ŒåŠ¨ä½œ
    robot.send_action(action)
    
    # æ£€æŸ¥ä»»åŠ¡å®Œæˆ
    if robot.check_task_completion():
        break
```

### åœºæ™¯4: ä»¿çœŸåˆ°çœŸå®è¿ç§»

```mermaid
sequenceDiagram
    participant S as ä»¿çœŸç¯å¢ƒ
    participant P as ç­–ç•¥æ¨¡å‹
    participant R as çœŸå®æœºå™¨äºº
    
    S->>P: ä»¿çœŸè®­ç»ƒæ•°æ®
    P->>P: å­¦ä¹ ç­–ç•¥
    
    Note over P: Domain Randomization<br/>è§†è§‰å¢å¼º<br/>åŠ¨åŠ›å­¦å˜åŒ–
    
    P->>R: é›¶æ ·æœ¬è¿ç§»
    R->>R: å¾®è°ƒé€‚åº”
    
    Note over R: å°‘é‡çœŸå®æ•°æ®<br/>åœ¨çº¿å­¦ä¹ 
    
    R->>P: æ€§èƒ½åé¦ˆ
    P->>S: æ›´æ–°ä»¿çœŸ
```

**Sim-to-Real ä»£ç ï¼š**

```python
# 1. åœ¨ä»¿çœŸä¸­è®­ç»ƒ
lerobot-train \
  --policy=tdmpc \
  --env.type=metaworld \
  --env.task=reach-v2 \
  --training.num_steps=200000 \
  --training.domain_randomization=true \
  --output_dir=./models/sim_reach

# 2. åœ¨çœŸå®æœºå™¨äººä¸Šå¾®è°ƒ
lerobot-finetune \
  --policy.path=./models/sim_reach \
  --robot-type=so100 \
  --num-episodes=10 \
  --online-learning=true \
  --output_dir=./models/real_reach

# 3. è¯„ä¼°è¿ç§»æ•ˆæœ
lerobot-eval \
  --policy.path=./models/real_reach \
  --robot-type=so100 \
  --eval.n_episodes=20
```

## é«˜çº§ç‰¹æ€§

### è‡ªå®šä¹‰ç­–ç•¥å®ç°

```python
from lerobot.common.policies.policy_protocol import PolicyProtocol
import torch
import torch.nn as nn

class MyCustomPolicy(nn.Module, PolicyProtocol):
    """è‡ªå®šä¹‰ç­–ç•¥ç¤ºä¾‹"""
    
    def __init__(self, config):
        super().__init__()
        self.config = config
        
        # å®šä¹‰ç½‘ç»œå±‚
        self.encoder = nn.Sequential(
            nn.Linear(config.state_dim, 256),
            nn.ReLU(),
            nn.Linear(256, 512),
            nn.ReLU(),
        )
        
        self.actor = nn.Linear(512, config.action_dim)
    
    def forward(self, batch):
        """è®­ç»ƒå‰å‘ä¼ æ’­"""
        states = batch["observation.state"]
        actions = batch["action"]
        
        # ç¼–ç 
        features = self.encoder(states)
        
        # é¢„æµ‹åŠ¨ä½œ
        predicted_actions = self.actor(features)
        
        # è®¡ç®—æŸå¤±
        loss = nn.functional.mse_loss(predicted_actions, actions)
        
        return {"loss": loss, "predicted_actions": predicted_actions}
    
    def select_action(self, observation):
        """æ¨ç†æ—¶é€‰æ‹©åŠ¨ä½œ"""
        with torch.no_grad():
            states = observation["observation.state"]
            features = self.encoder(states)
            action = self.actor(features)
        
        return {"action": action}
    
    @classmethod
    def from_pretrained(cls, path):
        """ä»é¢„è®­ç»ƒæ¨¡å‹åŠ è½½"""
        # å®ç°åŠ è½½é€»è¾‘
        pass
    
    def save_pretrained(self, path):
        """ä¿å­˜æ¨¡å‹"""
        torch.save(self.state_dict(), f"{path}/policy.pt")
```

### å¤šæ¨¡æ€èåˆ

```python
import torch
import torch.nn as nn
from torchvision.models import resnet18

class MultiModalPolicy(nn.Module):
    """å¤šæ¨¡æ€ç­–ç•¥ï¼šèåˆè§†è§‰ã€çŠ¶æ€å’Œè§¦è§‰"""
    
    def __init__(self, config):
        super().__init__()
        
        # è§†è§‰ç¼–ç å™¨
        self.vision_encoder = resnet18(pretrained=True)
        self.vision_encoder.fc = nn.Identity()
        
        # çŠ¶æ€ç¼–ç å™¨
        self.state_encoder = nn.Sequential(
            nn.Linear(config.state_dim, 128),
            nn.ReLU(),
            nn.Linear(128, 256),
        )
        
        # è§¦è§‰ç¼–ç å™¨
        self.tactile_encoder = nn.Sequential(
            nn.Linear(config.tactile_dim, 64),
            nn.ReLU(),
            nn.Linear(64, 128),
        )
        
        # èåˆå±‚
        self.fusion = nn.Sequential(
            nn.Linear(512 + 256 + 128, 512),
            nn.ReLU(),
            nn.Linear(512, config.action_dim),
        )
    
    def forward(self, batch):
        # å¤„ç†è§†è§‰è¾“å…¥
        images = batch["observation.images.front"]
        vision_features = self.vision_encoder(images)
        
        # å¤„ç†çŠ¶æ€è¾“å…¥
        states = batch["observation.state"]
        state_features = self.state_encoder(states)
        
        # å¤„ç†è§¦è§‰è¾“å…¥
        tactile = batch["observation.tactile"]
        tactile_features = self.tactile_encoder(tactile)
        
        # èåˆæ‰€æœ‰æ¨¡æ€
        combined = torch.cat([vision_features, state_features, tactile_features], dim=-1)
        action = self.fusion(combined)
        
        # è®¡ç®—æŸå¤±
        target_action = batch["action"]
        loss = nn.functional.mse_loss(action, target_action)
        
        return {"loss": loss, "predicted_actions": action}
```

### åœ¨çº¿å­¦ä¹ ä¸é€‚åº”

```python
from lerobot.common.online_learning import OnlineLearner

class AdaptivePolicy:
    """æ”¯æŒåœ¨çº¿å­¦ä¹ çš„è‡ªé€‚åº”ç­–ç•¥"""
    
    def __init__(self, base_policy, learning_rate=1e-4):
        self.policy = base_policy
        self.optimizer = torch.optim.Adam(
            self.policy.parameters(),
            lr=learning_rate
        )
        self.replay_buffer = []
        
    def collect_and_adapt(self, robot, num_steps=100):
        """æ”¶é›†æ•°æ®å¹¶åœ¨çº¿é€‚åº”"""
        
        for step in range(num_steps):
            # æ‰§è¡ŒåŠ¨ä½œ
            obs = robot.get_observation()
            action = self.policy.select_action(obs)
            robot.send_action(action)
            
            # è·å–åé¦ˆ
            next_obs = robot.get_observation()
            reward = compute_reward(obs, action, next_obs)
            
            # å­˜å‚¨ç»éªŒ
            self.replay_buffer.append({
                "observation": obs,
                "action": action,
                "reward": reward,
                "next_observation": next_obs,
            })
            
            # åœ¨çº¿æ›´æ–°
            if len(self.replay_buffer) >= 32:
                self.update_policy()
    
    def update_policy(self):
        """ä½¿ç”¨æœ€è¿‘çš„ç»éªŒæ›´æ–°ç­–ç•¥"""
        batch = self.sample_batch()
        
        output = self.policy(batch)
        loss = output["loss"]
        
        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()
        
        # æ¸…ç†æ—§æ•°æ®
        if len(self.replay_buffer) > 1000:
            self.replay_buffer = self.replay_buffer[-1000:]
```

### æ¨¡å‹å‹ç¼©ä¸ä¼˜åŒ–

```python
import torch
from torch.quantization import quantize_dynamic

# 1. åŠ¨æ€é‡åŒ–
policy = ACTPolicy.from_pretrained("lerobot/act_aloha_mobile_cabinet")
quantized_policy = quantize_dynamic(
    policy,
    {torch.nn.Linear},
    dtype=torch.qint8
)

# 2. å¯¼å‡º ONNX
dummy_input = {
    "observation.state": torch.randn(1, 14),
    "observation.images.front": torch.randn(1, 3, 480, 640),
}

torch.onnx.export(
    policy,
    dummy_input,
    "policy.onnx",
    input_names=["state", "image"],
    output_names=["action"],
    dynamic_axes={
        "state": {0: "batch_size"},
        "image": {0: "batch_size"},
        "action": {0: "batch_size"},
    }
)

# 3. TorchScript
scripted_policy = torch.jit.script(policy)
scripted_policy.save("policy_scripted.pt")

# 4. ä½¿ç”¨ TensorRT ä¼˜åŒ–ï¼ˆéœ€è¦å®‰è£… torch-tensorrtï¼‰
import torch_tensorrt

trt_policy = torch_tensorrt.compile(
    policy,
    inputs=[
        torch_tensorrt.Input(shape=[1, 14]),
        torch_tensorrt.Input(shape=[1, 3, 480, 640]),
    ],
    enabled_precisions={torch.float16},
)
```

## æœ€ä½³å®è·µ

### æ•°æ®æ”¶é›†å»ºè®®

```mermaid
graph TB
    A[æ•°æ®æ”¶é›†æœ€ä½³å®è·µ] --> B[æ•°æ®è´¨é‡]
    A --> C[æ•°æ®å¤šæ ·æ€§]
    A --> D[æ•°æ®è§„æ¨¡]
    A --> E[æ ‡æ³¨è§„èŒƒ]
    
    B --> B1[ç¨³å®šçš„æ¼”ç¤º]
    B --> B2[æˆåŠŸçš„è½¨è¿¹]
    B --> B3[ä¸€è‡´çš„æ ‡å‡†]
    B --> B4[å‡å°‘å™ªå£°]
    
    C --> C1[å¤šç§åœºæ™¯]
    C --> C2[ä¸åŒèµ·å§‹ä½ç½®]
    C --> C3[ç¯å¢ƒå˜åŒ–]
    C --> C4[å¹²æ‰°å› ç´ ]
    
    D --> D1[è‡³å°‘50ä¸ªå›åˆ]
    D --> D2[æ¯å›åˆ100+å¸§]
    D --> D3[æ€»å…±5000+æ ·æœ¬]
    
    E --> E1[ç»Ÿä¸€å‘½å]
    E --> E2[å®Œæ•´å…ƒæ•°æ®]
    E --> E3[ç‰ˆæœ¬æ§åˆ¶]
    
    style A fill:#f9c
```

### è®­ç»ƒè°ƒä¼˜æŠ€å·§

**1. è¶…å‚æ•°è°ƒæ•´ï¼š**

```python
# æ¨èçš„ ACT è¶…å‚æ•°
act_config = {
    "chunk_size": 100,          # åŠ¨ä½œåºåˆ—é•¿åº¦
    "n_obs_steps": 1,           # è§‚å¯Ÿå†å²é•¿åº¦
    "dim_model": 512,           # æ¨¡å‹ç»´åº¦
    "n_heads": 8,               # æ³¨æ„åŠ›å¤´æ•°
    "n_encoder_layers": 4,      # ç¼–ç å™¨å±‚æ•°
    "n_decoder_layers": 7,      # è§£ç å™¨å±‚æ•°
    "lr": 1e-5,                 # å­¦ä¹ ç‡
    "weight_decay": 1e-4,       # æƒé‡è¡°å‡
    "kl_weight": 10,            # KL æ•£åº¦æƒé‡
    "batch_size": 32,           # æ‰¹æ¬¡å¤§å°
}

# æ¨èçš„ Diffusion è¶…å‚æ•°
diffusion_config = {
    "n_action_steps": 8,        # é¢„æµ‹æ­¥æ•°
    "num_inference_steps": 10,  # æ¨ç†æ‰©æ•£æ­¥æ•°
    "down_dims": [256, 512, 1024],  # ä¸‹é‡‡æ ·ç»´åº¦
    "lr": 1e-4,
    "batch_size": 64,
}
```

**2. æ•°æ®å¢å¼ºï¼š**

```python
from lerobot.common.datasets.transforms import (
    RandomCrop,
    ColorJitter,
    AddGaussianNoise,
)

# è§†è§‰å¢å¼º
transforms = [
    RandomCrop(scale=(0.8, 1.0)),
    ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),
]

# çŠ¶æ€å¢å¼º
state_transforms = [
    AddGaussianNoise(std=0.01),
]
```

**3. å­¦ä¹ ç‡è°ƒåº¦ï¼š**

```python
from torch.optim.lr_scheduler import CosineAnnealingLR, OneCycleLR

# Cosine é€€ç«
scheduler = CosineAnnealingLR(
    optimizer,
    T_max=100000,
    eta_min=1e-6
)

# One Cycle ç­–ç•¥
scheduler = OneCycleLR(
    optimizer,
    max_lr=1e-4,
    total_steps=100000,
    pct_start=0.1,
)
```

### éƒ¨ç½²ä¼˜åŒ–

```mermaid
graph LR
    A[éƒ¨ç½²ä¼˜åŒ–] --> B[æ¨¡å‹ä¼˜åŒ–]
    A --> C[æ¨ç†åŠ é€Ÿ]
    A --> D[å®‰å…¨ä¿éšœ]
    
    B --> B1[é‡åŒ–]
    B --> B2[å‰ªæ]
    B --> B3[è’¸é¦]
    
    C --> C1[æ‰¹å¤„ç†]
    C --> C2[å¹¶è¡Œæ¨ç†]
    C --> C3[ç¡¬ä»¶åŠ é€Ÿ]
    
    D --> D1[åŠ¨ä½œé™åˆ¶]
    D --> D2[ç¢°æ’æ£€æµ‹]
    D --> D3[ç´§æ€¥åœæ­¢]
    D --> D4[æ—¥å¿—è®°å½•]
    
    style A fill:#9cf
```

**å®‰å…¨éƒ¨ç½²ç¤ºä¾‹ï¼š**

```python
class SafeRobotController:
    """å¸¦å®‰å…¨æ£€æŸ¥çš„æœºå™¨äººæ§åˆ¶å™¨"""
    
    def __init__(self, robot, policy, safety_config):
        self.robot = robot
        self.policy = policy
        self.config = safety_config
        
        # å®‰å…¨é™åˆ¶
        self.joint_limits = safety_config["joint_limits"]
        self.velocity_limits = safety_config["velocity_limits"]
        self.workspace_bounds = safety_config["workspace_bounds"]
        
        # ç›‘æ§
        self.action_history = []
        self.emergency_stop = False
    
    def execute_action(self, observation):
        """å®‰å…¨åœ°æ‰§è¡ŒåŠ¨ä½œ"""
        
        # 1. æ¨ç†åŠ¨ä½œ
        with torch.no_grad():
            action = self.policy.select_action(observation)["action"]
        
        # 2. å®‰å…¨æ£€æŸ¥
        if not self.is_action_safe(action):
            print("âš ï¸ ä¸å®‰å…¨çš„åŠ¨ä½œè¢«é˜»æ­¢")
            return self.get_safe_action()
        
        # 3. æ‰§è¡Œ
        self.robot.send_action({"joint_positions": action})
        self.action_history.append(action)
        
        return action
    
    def is_action_safe(self, action):
        """æ£€æŸ¥åŠ¨ä½œæ˜¯å¦å®‰å…¨"""
        
        # æ£€æŸ¥å…³èŠ‚é™åˆ¶
        for i, (pos, (min_pos, max_pos)) in enumerate(zip(action, self.joint_limits)):
            if pos < min_pos or pos > max_pos:
                print(f"å…³èŠ‚ {i} è¶…å‡ºé™åˆ¶: {pos} not in [{min_pos}, {max_pos}]")
                return False
        
        # æ£€æŸ¥é€Ÿåº¦é™åˆ¶
        if len(self.action_history) > 0:
            velocity = action - self.action_history[-1]
            if torch.abs(velocity).max() > self.velocity_limits:
                print(f"é€Ÿåº¦è¿‡å¤§: {velocity.max()}")
                return False
        
        # æ£€æŸ¥å·¥ä½œç©ºé—´
        ee_pos = self.robot.forward_kinematics(action)
        if not self.is_in_workspace(ee_pos):
            print(f"è¶…å‡ºå·¥ä½œç©ºé—´: {ee_pos}")
            return False
        
        return True
    
    def get_safe_action(self):
        """è·å–å®‰å…¨çš„åå¤‡åŠ¨ä½œ"""
        current_obs = self.robot.get_observation()
        return current_obs["joint_positions"]  # ä¿æŒå½“å‰ä½ç½®
    
    def is_in_workspace(self, position):
        """æ£€æŸ¥ä½ç½®æ˜¯å¦åœ¨å·¥ä½œç©ºé—´å†…"""
        x, y, z = position
        x_min, x_max = self.workspace_bounds["x"]
        y_min, y_max = self.workspace_bounds["y"]
        z_min, z_max = self.workspace_bounds["z"]
        
        return (x_min <= x <= x_max and
                y_min <= y <= y_max and
                z_min <= z <= z_max)
```

## ç¤¾åŒºä¸èµ„æº

### å®˜æ–¹èµ„æº

```mermaid
graph TB
    A[LeRobotèµ„æº] --> B[æ–‡æ¡£]
    A --> C[ç¤¾åŒº]
    A --> D[ç¤ºä¾‹]
    A --> E[æ”¯æŒ]
    
    B --> B1[å®˜æ–¹æ–‡æ¡£<br/>huggingface.co/docs/lerobot]
    B --> B2[APIå‚è€ƒ]
    B --> B3[æ•™ç¨‹]
    
    C --> C1[DiscordæœåŠ¡å™¨]
    C --> C2[GitHubè®¨è®º]
    C --> C3[X/Twitter]
    
    D --> D1[ç¤ºä¾‹ä»£ç ]
    D --> D2[é¢„è®­ç»ƒæ¨¡å‹]
    D --> D3[æ•°æ®é›†]
    
    E --> E1[GitHub Issues]
    E --> E2[é—®ç­”ç¤¾åŒº]
    E --> E3[è´¡çŒ®æŒ‡å—]
    
    style A fill:#f96
```

### å­¦ä¹ è·¯å¾„

**åˆå­¦è€…ï¼š**
1. å®Œæˆå¿«é€Ÿå¼€å§‹æ•™ç¨‹
2. å°è¯•é¢„è®­ç»ƒæ¨¡å‹æ¨ç†
3. åœ¨ä»¿çœŸç¯å¢ƒä¸­è®­ç»ƒç®€å•ç­–ç•¥
4. å­¦ä¹ æ•°æ®é›†æ ¼å¼

**ä¸­çº§ç”¨æˆ·ï¼š**
1. å®ç°è‡ªå®šä¹‰ Robot ç±»
2. æ”¶é›†å’Œç®¡ç†è‡ªå·±çš„æ•°æ®é›†
3. è®­ç»ƒå¤šç§ç­–ç•¥æ¨¡å‹
4. åœ¨çœŸå®æœºå™¨äººä¸Šéƒ¨ç½²

**é«˜çº§ç”¨æˆ·ï¼š**
1. å®ç°è‡ªå®šä¹‰ç­–ç•¥ç®—æ³•
2. è´¡çŒ®æ–°çš„ç¡¬ä»¶æ”¯æŒ
3. ä¼˜åŒ–è®­ç»ƒå’Œæ¨ç†æ€§èƒ½
4. å‚ä¸ç¤¾åŒºå¼€å‘

### è´¡çŒ®æŒ‡å—

LeRobot æ¬¢è¿å„ç§å½¢å¼çš„è´¡çŒ®ï¼š

**ä»£ç è´¡çŒ®ï¼š**
```bash
# 1. Fork ä»“åº“
git clone https://github.com/your-username/lerobot.git
cd lerobot

# 2. åˆ›å»ºåˆ†æ”¯
git checkout -b feature/my-new-feature

# 3. å®‰è£…å¼€å‘ä¾èµ–
pip install -e ".[dev]"
pre-commit install

# 4. è¿›è¡Œæ›´æ”¹å¹¶æµ‹è¯•
pytest tests/

# 5. æäº¤ Pull Request
git push origin feature/my-new-feature
```

**å…¶ä»–è´¡çŒ®æ–¹å¼ï¼š**
- æŠ¥å‘Š Bug
- æ”¹è¿›æ–‡æ¡£
- åˆ†äº«æ•°æ®é›†
- æä¾›ä½¿ç”¨æ¡ˆä¾‹
- å›ç­”ç¤¾åŒºé—®é¢˜

## æ•…éšœæ’æŸ¥

### å¸¸è§é—®é¢˜

**1. å®‰è£…é—®é¢˜ï¼š**

```bash
# CUDA ç‰ˆæœ¬ä¸åŒ¹é…
pip install torch torchvision --index-url https://download.pytorch.org/whl/cu118

# ä¾èµ–å†²çª
pip install lerobot --no-deps
pip install -r requirements.txt
```

**2. æ•°æ®åŠ è½½æ…¢ï¼š**

```python
# å¢åŠ  workers
dataloader = DataLoader(
    dataset,
    batch_size=32,
    num_workers=8,  # å¢åŠ å·¥ä½œè¿›ç¨‹
    prefetch_factor=2,
    persistent_workers=True
)
```

**3. GPU å†…å­˜ä¸è¶³ï¼š**

```python
# å‡å°æ‰¹æ¬¡å¤§å°
--training.batch_size=16

# ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯
--training.gradient_accumulation_steps=2

# å¯ç”¨æ··åˆç²¾åº¦è®­ç»ƒ
--training.use_amp=true
```

**4. æœºå™¨äººè¿æ¥å¤±è´¥ï¼š**

```python
# æ£€æŸ¥ç«¯å£
import serial.tools.list_ports
ports = serial.tools.list_ports.comports()
for port in ports:
    print(f"{port.device}: {port.description}")

# è®¾ç½®æƒé™ï¼ˆLinuxï¼‰
sudo usermod -a -G dialout $USER
sudo chmod 666 /dev/ttyUSB0
```

### è°ƒè¯•æŠ€å·§

```python
# 1. å¯ç”¨è¯¦ç»†æ—¥å¿—
import logging
logging.basicConfig(level=logging.DEBUG)

# 2. å¯è§†åŒ–æ•°æ®
from lerobot.common.visualization import visualize_episode

dataset = LeRobotDataset("lerobot/pusht")
visualize_episode(dataset, episode_idx=0, output_path="episode_0.mp4")

# 3. æ£€æŸ¥æ¨¡å‹è¾“å‡º
policy.eval()
with torch.no_grad():
    output = policy(sample_batch)
    print(f"è¾“å‡ºå½¢çŠ¶: {output['action'].shape}")
    print(f"åŠ¨ä½œèŒƒå›´: [{output['action'].min()}, {output['action'].max()}]")

# 4. åˆ†æè®­ç»ƒæ›²çº¿
import matplotlib.pyplot as plt
import pandas as pd

logs = pd.read_csv("training_logs.csv")
plt.plot(logs["step"], logs["loss"])
plt.xlabel("æ­¥æ•°")
plt.ylabel("æŸå¤±")
plt.savefig("training_curve.png")
```

## æ€»ç»“

LeRobot æ˜¯ä¸€ä¸ªå¼ºå¤§è€Œæ˜“ç”¨çš„æœºå™¨äººå­¦ä¹ æ¡†æ¶ï¼Œå®ƒé€šè¿‡ä»¥ä¸‹æ ¸å¿ƒä¼˜åŠ¿é™ä½äº†æœºå™¨äºº AI çš„é—¨æ§›ï¼š

```mermaid
mindmap
  root((LeRobotä¼˜åŠ¿))
    æ˜“ç”¨æ€§
      ç»Ÿä¸€æ¥å£
      è¯¦ç»†æ–‡æ¡£
      ä¸°å¯Œç¤ºä¾‹
      æ´»è·ƒç¤¾åŒº
    çµæ´»æ€§
      ç¡¬ä»¶æ— å…³
      ç®—æ³•å¤šæ ·
      æ˜“äºæ‰©å±•
      å¼€æºç”Ÿæ€
    æ€§èƒ½
      å‰æ²¿æ¨¡å‹
      é«˜æ•ˆè®­ç»ƒ
      å¿«é€Ÿæ¨ç†
      çœŸå®éªŒè¯
    ç”Ÿæ€
      HF Hubé›†æˆ
      æ ‡å‡†æ•°æ®æ ¼å¼
      é¢„è®­ç»ƒæ¨¡å‹
      åŸºå‡†æµ‹è¯•
```

æ— è®ºæ‚¨æ˜¯æœºå™¨äººå­¦ä¹ çš„åˆå­¦è€…ï¼Œè¿˜æ˜¯å¸Œæœ›å¿«é€ŸåŸå‹åŒ–æ–°æƒ³æ³•çš„ç ”ç©¶è€…ï¼ŒLeRobot éƒ½èƒ½ä¸ºæ‚¨æä¾›å®Œæ•´çš„å·¥å…·é“¾ï¼Œä»æ•°æ®æ”¶é›†ã€æ¨¡å‹è®­ç»ƒåˆ°å®é™…éƒ¨ç½²ã€‚

**å¼€å§‹æ‚¨çš„æœºå™¨äººå­¦ä¹ ä¹‹æ—…ï¼š**

```bash
# å®‰è£… LeRobot
pip install lerobot

# æ¢ç´¢æ•°æ®é›†
lerobot-info

# è®­ç»ƒæ‚¨çš„ç¬¬ä¸€ä¸ªæ¨¡å‹
lerobot-train --policy=act --dataset.repo_id=lerobot/pusht

# åŠ å…¥ç¤¾åŒº
# Discord: discord.gg/lerobot
# GitHub: github.com/huggingface/lerobot
```

## å‚è€ƒèµ„æº

- **å®˜æ–¹æ–‡æ¡£**: [https://huggingface.co/docs/lerobot](https://huggingface.co/docs/lerobot)
- **GitHub ä»“åº“**: [https://github.com/huggingface/lerobot](https://github.com/huggingface/lerobot)
- **DeepWiki**: [https://deepwiki.com/huggingface/lerobot](https://deepwiki.com/huggingface/lerobot)
- **Hugging Face Hub**: [https://huggingface.co/lerobot](https://huggingface.co/lerobot)
- **è®ºæ–‡å¼•ç”¨**:

```bibtex
@misc{cadene2024lerobot,
    author = {Cadene, Remi and Alibert, Simon and Soare, Alexander and Gallouedec, Quentin and Zouitine, Adil and Palma, Steven and Kooijmans, Pepijn and Aractingi, Michel and Shukor, Mustafa and Aubakirova, Dana and Russi, Martino and Capuano, Francesco and Pascal, Caroline and Choghari, Jade and Moss, Jess and Wolf, Thomas},
    title = {LeRobot: State-of-the-art Machine Learning for Real-World Robotics in Pytorch},
    howpublished = "\url{https://github.com/huggingface/lerobot}",
    year = {2024}
}
```

---

*æœ¬æ–‡æ¡£ç”± LeRobot ç¤¾åŒºç»´æŠ¤ï¼Œæœ€åæ›´æ–°äº 2026-02-08*



